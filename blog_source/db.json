{"meta":{"version":1,"warehouse":"5.0.1"},"models":{"Asset":[{"_id":"themes/fluid/source/css/gitalk.css","path":"css/gitalk.css","modified":0,"renderable":1},{"_id":"themes/fluid/source/css/highlight-dark.styl","path":"css/highlight-dark.styl","modified":0,"renderable":1},{"_id":"themes/fluid/source/css/highlight.styl","path":"css/highlight.styl","modified":0,"renderable":1},{"_id":"themes/fluid/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/fluid/source/img/avatar.png","path":"img/avatar.png","modified":0,"renderable":1},{"_id":"themes/fluid/source/img/default.png","path":"img/default.png","modified":0,"renderable":1},{"_id":"themes/fluid/source/img/loading.gif","path":"img/loading.gif","modified":0,"renderable":1},{"_id":"themes/fluid/source/img/police_beian.png","path":"img/police_beian.png","modified":0,"renderable":1},{"_id":"themes/fluid/source/img/default_.png","path":"img/default_.png","modified":0,"renderable":1},{"_id":"themes/fluid/source/js/boot.js","path":"js/boot.js","modified":0,"renderable":1},{"_id":"themes/fluid/source/js/color-schema.js","path":"js/color-schema.js","modified":0,"renderable":1},{"_id":"themes/fluid/source/img/image.png","path":"img/image.png","modified":0,"renderable":1},{"_id":"themes/fluid/source/img/fluid.png","path":"img/fluid.png","modified":0,"renderable":1},{"_id":"themes/fluid/source/js/events.js","path":"js/events.js","modified":0,"renderable":1},{"_id":"themes/fluid/source/js/plugins.js","path":"js/plugins.js","modified":0,"renderable":1},{"_id":"themes/fluid/source/js/umami-view.js","path":"js/umami-view.js","modified":0,"renderable":1},{"_id":"themes/fluid/source/js/utils.js","path":"js/utils.js","modified":0,"renderable":1},{"_id":"themes/fluid/source/js/img-lazyload.js","path":"js/img-lazyload.js","modified":0,"renderable":1},{"_id":"themes/fluid/source/xml/local-search.xml","path":"xml/local-search.xml","modified":0,"renderable":1},{"_id":"themes/fluid/source/js/leancloud.js","path":"js/leancloud.js","modified":0,"renderable":1},{"_id":"themes/fluid/source/js/local-search.js","path":"js/local-search.js","modified":0,"renderable":1},{"_id":"source/img/og_tinyGL.png","path":"img/og_tinyGL.png","modified":0,"renderable":0},{"_id":"source/display-cabinet/og_tinyGL.png","path":"display-cabinet/og_tinyGL.png","modified":0,"renderable":0}],"Cache":[{"_id":"source/_posts/StartMyBlog.md","hash":"c601978189c6966addf9039420907b7f4b2a9a48","modified":1728572314819},{"_id":"source/_posts/digital-human-render-1.md","hash":"8be912397967deda4fc89a3dbac1267596a68181","modified":1735386017307},{"_id":"source/_posts/digital-human-render-2.md","hash":"84f55dfd8b59de67fe18c56cc50627b55e7f28fb","modified":1735964536711},{"_id":"source/_posts/ProceduralTerrainGeneration2-optimize.md","hash":"61da5dd08d14a0723d3cda94e6b76885c328c261","modified":1732029447531},{"_id":"source/_posts/ProceduralTerrainGeneration.md","hash":"df86726f2d869b6ba9c624f2495db3d88cdfdc98","modified":1730820648824},{"_id":"source/_posts/ProceduralTerrainGeneration2.md","hash":"d4027c11be5e5fa2e65874c422517f8a2c9f8550","modified":1730820574319},{"_id":"source/_posts/cascade-shadow-map.md","hash":"b7d06ca65fd16b88e0379f12fac1bdb685f062f8","modified":1729002713649},{"_id":"source/_posts/defer-render.md","hash":"24f009f8f1273dbdff57f8e4c479d18e4a59f320","modified":1729330996358},{"_id":"source/about/index.md","hash":"05cf53d0da2e813ced689b9b5ae126b571cadc03","modified":1730212344736},{"_id":"source/_posts/depth-of-field.md","hash":"0d6a72a99aacf18dbecf821964595633b4aeaabe","modified":1730128759418},{"_id":"source/_posts/reflective-shadow-map.md","hash":"8b470a2fdfec1ff5a5eab6dc3ec93b6a1811c65d","modified":1733055178064},{"_id":"source/_posts/ue-ai-texture-generation.md","hash":"be5f98009751d1e8daaf587730e256fc1d401e5d","modified":1734874511233},{"_id":"source/_posts/screen-space-reflection.md","hash":"e24ba924af367eab5b3452c0ec5ee09e7b20d698","modified":1734143739996},{"_id":"source/_posts/single-scatter-atmosphere.md","hash":"8db5bd0a173622c39ddfd4da72eed1964e1f9915","modified":1729002701794},{"_id":"source/_posts/soft-shadow.md","hash":"c1487cb47301dbdbffd6a7c16c94f44e2aab9ade","modified":1733631627459},{"_id":"source/display-cabinet/index.md","hash":"388b252d55531e265f8aaf5041e81219f1132235","modified":1730213320119},{"_id":"source/_posts/cascade-shadow-map/csm_far.png","hash":"d6f8dce239df0ca3fb8008e6beea34d08fee464a","modified":1726459035207},{"_id":"source/_posts/cascade-shadow-map/csm_mid.png","hash":"886730be4cb07cf725f965acf50bd8dbe28ba87e","modified":1726459011390},{"_id":"source/_posts/digital-human-render-1/metahuman_mugshot.jpg","hash":"fe9f8c1b0dbd17293c28066e4c5b4a5ff067d5bb","modified":1735385735840},{"_id":"source/_posts/cascade-shadow-map/csm_near.png","hash":"e575a785c09d4c192db4c0e094d919f88e31029a","modified":1726459059738},{"_id":"source/_posts/digital-human-render-2/layers_of_skin.png","hash":"138537a14451e6f3aacdcdfa68cdb01d08376bac","modified":1735969804473},{"_id":"source/_posts/digital-human-render-2/diffuse-comparison.png","hash":"7b89cf35af0cc6373fc00cb16665cd898ad55c53","modified":1735970427908},{"_id":"source/_posts/digital-human-render-2/specular-lobe.png","hash":"e0cfe780b8f0c9b31e47a179a1d45ac7e468acd8","modified":1735970151332},{"_id":"source/_posts/soft-shadow/get-d_blocker-1.png","hash":"76a930f535d50a0806c3e9d12ceb4c81c3766882","modified":1733629530392},{"_id":"source/_posts/soft-shadow/umbra-contrast.png","hash":"3fa9fcf1cf3786dbb7ef4dcd7cd1b7f43d0ba55b","modified":1733628506207},{"_id":"source/_posts/soft-shadow/umbra-principle.png","hash":"870ce6426dc622ee5bab3e83e01ae9a3b4362c6c","modified":1733628276970},{"_id":"source/_posts/screen-space-reflection/ssr_under_sample.png","hash":"01f5ea68a28df4bb068ea72af5804f1f841458d3","modified":1734141264633},{"_id":"source/_posts/ProceduralTerrainGeneration/calc_soft_shadow.png","hash":"7bc24f450d820326f7c05fc625259f59481db0db","modified":1728292147567},{"_id":"source/_posts/defer-render/ssao.png","hash":"b208b05bfddb222235f6617fd22ae6d272f861cb","modified":1725633120670},{"_id":"source/_posts/digital-human-render-1/初音未来.png","hash":"c1515afbc6641e879d8528f32aa870f18690aa9c","modified":1735384538738},{"_id":"source/_posts/digital-human-render-2/reflectance-lookup-table.png","hash":"6694dd988401383fb864e4b599fff41385096ed2","modified":1735971035070},{"_id":"source/_posts/digital-human-render-1/虚拟偶像市场规模.png","hash":"dbd45b900479559e9683a121d7d4aca343216761","modified":1735384997071},{"_id":"source/_posts/reflective-shadow-map/rsm_principle_2.png","hash":"9d502e79a04738d234249fb13c3ff9e287f90f28","modified":1733046251803},{"_id":"source/_posts/soft-shadow/get-d_blocker-2.png","hash":"d90513508242415a0cad2cb0cce6cc8787779660","modified":1733630171691},{"_id":"source/_posts/ue-ai-texture-generation/texgen_bp_macro.png","hash":"076be898d4349c32f0e2e150212afce8681263bc","modified":1734863788141},{"_id":"source/_posts/screen-space-reflection/ssr_over_sample.png","hash":"7ad4eefe6ed3c9eac57f3887f4354ca22ba19e27","modified":1734141248034},{"_id":"source/_posts/digital-human-render-1/林明美.png","hash":"495e8e6a0bf5de115d2aecc64c0ec78977666ee9","modified":1735384431007},{"_id":"source/_posts/digital-human-render-1/老黄建模.png","hash":"5fc3ea52711df5b194732514cf9987a0d89421ad","modified":1735385249166},{"_id":"source/_posts/digital-human-render-1/老黄驱动.png","hash":"15d95c9e56afed5c007f286ceb289bd8d0e8864c","modified":1735385387144},{"_id":"source/_posts/digital-human-render-2/BSSRDF.png","hash":"cbd4bf8293e461e85f7c70e0f5e9bf76fc309551","modified":1735970569329},{"_id":"source/_posts/digital-human-render-2/BTDF.png","hash":"4653f52fc9136d61d74133c56c93d1c52ab99e60","modified":1735971943242},{"_id":"source/_posts/digital-human-render-2/screen-space-blur.png","hash":"2252754914ddbe14f09d136e9bd99b2ff0b54666","modified":1735971614717},{"_id":"source/_posts/digital-human-render-2/texture-space-blur.png","hash":"3de671f74ee27649284ff301c34ba92744ca67c2","modified":1735971510287},{"_id":"source/_posts/reflective-shadow-map/rsm_info.png","hash":"9e4355069be01e0ff6af9bc41a2d0e3d9ae966ab","modified":1732451447354},{"_id":"source/_posts/screen-space-reflection/ssr_ss_sample.png","hash":"215ec66b0f42f10afb4e798f142a88e829dc7c1c","modified":1734141597515},{"_id":"source/_posts/ProceduralTerrainGeneration/shadertoy_oc5_noshadow.png","hash":"64c2e7528dfe87ea1e450decd23c569918479c18","modified":1727864785397},{"_id":"source/_posts/ProceduralTerrainGeneration/shadertoy_terrain_oct5.png","hash":"0f4daabfa7d24bf09919beb71c0701d1c9c7e524","modified":1727864061852},{"_id":"source/_posts/digital-human-render-2/BSSRDF-1.png","hash":"5a296e452304352d836d596b9fc9290b3f5152bb","modified":1735970831970},{"_id":"source/_posts/digital-human-render-1/老黄模型渲染.png","hash":"5ead1685c64d87b187b52e37cf1cfd63f9b75644","modified":1735385390005},{"_id":"source/_posts/digital-human-render-2/Cook-Torrance-BRDF.png","hash":"89f038aa54b93f5225b0d5bb57492b174aa4968a","modified":1735970053451},{"_id":"source/_posts/digital-human-render-2/skin-specular.png","hash":"1cd412abb03b7a925e43b3f54b61f44220f88386","modified":1735970186521},{"_id":"source/_posts/reflective-shadow-map/rsm_off_sphere.png","hash":"7f1d0e24a55b55f1b91e6b3eb919f6b444abec54","modified":1733053969848},{"_id":"source/_posts/soft-shadow/penumbra.png","hash":"954283c20a01c51408cf2b1d762e96ccd89bbb3a","modified":1733630370419},{"_id":"source/_posts/digital-human-render-1/siren.png","hash":"787586bf5f2a20f97674bbc62697cfc8f16adbaa","modified":1735384639603},{"_id":"source/_posts/depth-of-field/game1.jpg","hash":"0034e1ebc0e270dca6ea9c6fc878f3889d54e091","modified":1730128605763},{"_id":"source/_posts/reflective-shadow-map/rsm_off_suit.png","hash":"3c6d18b25f8f84dba48b1ef8911bc9a745e222ce","modified":1733054171656},{"_id":"source/_posts/reflective-shadow-map/rsm_sample.png","hash":"be7dffe23b9e4e42b7612026466fb70fc0c1dca2","modified":1733053255667},{"_id":"source/_posts/soft-shadow/normal-shadow.png","hash":"7ab8c46a962e071b2bfb011ff9572369f070df1e","modified":1733626408839},{"_id":"source/_posts/reflective-shadow-map/rsm_on_sphere.png","hash":"72ea15cd39fff2d6d343268bede9e1c95bab94e8","modified":1733053974124},{"_id":"source/_posts/ProceduralTerrainGeneration/shadertoy_terrain.png","hash":"56cc9ca4c38d9dea545c3fc71e930ff792e46fac","modified":1727862639660},{"_id":"source/_posts/ProceduralTerrainGeneration/shadertoy_oc11_hardshadow.png","hash":"4c4b64ea717fa659999b44c3218fe77f304e1d12","modified":1727864947470},{"_id":"source/_posts/ProceduralTerrainGeneration2/terrain_no_grass.png","hash":"6fd3d29360d763f99afc976dbec8ca77c4edcf2c","modified":1730732749897},{"_id":"source/_posts/digital-human-render-2/blur-camparison.png","hash":"3cc296ca2e21844a0c7f430a31142dcfc43cf355","modified":1735971638037},{"_id":"source/_posts/digital-human-render-1/老黄扫描.png","hash":"93cbeb7dc2b112e31ac849509c4a8cb5ed2831d3","modified":1735385243592},{"_id":"source/_posts/digital-human-render-2/ue-skin.png","hash":"212e6ff77cfd78d459a0f7878a46b4f47677c51b","modified":1735972048617},{"_id":"source/_posts/reflective-shadow-map/rsm_on_suit.png","hash":"aebb00ccc8dab9d657d10a260d069ddf51c4d455","modified":1733054181156},{"_id":"source/_posts/reflective-shadow-map/rsm_principle_1.png","hash":"198f7461830637664bb7ebb7fc40100fe9ed41d1","modified":1733046288939},{"_id":"source/_posts/soft-shadow/pcf-shadow-3.png","hash":"5b0473c8a5106943b16ee20c19e6b5b028015ce9","modified":1733627202361},{"_id":"source/_posts/soft-shadow/real_shadow.png","hash":"c2e7f5a8f5eb40448b3b1028d4369fdc5a471bc6","modified":1733628002636},{"_id":"source/_posts/ProceduralTerrainGeneration/shadertoy_oc11_noshadow.png","hash":"48d0f8a6b7cd0289041b3bea39c24782aaf930f7","modified":1727864806418},{"_id":"source/_posts/ProceduralTerrainGeneration2/terrain_with_sky.png","hash":"25bc38e72b9bde4ea69bbde7a25029becdf3a757","modified":1730733306047},{"_id":"source/_posts/ProceduralTerrainGeneration2/terrain_with_sky_fog.png","hash":"dc9fa37962207b51ec4b3659057a2eccc49f6bb0","modified":1730733326815},{"_id":"source/_posts/ProceduralTerrainGeneration2/terrain_with_grass.png","hash":"ecbed0af037da143c48b1dc2a52c3ed6d1569808","modified":1730732760150},{"_id":"source/_posts/ProceduralTerrainGeneration2/terrain_with_sky_fog_less_ambient.png","hash":"819633b9a30cd7a285f4a8f3fecd454bee8468d4","modified":1730733338763},{"_id":"source/_posts/digital-human-render-1/metahuman面部驱动.png","hash":"ae02dd9c189215b55e799746d92bc3963c2865b4","modified":1735385381232},{"_id":"source/_posts/single-scatter-atmosphere/single-scatter-atmosphere.png","hash":"7266482600228204c32e0fa281e85f626baf7773","modified":1724580719028},{"_id":"source/_posts/soft-shadow/pcf-shadow-1.png","hash":"ca79acf3405964cf1c55d79dc70247d9253296aa","modified":1733627202363},{"_id":"source/_posts/screen-space-reflection/ssr-step4.gif","hash":"99869235c0ebaee933152c538e8aca432e0e7bca","modified":1734138186389},{"_id":"source/_posts/defer-render/defer_render_banner.png","hash":"1962262b49a5eff662ac3a76846d4f6414660bc8","modified":1725633833915},{"_id":"source/_posts/screen-space-reflection/ssr_result.png","hash":"6c9bffb3ff2b88a4d5ea119642a40350445a8df2","modified":1734142756289},{"_id":"source/_posts/screen-space-reflection/ssr_normal_s32.png","hash":"cec7935113cf6206898b8b5dfe539e46d9e497eb","modified":1734139692323},{"_id":"source/_posts/screen-space-reflection/ssr_sample_twice.png","hash":"b18cab4ec8ab1d9147b04aefd2caa85372561f2e","modified":1734140882289},{"_id":"source/_posts/ProceduralTerrainGeneration2/terrain_with_sky_fog_diffuse_from_mountain.png","hash":"82daa3b5403c88308c8fae59330d9a9c008b24aa","modified":1730733353026},{"_id":"source/_posts/ProceduralTerrainGeneration2/terrain_with_sky_fog_diffuse_from_sky.png","hash":"8b3ddaad30616a7b0b5c606aa4bf89b43afb7192","modified":1730733362737},{"_id":"source/_posts/ue-ai-texture-generation/texgen_bp.png","hash":"f2e1fa65b29332d04311094be7422a9a9edbc660","modified":1734863540019},{"_id":"source/_posts/screen-space-reflection/ssr_normal_s128.png","hash":"513a2cb22ce5c11e230984bff14d5388a0495da2","modified":1734139738159},{"_id":"source/_posts/ue-ai-texture-generation/texgen_cmd.png","hash":"de1b6ef7191ee713ba3e4e769437db4e1dce0a2a","modified":1734863825778},{"_id":"themes/fluid/source/css/_pages/_tag/tag.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1721800613000},{"_id":"source/_posts/soft-shadow/PCSS_OFF.png","hash":"0694fc4bf6a06f3f52ec4329ceefc874efca5c54","modified":1733630911840},{"_id":"source/_posts/ProceduralTerrainGeneration2/terrain_with_all_cloud.png","hash":"89318f0a3a168ac5b9cbfde0a729f45e6d918d80","modified":1730820129800},{"_id":"source/_posts/ProceduralTerrainGeneration2/terrain_with_high_cloud.png","hash":"17f1b24ce2e0e425166da356780fdd6e9c38343c","modified":1730733383869},{"_id":"themes/fluid/.editorconfig","hash":"33218fbd623feb43edf5f99f15965392cecc44a6","modified":1721800613000},{"_id":"themes/fluid/.gitattributes","hash":"a54f902957d49356376b59287b894b1a3d7a003f","modified":1721800613000},{"_id":"themes/fluid/.gitignore","hash":"ae3bfcb89777657c5dfb5169d91445dcb0e5ab98","modified":1721800613000},{"_id":"themes/fluid/.eslintrc","hash":"4bc2b19ce2b8c4d242f97d4ccf2d741e68ab0097","modified":1721800613000},{"_id":"themes/fluid/README.md","hash":"ff9b0e1fb9dba665af2f1e4a577f8cb9e840464b","modified":1721800613000},{"_id":"themes/fluid/LICENSE","hash":"26f9356fd6e84b5a88df6d9014378f41b65ba209","modified":1721800613000},{"_id":"themes/fluid/README_en.md","hash":"365184a73af40e7365504c3077f3d80dfee1d80e","modified":1721800613000},{"_id":"themes/fluid/languages/en.yml","hash":"9c580471257f5a32bee701a059a45ea96755dcdc","modified":1721800613000},{"_id":"themes/fluid/_config.yml","hash":"fedf0812c0b0f20989e05e51a3571904789ceb4a","modified":1730212969072},{"_id":"themes/fluid/package.json","hash":"7746460fc2eba7439b494c46aa9b5ded81370819","modified":1721800613000},{"_id":"themes/fluid/languages/es.yml","hash":"026ddf1a49bf8ddfef6ed86ab4d6af143c1dd95f","modified":1721800613000},{"_id":"themes/fluid/languages/eo.yml","hash":"7c1a0c9f6186b6643b19d3980f055329bdb4efa4","modified":1721800613000},{"_id":"themes/fluid/languages/ru.yml","hash":"93818f8bf07195fb1ebffbb5210e531b0e3a6ec4","modified":1721800613000},{"_id":"themes/fluid/languages/ja.yml","hash":"550b95d3614a64592f02666938d235e9f11e449e","modified":1721800613000},{"_id":"themes/fluid/languages/de.yml","hash":"58dccef1d98b472dc4e6f4693c2297b0c9c5afba","modified":1721800613000},{"_id":"themes/fluid/languages/zh-CN.yml","hash":"a60847136709bb95586a98d9d67b50390a8d2c96","modified":1721800613000},{"_id":"themes/fluid/layout/404.ejs","hash":"b84d575c7b7f778b4cb64e89ad3d0aed4a896820","modified":1721800613000},{"_id":"themes/fluid/layout/archive.ejs","hash":"7c1f44005849791feae4abaa10fae4cb983d3277","modified":1721800613000},{"_id":"themes/fluid/layout/categories.ejs","hash":"13859726c27b6c79b5876ec174176d0f9c1ee164","modified":1721800613000},{"_id":"themes/fluid/layout/category.ejs","hash":"f099161b738a16a32253f42085b5444f902018ed","modified":1721800613000},{"_id":"themes/fluid/languages/zh-TW.yml","hash":"e1043de394f6dcf5c0647adcfdefe60637f78426","modified":1721800613000},{"_id":"themes/fluid/layout/about.ejs","hash":"052e9fc19c753f53fdc083c7fb098e3668880140","modified":1721800613000},{"_id":"themes/fluid/languages/zh-HK.yml","hash":"51c2b4d64c6992a39bfd2586a1bdf5fbbbdf0175","modified":1721800613000},{"_id":"themes/fluid/layout/index.ejs","hash":"33c3317cdcee062789de2336dd8d0cc7f86d3650","modified":1721800613000},{"_id":"themes/fluid/layout/layout.ejs","hash":"7e0023474128fbe4d68c467704c41f1712432415","modified":1721800613000},{"_id":"themes/fluid/layout/page.ejs","hash":"ed5007a3feb8f14d3d2843271bfb298eb0c56219","modified":1721800613000},{"_id":"themes/fluid/layout/tag.ejs","hash":"9d686364c4d16a1a9219471623af452035c5b966","modified":1721800613000},{"_id":"themes/fluid/scripts/events/index.js","hash":"79de5a379b28cad759a49048351c7f6b8915bd7d","modified":1721800613000},{"_id":"themes/fluid/layout/tags.ejs","hash":"1d06af34b6cf1d8a20d2eb565e309326ceba309f","modified":1721800613000},{"_id":"themes/fluid/layout/links.ejs","hash":"1cac32ec4579aaf7b9fa39d317497331d4c5e1dd","modified":1721800613000},{"_id":"themes/fluid/.github/workflows/limit.yaml","hash":"f8bd2edeb4424ee7a055b31583445d5d5dff91a4","modified":1721800613000},{"_id":"themes/fluid/layout/post.ejs","hash":"9bf0d357a607a282f3b9cb04525a4df0cc2a8b76","modified":1721800613000},{"_id":"themes/fluid/.github/workflows/publish.yaml","hash":"6f02e6440d88629229556e3fd47d0280fe2240db","modified":1721800613000},{"_id":"themes/fluid/scripts/generators/local-search.js","hash":"9ac5ddad06e9b0e6015ce531430018182a4bc0fa","modified":1721800613000},{"_id":"themes/fluid/scripts/generators/pages.js","hash":"d3e75f53c59674d171309e50702954671f31f1a4","modified":1721800613000},{"_id":"themes/fluid/scripts/generators/index-generator.js","hash":"9159fc22fa84a7b605dd15fe4104f01fe9c71147","modified":1721800613000},{"_id":"themes/fluid/.github/workflows/cr.yaml","hash":"19a8a00f5ba9607d82265572fe1202b64a8b0822","modified":1721800613000},{"_id":"themes/fluid/scripts/tags/button.js","hash":"3eb43a8cdea0a64576ad6b31b4df6c2bf5698d4c","modified":1721800613000},{"_id":"themes/fluid/scripts/tags/checkbox.js","hash":"6eaf53cf4bfc756a65bda18184cf8998a12c861d","modified":1721800613000},{"_id":"themes/fluid/scripts/tags/fold.js","hash":"73e4fd12ce3e47981479391ed354b7d9d3279f70","modified":1721800613000},{"_id":"themes/fluid/scripts/tags/group-image.js","hash":"4aeebb797026f1df25646a5d69f7fde79b1bcd26","modified":1721800613000},{"_id":"themes/fluid/scripts/tags/label.js","hash":"f05a6d32cca79535b22907dc03edb9d3fa2d8176","modified":1721800613000},{"_id":"themes/fluid/.github/ISSUE_TEMPLATE/question_zh.md","hash":"fff07ce0472afc368d388637cb9d438195da9b5b","modified":1721800613000},{"_id":"themes/fluid/.github/ISSUE_TEMPLATE/bug_report.md","hash":"554c0d0e086a0784d83ee71c83f8bceeb60aecc8","modified":1721800613000},{"_id":"themes/fluid/.github/ISSUE_TEMPLATE/bug_report_zh.md","hash":"c8b0d49c49e3c88872fd3b37909345ff5b2b6aa0","modified":1721800613000},{"_id":"themes/fluid/scripts/tags/mermaid.js","hash":"75160561e1ef3603b6d2ad2938464ab1cb77fd38","modified":1721800613000},{"_id":"themes/fluid/scripts/tags/note.js","hash":"e3b456a079e5dc0032473b516c865b20f83d2c26","modified":1721800613000},{"_id":"themes/fluid/scripts/helpers/engine.js","hash":"d3a231d106795ce99cb0bc77eb65f9ae44515933","modified":1721800613000},{"_id":"themes/fluid/scripts/helpers/export-config.js","hash":"8e67b522c47aa250860e3fe2c733f1f958a506c0","modified":1721800613000},{"_id":"themes/fluid/scripts/helpers/date.js","hash":"9bda6382f61b40a20c24af466fe10c8366ebb74c","modified":1721800613000},{"_id":"themes/fluid/scripts/helpers/import.js","hash":"ca53e8dbf7d44cfd372cfa79ac60f35a7d5b0076","modified":1721800613000},{"_id":"themes/fluid/scripts/helpers/page.js","hash":"4607607445233b3029ef20ed5e91de0da0a7f9c5","modified":1721800613000},{"_id":"themes/fluid/scripts/helpers/scope.js","hash":"d41d9d658fcb54964b388598e996747aadb85b0f","modified":1721800613000},{"_id":"themes/fluid/scripts/helpers/injects.js","hash":"1ad2ae6b11bd8806ee7dd6eb7140d8b54a95d613","modified":1721800613000},{"_id":"themes/fluid/scripts/helpers/url.js","hash":"2a6a8288176d0e0f6ec008056bf2745a86e8943e","modified":1721800613000},{"_id":"themes/fluid/scripts/helpers/utils.js","hash":"966689d7c5e4320008285395fbaa2751f6209be5","modified":1721800613000},{"_id":"themes/fluid/scripts/helpers/wordcount.js","hash":"4d48c424e47ff9a17a563167ea5f480890267adf","modified":1721800613000},{"_id":"themes/fluid/scripts/utils/compare-versions.js","hash":"dbbc928c914fc2bd242cd66aa0c45971aec13a5d","modified":1721800613000},{"_id":"themes/fluid/scripts/utils/resolve.js","hash":"8c4a8b62aa8608f12f1e9046231dff04859dc3e9","modified":1721800613000},{"_id":"themes/fluid/scripts/utils/object.js","hash":"33b57e4decdc5e75c518859f168c8ba80b2c665b","modified":1721800613000},{"_id":"themes/fluid/scripts/utils/crypto.js","hash":"ae4ad8a188ef5b3fa6818b01629fc962b3de8551","modified":1721800613000},{"_id":"themes/fluid/scripts/utils/url-join.js","hash":"718aab5e7b2059a06b093ca738de420d9afa44ba","modified":1721800613000},{"_id":"themes/fluid/scripts/filters/default-injects.js","hash":"b2013ae8e189cd07ebc8a2ff48a78e153345210f","modified":1721800613000},{"_id":"themes/fluid/scripts/filters/post-filter.js","hash":"82bb06686158ebe160a631c79f156cd4fde35656","modified":1721800613000},{"_id":"themes/fluid/scripts/filters/locals.js","hash":"58d0fec976f6b1d35e7ea03edc45414088acf05c","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/archive-list.ejs","hash":"7520fbf91f762207c2ab06b2c293235cd5b23905","modified":1721800613000},{"_id":"themes/fluid/.github/ISSUE_TEMPLATE/feature_request.md","hash":"c134dd57ffd269b93402ccfffe7dbe0f0b583bec","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/category-chains.ejs","hash":"18309584aab83bc4deb20723ebad832149dd2e24","modified":1721800613000},{"_id":"themes/fluid/.github/ISSUE_TEMPLATE/question.md","hash":"ab5eab9e3ff889c4ba7fd82846e7f5b7ae15bebc","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/comments.ejs","hash":"d707c47b2638c94e489bc43d4cfd098b7c58447f","modified":1721800613000},{"_id":"themes/fluid/.github/ISSUE_TEMPLATE/feature_request_zh.md","hash":"ed08574b196447376dd74411cca664ac9227a5d4","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/footer.ejs","hash":"40c8b0852873032e7aaef3f68e8ea08706cdef13","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/category-list.ejs","hash":"f8d2f1907450e61968e6d54443e9be8138196a77","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/css.ejs","hash":"1dadb118d580280524ed0a5f69bd34d234a92276","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/header.ejs","hash":"0d5e397d30051e5fbabe7b47cfd1f1e6a5820af1","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/head.ejs","hash":"67be642f99482c07904474f410cfbc2f99003288","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/scripts.ejs","hash":"da5810785105e5075861593c7ac22c7aa9665a72","modified":1721800613000},{"_id":"themes/fluid/source/css/highlight.styl","hash":"a9efc52a646a9e585439c768557e3e3c9e3326dc","modified":1721800613000},{"_id":"themes/fluid/source/css/highlight-dark.styl","hash":"45695ef75c31a4aa57324dd408b7e2327a337018","modified":1721800613000},{"_id":"themes/fluid/source/css/main.styl","hash":"855ae5fe229c51afa57f7645f6997a27a705d7e4","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/paginator.ejs","hash":"0f38a2c238169edcb63fc46c23bfc529ff3859b7","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/search.ejs","hash":"70e1c929e084ca8a2648cedabf29b372511ea2b8","modified":1721800613000},{"_id":"themes/fluid/source/css/gitalk.css","hash":"a57b3cc8e04a0a4a27aefa07facf5b5e7bca0e76","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/markdown-plugins.ejs","hash":"fc4bdf7de0cf1a66d0e5e4fba1b31d6f7ed49468","modified":1721800613000},{"_id":"themes/fluid/source/img/avatar.png","hash":"fe739a158cc128f70f780eb5fa96f388b81d478f","modified":1721800613000},{"_id":"themes/fluid/source/js/boot.js","hash":"38bd26c6b7acdafda86dda3560e6a3ca488d3c76","modified":1721800613000},{"_id":"themes/fluid/source/img/police_beian.png","hash":"90efded6baa2dde599a9d6b1387973e8e64923ea","modified":1721800613000},{"_id":"themes/fluid/source/img/loading.gif","hash":"2d2fc0f947940f98c21afafef39ecf226a2e8d55","modified":1721800613000},{"_id":"themes/fluid/source/js/color-schema.js","hash":"1ef88c881b9f942deadde3d890387b94c617342a","modified":1721800613000},{"_id":"themes/fluid/source/img/fluid.png","hash":"64b215db2cb3af98fe639e94537cb5209f959c78","modified":1721800613000},{"_id":"themes/fluid/source/js/img-lazyload.js","hash":"cbdeca434ec4da51f488c821d51b4d23c73294af","modified":1721800613000},{"_id":"themes/fluid/source/js/plugins.js","hash":"c34916291e392a774ff3e85c55badb83e8661297","modified":1721800613000},{"_id":"themes/fluid/source/js/utils.js","hash":"b82e7c289a66dfd36064470fd41c0e96fc598b43","modified":1721800613000},{"_id":"themes/fluid/scripts/events/lib/compatible-configs.js","hash":"ef474d1fa5bbafc52619ced0f9dc7eaf2affb363","modified":1721800613000},{"_id":"themes/fluid/source/js/events.js","hash":"6869811f67e4c3de3edfa4b08464bb242b97a402","modified":1721800613000},{"_id":"themes/fluid/source/js/umami-view.js","hash":"33c4b3883fa747604074ad3921606eeeaeb50716","modified":1721800613000},{"_id":"themes/fluid/scripts/events/lib/highlight.js","hash":"a5fe1deccb73b5f578797dbb11038efc15f63ce8","modified":1721800613000},{"_id":"themes/fluid/scripts/events/lib/hello.js","hash":"bd8376e1cf7892dc2daa58f2f443574be559fdbf","modified":1721800613000},{"_id":"themes/fluid/scripts/events/lib/footnote.js","hash":"c19ac8050b82c3676b0332a56099ccfcc36d9d52","modified":1721800613000},{"_id":"themes/fluid/scripts/events/lib/lazyload.js","hash":"9ba0d4bc224e22af8a5a48d6ff13e5a0fcfee2a4","modified":1721800613000},{"_id":"themes/fluid/source/xml/local-search.xml","hash":"8c96ba6a064705602ce28d096fd7dd9069630a55","modified":1721800613000},{"_id":"themes/fluid/scripts/events/lib/merge-configs.js","hash":"7c944c43b2ece5dd84859bd9d1fe955d13427387","modified":1721800613000},{"_id":"themes/fluid/scripts/events/lib/injects.js","hash":"5ae4b07204683e54b5a1b74e931702bbce2ac23e","modified":1721800613000},{"_id":"themes/fluid/source/js/local-search.js","hash":"b9945f76f8682f3ec32edfb285b26eb559f7b7e8","modified":1721800613000},{"_id":"themes/fluid/source/js/leancloud.js","hash":"eff77c7a5c399fcaefda48884980571e15243fc9","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/comments/changyan.ejs","hash":"c9b2d68ed3d375f1953e7007307d2a3f75ed6249","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/comments/discuss.ejs","hash":"98d065b58ce06b7d18bff3c974e96fa0f34ae03a","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/footer/beian.ejs","hash":"4fb9b5dd3f3e41a586d6af44e5069afe7c81fff2","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/footer/statistics.ejs","hash":"954a29b58d72647d20450da270b5d8fb2e0824f5","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/comments/giscus.ejs","hash":"95f8b866b158eff9352c381c243b332a155a5110","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/comments/disqus.ejs","hash":"aab4a4d24c55231a37db308ae94414319cecdd9b","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/comments/gitalk.ejs","hash":"843bc141a4545eb20d1c92fb63c85d459b4271ec","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/comments/valine.ejs","hash":"19ba937553dddd317f827d682661a1066a7b1f30","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/comments/waline.ejs","hash":"3d08c73b77e412d2f06a24d9344565fc7dbc76f8","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/comments/utterances.ejs","hash":"c7ccf7f28308334a6da6f5425b141a24b5eca0e2","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/comments/cusdis.ejs","hash":"5f9dc012be27040bbe874d0c093c0d53958cc987","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/comments/livere.ejs","hash":"2264758fed57542a7389c7aa9f00f1aefa17eb87","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/plugins/code-widget.ejs","hash":"3a505cba37942badf62a56bbb8b605b72af330aa","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/header/navigation.ejs","hash":"37d750428772d7c71ba36ce0c2540780d90fadea","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/plugins/analytics.ejs","hash":"e6dcbf1c2f56314d56bb46b50aca86ff68cacebd","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/header/banner.ejs","hash":"e07757b59e7b89eea213d0e595cb5932f812fd32","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/plugins/anchorjs.ejs","hash":"40181442d3a2b8734783a0ad7caf2d2522e3f2ab","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/plugins/encrypt.ejs","hash":"0fff24cf5bf99fbe5c56c292e2eac4a89bf29db4","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/comments/twikoo.ejs","hash":"d84bcb5ccd78470a60c067fc914ac0ac67ac8777","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/plugins/fancybox.ejs","hash":"9d1ea2a46b8c8ad8c168594d578f40764818ef13","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/comments/remark42.ejs","hash":"d4e9532feeb02aed61bd15eda536b5b631454dac","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/plugins/nprogress.ejs","hash":"4c2d39ce816b8a6dcd6b53113c8695f8bd650a23","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/plugins/highlight.ejs","hash":"7529dd215b09d3557804333942377b9e20fa554e","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/post/category-bar.ejs","hash":"8772bce97ed297e7a88523f4e939ed6436c22f87","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/plugins/math.ejs","hash":"dcbf9a381ee76f2f1f75fcbc22c50a502ec85023","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/post/sidebar-left.ejs","hash":"9992c99b3eb728ad195970e1b84d665f2c8691c4","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/post/copyright.ejs","hash":"cbfa32c5f5973133afd043853b24f8200455cb2d","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/plugins/mermaid.ejs","hash":"03ac02762f801970d1c4e73d6ec8d4c503780e50","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/plugins/typed.ejs","hash":"f345374885cd6a334f09a11f59c443b5d577c06c","modified":1721800613000},{"_id":"themes/fluid/source/css/_functions/base.styl","hash":"2e46f3f4e2c9fe34c1ff1c598738fc7349ae8188","modified":1721800613000},{"_id":"themes/fluid/source/css/_mixins/base.styl","hash":"542e306ee9494e8a78e44d6d7d409605d94caeb3","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/post/meta-bottom.ejs","hash":"375974ec017696e294dc12469fb0ae257800dc2d","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/plugins/moment.ejs","hash":"4ff3fb1b60ccc95a0af3bbdbd0757fedefc088b5","modified":1721800613000},{"_id":"themes/fluid/source/css/_pages/pages.styl","hash":"b8e887bc7fb3b765a1f8ec9448eff8603a41984f","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/post/toc.ejs","hash":"635a89060fbf72eeda066fc4bd0a97462f069417","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/post/sidebar-right.ejs","hash":"d5fcc9b60e02f869a29a8c17a16a6028ecc1e6d8","modified":1721800613000},{"_id":"themes/fluid/source/css/_pages/_about/about.styl","hash":"97fe42516ea531fdad771489b68aa8b2a7f6ae46","modified":1721800613000},{"_id":"themes/fluid/source/css/_variables/base.styl","hash":"4ed5f0ae105ef4c7dd92eaf652ceda176c38e502","modified":1721800613000},{"_id":"themes/fluid/source/css/_pages/_archive/archive.styl","hash":"c475e6681546d30350eaed11f23081ecae80c375","modified":1721800613000},{"_id":"themes/fluid/layout/_partials/post/meta-top.ejs","hash":"54dd479dbb440126e4ddd9d902229db5afaaae98","modified":1721800613000},{"_id":"themes/fluid/source/css/_pages/_base/color-schema.styl","hash":"85492ef64d7e5f70f0f7e46d570bbc911e686d7e","modified":1721800613000},{"_id":"themes/fluid/source/css/_pages/_base/base.styl","hash":"643284c567665f96915f0b64e59934dda315f74d","modified":1721800613000},{"_id":"themes/fluid/source/css/_pages/_category/category-chain.styl","hash":"0cdf7ef50dfd0669d3b257821384ff31cd81b7c9","modified":1721800613000},{"_id":"themes/fluid/source/css/_pages/_post/comment.styl","hash":"780f3788e7357bcd3f3262d781cb91bb53976a93","modified":1721800613000},{"_id":"themes/fluid/source/css/_pages/_base/inline.styl","hash":"411a3fa3f924a87e00ff04d18b5c83283b049a4d","modified":1721800613000},{"_id":"themes/fluid/source/css/_pages/_index/index.styl","hash":"25fb6fa4c783b847c632584c49a7e1593cdb2f5d","modified":1721800613000},{"_id":"themes/fluid/source/css/_pages/_links/links.styl","hash":"5c7f2044e3f1da05a3229537c06bd879836f8d6e","modified":1721800613000},{"_id":"themes/fluid/source/css/_pages/_category/category-list.styl","hash":"7edfe1b571ecca7d08f5f4dbcf76f4ffdcfbf0b5","modified":1721800613000},{"_id":"themes/fluid/source/css/_pages/_base/print.styl","hash":"166afbc596ea4b552bad7290ec372d25ec34db7b","modified":1721800613000},{"_id":"themes/fluid/source/css/_pages/_category/category-bar.styl","hash":"cc6df43fef6bb3efecbfdd8b9e467424a1dea581","modified":1721800613000},{"_id":"themes/fluid/source/css/_pages/_tag/tags.styl","hash":"65bfc01c76abc927fa1a23bf2422892b0d566c3f","modified":1721800613000},{"_id":"themes/fluid/source/css/_pages/_post/markdown.styl","hash":"1e3d3a82721e7c10bcfcecec6d81cf2979039452","modified":1721800613000},{"_id":"themes/fluid/source/css/_pages/_post/post-page.styl","hash":"7eee3f78296a3c81849a5415d1d43dcc6e03e6aa","modified":1721800613000},{"_id":"themes/fluid/source/css/_pages/_base/_widget/anchorjs.styl","hash":"e0cebda4a6f499aff75e71417d88caa7ceb13b94","modified":1721800613000},{"_id":"themes/fluid/source/css/_pages/_base/keyframes.styl","hash":"94065ea50f5bef7566d184f2422f6ac20866ba22","modified":1721800613000},{"_id":"themes/fluid/source/css/_pages/_base/_widget/banner.styl","hash":"7a0bd629bc234fc75e3cc8e3715ffada92f09e73","modified":1721800613000},{"_id":"themes/fluid/source/css/_pages/_base/_widget/code-widget.styl","hash":"b66ab013f0f37d724a149b85b3c7432afcf460ad","modified":1721800613000},{"_id":"themes/fluid/source/css/_pages/_base/_widget/board.styl","hash":"4397037fc3f0033dbe546c33cd9dbdabd8cb1632","modified":1721800613000},{"_id":"themes/fluid/source/css/_pages/_post/highlight.styl","hash":"4df764d298fe556e501db4afc2b05686fe6ebcfb","modified":1721800613000},{"_id":"themes/fluid/source/css/_pages/_base/_widget/ngrogress.styl","hash":"5d225357b4a58d46118e6616377168336ed44cb2","modified":1721800613000},{"_id":"themes/fluid/source/css/_pages/_base/_widget/footnote.styl","hash":"ae9289cc89649af2042907f8a003303b987f3404","modified":1721800613000},{"_id":"themes/fluid/source/css/_pages/_base/_widget/copyright.styl","hash":"26f71a9cd60d96bb0cb5bbdf58150b8e524d9707","modified":1721800613000},{"_id":"themes/fluid/source/css/_pages/_post/post-tag.styl","hash":"c96d36aa8fe20f0c3c1a29ee2473cd8064b10f73","modified":1721800613000},{"_id":"themes/fluid/source/css/_pages/_base/_widget/noscript.styl","hash":"0cf2f2bb44f456150d428016675d5876a9d2e2aa","modified":1721800613000},{"_id":"themes/fluid/source/css/_pages/_base/_widget/pagination.styl","hash":"8bb1b68e5f3552cb48c2ffa31edbc53646a8fb4c","modified":1721800613000},{"_id":"themes/fluid/source/css/_pages/_base/_widget/qrcode.styl","hash":"78704a94c0436097abfb0e0a57abeb3429c749b7","modified":1721800613000},{"_id":"themes/fluid/source/css/_pages/_base/_widget/header.styl","hash":"d42b748f2f49ef32aafb1a21d75991d2459da927","modified":1721800613000},{"_id":"themes/fluid/source/css/_pages/_base/_widget/footer.styl","hash":"2caaca71dd1ff63d583099ed817677dd267b457e","modified":1721800613000},{"_id":"themes/fluid/source/css/_pages/_base/_widget/modal.styl","hash":"adf6c1e5c8e1fb41c77ce6e2258001df61245aa2","modified":1721800613000},{"_id":"themes/fluid/source/css/_pages/_base/_widget/search.styl","hash":"10f7e91a91e681fb9fe46f9df7707b9ef78707c8","modified":1721800613000},{"_id":"source/_posts/soft-shadow/PCSS_ON.png","hash":"b5379d416aab14e2ce74b7530bcb5bf1ba803c7d","modified":1733630919694},{"_id":"themes/fluid/source/css/_pages/_base/_widget/scroll-btn.styl","hash":"f0e429a27fa8a7658fcbddbb4d4dbe4afa12499a","modified":1721800613000},{"_id":"source/_posts/digital-human-render-2/pbr_sample.png","hash":"e2691bdd78175373dc93f5a9598f13a210bd2430","modified":1735967712886},{"_id":"themes/fluid/source/css/_pages/_base/_widget/toc.styl","hash":"9e7452aa2372153f25d7a4675c9d36d281a65d24","modified":1721800613000},{"_id":"source/_posts/depth-of-field/dilate_after.png","hash":"c5e1bc9574c43cf344e177a7a0bbb501bc6eddb9","modified":1730127373710},{"_id":"source/_posts/single-scatter-atmosphere/single-scatter-atmosphere.mp4","hash":"cf48cd4f4e5bf60302882e4ab93cca403793b8eb","modified":1724773813109},{"_id":"source/_posts/screen-space-reflection/screen-space-reflection-in-kong.png","hash":"c6316de441e1ff2cc028805cf085b6425a364aea","modified":1733839777949},{"_id":"source/img/og_tinyGL.png","hash":"e64a65a8e7f65f77498e678481bfe0cc65645caf","modified":1730212141075},{"_id":"source/display-cabinet/og_tinyGL.png","hash":"e64a65a8e7f65f77498e678481bfe0cc65645caf","modified":1730212141075},{"_id":"source/_posts/screen-space-reflection/ssr_mugshot.png","hash":"7461c3996e0e6ccd85868a2deb1a1dd95babb6f4","modified":1734144111072},{"_id":"source/_posts/depth-of-field/dilate_before.png","hash":"c37d5697d706fcf09ae6ef0ac82f4b3abb278cf4","modified":1730127367993},{"_id":"source/_posts/cascade-shadow-map/sm_near.png","hash":"3a2e2ba049b19ee345a7944d5eed95bc9bec4d38","modified":1726475189766},{"_id":"source/_posts/depth-of-field/dof_near.png","hash":"13cf595da86662b7836a26beb4230a5fb2cdf58a","modified":1730128130810},{"_id":"source/_posts/depth-of-field/dof_far.png","hash":"a158cb25f92b20004de9a54390d55cbb760f114d","modified":1730128139308},{"_id":"themes/fluid/source/img/image.png","hash":"77d6c488036d5caa852b645e753011476668d168","modified":1707126287675},{"_id":"themes/fluid/source/img/default_.png","hash":"167a12978d80371cf578c8a2e45c24a2eb25b6fb","modified":1721800613000},{"_id":"source/_posts/soft-shadow/soft-shadow-thumbnail.png","hash":"e5dab3b924f72a07ba555bb00fea5e0abf4421df","modified":1733631141763},{"_id":"source/_posts/cascade-shadow-map/sm_far.png","hash":"4ebea9aeec6dfbdc98466a6fc2813949f8687f65","modified":1726475241147},{"_id":"source/_posts/ue-ai-texture-generation/run_texgen.png","hash":"91696ace11fe8b8a3e880dd354f277eb5748aab6","modified":1734862919017},{"_id":"source/_posts/defer-render/defer_render.png","hash":"674b2ebba5e97d292f2459382ebee9322476993b","modified":1725631498369},{"_id":"source/_posts/defer-render/no_defer_render.png","hash":"19969d958ab565da447389bae3bd72abfb8bd34f","modified":1725631457606},{"_id":"themes/fluid/source/img/default.png","hash":"2e799d8027ae3d05c60d6d5d5876a2e180da79be","modified":1726387615473},{"_id":"source/_posts/single-scatter-atmosphere/kong-screen-shot.png","hash":"f54c4ce1299d4e9f4ad89f97fdb86eafaa35ee3f","modified":1724575733127},{"_id":"source/_posts/cascade-shadow-map/csm_result.png","hash":"a29eba82d35e09260b7ec148c9c55c17cbfd05b4","modified":1726386766572},{"_id":"source/_posts/screen-space-reflection/ssr-step3.gif","hash":"c052a44123cffd14b0ced4ce1df8b566b4c66afe","modified":1733841505307},{"_id":"source/_posts/depth-of-field/dof_butterfly.JPG","hash":"171654a74a208937641a465da8c3d08453d734a7","modified":1693023971000},{"_id":"public/local-search.xml","hash":"75e11598016f36deaddcbd831fb17e302612a052","modified":1735972348204},{"_id":"public/about/index.html","hash":"98f9c5de030bc712f269862eb37b28ecf87bb555","modified":1735972348204},{"_id":"public/display-cabinet/index.html","hash":"733b91446437dbb94b73faacdd31c65bd8f6cc9d","modified":1735972348204},{"_id":"public/2025/01/04/digital-human-render-2/index.html","hash":"31908ac29a1e9af604af35ae2f4a44a4cf8d4d9a","modified":1735972348204},{"_id":"public/2024/12/28/digital-human-render-1/index.html","hash":"10ed69b747bf9f7b9cad01c780139559bfb3e8eb","modified":1735972348204},{"_id":"public/2024/12/22/ue-ai-texture-generation/index.html","hash":"3f255e7c38b3e3f31bb5fd29b0b1c9b949d5733c","modified":1735972348204},{"_id":"public/2024/12/10/screen-space-reflection/index.html","hash":"4fe5551b21f93d5823c8b0dcd00f6e26399e51e3","modified":1735972348204},{"_id":"public/2024/12/08/soft-shadow/index.html","hash":"c624e593e1836f26919f334e89bc95b3f280a698","modified":1735972348204},{"_id":"public/2024/11/24/reflective-shadow-map/index.html","hash":"031262a7bb185d1ea8c4487be4fda45b95e73f9b","modified":1735972348204},{"_id":"public/2024/11/19/ProceduralTerrainGeneration2-optimize/index.html","hash":"75f260054a153a4b1e2556f54e36dfec7da8f046","modified":1735972348204},{"_id":"public/2024/11/04/ProceduralTerrainGeneration2/index.html","hash":"6e8f51dd3751db9ab7e1a81377193bc593f9e83f","modified":1735972348204},{"_id":"public/2024/10/28/depth-of-field/index.html","hash":"1b78ed4d420ddfef34b09bf74c9c6557621476bc","modified":1735972348204},{"_id":"public/2024/10/19/defer-render/index.html","hash":"b7bd27c68b270df78113fdc6caf27fd6c061bcfd","modified":1735972348204},{"_id":"public/2024/10/15/single-scatter-atmosphere/index.html","hash":"947c1f3833667d6c5dcd74f44fb2e23e89aad1de","modified":1735972348204},{"_id":"public/2024/10/13/cascade-shadow-map/index.html","hash":"2ed0eef3576b150ae9b5f552681f68d8593de45d","modified":1735972348204},{"_id":"public/2024/10/11/ProceduralTerrainGeneration/index.html","hash":"7fc0410250a6a9bf89af87a80da4b367f3d6969f","modified":1735972348204},{"_id":"public/2024/10/09/StartMyBlog/index.html","hash":"efed617d1f351388c2577d50cb0cc5becf2cd874","modified":1735972348204},{"_id":"public/archives/index.html","hash":"7380f4ae56527db4fb7a7d25b45a6f80ffcc42ff","modified":1735972348204},{"_id":"public/archives/page/2/index.html","hash":"f843e65b6406cb6d8ff947704c40a890288fcca4","modified":1735972348204},{"_id":"public/archives/2024/index.html","hash":"bd374afbd51abb969d1f5ce982be9debdfb1049a","modified":1735972348204},{"_id":"public/archives/2024/page/2/index.html","hash":"a39ea00d4ad43991ad169b0d413b52b26628de26","modified":1735972348204},{"_id":"public/archives/2024/10/index.html","hash":"9512dd55b25abcae24a9b5c786d8fd559d4ec6dd","modified":1735972348204},{"_id":"public/archives/2024/11/index.html","hash":"07a3f4122a1b7968204979898a8eaf079000b4bb","modified":1735972348204},{"_id":"public/archives/2024/12/index.html","hash":"88cacd2864500e38d3a3f25654cfadd746451b2b","modified":1735972348204},{"_id":"public/archives/2025/index.html","hash":"a714a0e3071a366334a5fb6d92bac8c6d6d25ce5","modified":1735972348204},{"_id":"public/archives/2025/01/index.html","hash":"8149d24480a5144d91cf6f7f8fa13e2307c9952c","modified":1735972348204},{"_id":"public/categories/技术漫谈/page/2/index.html","hash":"561954db4619f09faca284bf60244b617663f6cc","modified":1735972348204},{"_id":"public/index.html","hash":"a04937ba08bbf329a30f18f91f311344f3353a95","modified":1735972348204},{"_id":"public/categories/技术漫谈/index.html","hash":"87ea91d69dc14a21edc19642cd36736ff7a70590","modified":1735972348204},{"_id":"public/categories/生活杂谈/index.html","hash":"9bb9e24ca9a52680600452b5f7219c9cd8dba012","modified":1735972348204},{"_id":"public/page/2/index.html","hash":"2ea6a338bfa69fe46b789fd36bb442a87612fc12","modified":1735972348204},{"_id":"public/tags/3D/index.html","hash":"fa101340352b97d16b7b27e7b1509d7c4068164c","modified":1735972348204},{"_id":"public/tags/3D/page/2/index.html","hash":"e9944ea4788524f022495f53079d6921776f7a64","modified":1735972348204},{"_id":"public/tags/render/index.html","hash":"a12c8c62a86e1505173f307b02e8b64f294f3696","modified":1735972348204},{"_id":"public/tags/渲染/index.html","hash":"4e526054b7f3dceaf94c2f46c1c762c07becb307","modified":1735972348204},{"_id":"public/tags/编程/index.html","hash":"75111b9f93802d7595f89fe46e93c39efc0bf5ae","modified":1735972348204},{"_id":"public/tags/渲染/page/2/index.html","hash":"fbd283cdd47f29e663e467463f2e7609b4238a42","modified":1735972348204},{"_id":"public/tags/程序化生成/index.html","hash":"1084dfb8255a87c04d0379b25901c908b2f1f9a6","modified":1735972348204},{"_id":"public/tags/生活/index.html","hash":"9fb2334a9d9c7e819d782d2516719bd2b2056451","modified":1735972348204},{"_id":"public/tags/数字孪生/index.html","hash":"51b5dfb7eae1cdc623375389317f4e422736bbce","modified":1735972348204},{"_id":"public/tags/AIGC/index.html","hash":"bec8e845c5576dd98f642abe4a4393170aebc747","modified":1735972348204},{"_id":"public/404.html","hash":"eb1938515f55ccc12b35944c7322dde683b69425","modified":1735972348204},{"_id":"public/tags/虚幻引擎/index.html","hash":"9b31882efb8c244571fd4aa5c6498c8b31da36d0","modified":1735972348204},{"_id":"public/tags/阴影/index.html","hash":"dfd2041d31d98dee5112dc2d07351189596bb043","modified":1735972348204},{"_id":"public/tags/index.html","hash":"acb9be296aa4adc2b0c32e8c764619509503e131","modified":1735972348204},{"_id":"public/categories/index.html","hash":"c756eb46cba0cfd11b3fac7ea6aad6019a051f42","modified":1735972348204},{"_id":"public/links/index.html","hash":"a9485d639952ec6a870a4d5891ccce237de389dc","modified":1735972348204},{"_id":"public/img/avatar.png","hash":"fe739a158cc128f70f780eb5fa96f388b81d478f","modified":1735972348204},{"_id":"public/img/police_beian.png","hash":"90efded6baa2dde599a9d6b1387973e8e64923ea","modified":1735972348204},{"_id":"public/img/loading.gif","hash":"2d2fc0f947940f98c21afafef39ecf226a2e8d55","modified":1735972348204},{"_id":"public/img/fluid.png","hash":"64b215db2cb3af98fe639e94537cb5209f959c78","modified":1735972348204},{"_id":"public/xml/local-search.xml","hash":"8c96ba6a064705602ce28d096fd7dd9069630a55","modified":1735972348204},{"_id":"public/2024/12/28/digital-human-render-1/metahuman_mugshot.jpg","hash":"fe9f8c1b0dbd17293c28066e4c5b4a5ff067d5bb","modified":1735972348204},{"_id":"public/2025/01/04/digital-human-render-2/diffuse-comparison.png","hash":"7b89cf35af0cc6373fc00cb16665cd898ad55c53","modified":1735972348204},{"_id":"public/2025/01/04/digital-human-render-2/layers_of_skin.png","hash":"138537a14451e6f3aacdcdfa68cdb01d08376bac","modified":1735972348204},{"_id":"public/2025/01/04/digital-human-render-2/specular-lobe.png","hash":"e0cfe780b8f0c9b31e47a179a1d45ac7e468acd8","modified":1735972348204},{"_id":"public/2024/10/13/cascade-shadow-map/csm_far.png","hash":"d6f8dce239df0ca3fb8008e6beea34d08fee464a","modified":1735972348204},{"_id":"public/2024/10/13/cascade-shadow-map/csm_mid.png","hash":"886730be4cb07cf725f965acf50bd8dbe28ba87e","modified":1735972348204},{"_id":"public/2024/10/13/cascade-shadow-map/csm_near.png","hash":"e575a785c09d4c192db4c0e094d919f88e31029a","modified":1735972348204},{"_id":"public/2024/12/08/soft-shadow/get-d_blocker-1.png","hash":"76a930f535d50a0806c3e9d12ceb4c81c3766882","modified":1735972348204},{"_id":"public/2024/12/08/soft-shadow/umbra-contrast.png","hash":"3fa9fcf1cf3786dbb7ef4dcd7cd1b7f43d0ba55b","modified":1735972348204},{"_id":"public/2024/12/08/soft-shadow/umbra-principle.png","hash":"870ce6426dc622ee5bab3e83e01ae9a3b4362c6c","modified":1735972348204},{"_id":"public/2024/12/10/screen-space-reflection/ssr_under_sample.png","hash":"01f5ea68a28df4bb068ea72af5804f1f841458d3","modified":1735972348204},{"_id":"public/2024/10/11/ProceduralTerrainGeneration/calc_soft_shadow.png","hash":"7bc24f450d820326f7c05fc625259f59481db0db","modified":1735972348204},{"_id":"public/2024/12/28/digital-human-render-1/初音未来.png","hash":"c1515afbc6641e879d8528f32aa870f18690aa9c","modified":1735972348204},{"_id":"public/2024/12/28/digital-human-render-1/虚拟偶像市场规模.png","hash":"dbd45b900479559e9683a121d7d4aca343216761","modified":1735972348204},{"_id":"public/2025/01/04/digital-human-render-2/reflectance-lookup-table.png","hash":"6694dd988401383fb864e4b599fff41385096ed2","modified":1735972348204},{"_id":"public/2024/10/19/defer-render/ssao.png","hash":"b208b05bfddb222235f6617fd22ae6d272f861cb","modified":1735972348204},{"_id":"public/2024/11/24/reflective-shadow-map/rsm_principle_2.png","hash":"9d502e79a04738d234249fb13c3ff9e287f90f28","modified":1735972348204},{"_id":"public/2024/12/22/ue-ai-texture-generation/texgen_bp_macro.png","hash":"076be898d4349c32f0e2e150212afce8681263bc","modified":1735972348204},{"_id":"public/2024/12/08/soft-shadow/get-d_blocker-2.png","hash":"d90513508242415a0cad2cb0cce6cc8787779660","modified":1735972348204},{"_id":"public/2024/12/10/screen-space-reflection/ssr_over_sample.png","hash":"7ad4eefe6ed3c9eac57f3887f4354ca22ba19e27","modified":1735972348204},{"_id":"public/css/gitalk.css","hash":"a57b3cc8e04a0a4a27aefa07facf5b5e7bca0e76","modified":1735972348204},{"_id":"public/css/highlight-dark.css","hash":"902294bada4323c0f51502d67cba8c3a0298952f","modified":1735972348204},{"_id":"public/css/main.css","hash":"14ebd9b515085666cee29bbcbe362ad3604ab62a","modified":1735972348204},{"_id":"public/css/highlight.css","hash":"04d4ddbb5e1d1007447c2fe293ee05aae9b9563e","modified":1735972348204},{"_id":"public/js/boot.js","hash":"38bd26c6b7acdafda86dda3560e6a3ca488d3c76","modified":1735972348204},{"_id":"public/js/color-schema.js","hash":"1ef88c881b9f942deadde3d890387b94c617342a","modified":1735972348204},{"_id":"public/js/events.js","hash":"6869811f67e4c3de3edfa4b08464bb242b97a402","modified":1735972348204},{"_id":"public/js/plugins.js","hash":"c34916291e392a774ff3e85c55badb83e8661297","modified":1735972348204},{"_id":"public/js/leancloud.js","hash":"eff77c7a5c399fcaefda48884980571e15243fc9","modified":1735972348204},{"_id":"public/js/umami-view.js","hash":"33c4b3883fa747604074ad3921606eeeaeb50716","modified":1735972348204},{"_id":"public/js/utils.js","hash":"b82e7c289a66dfd36064470fd41c0e96fc598b43","modified":1735972348204},{"_id":"public/js/local-search.js","hash":"b9945f76f8682f3ec32edfb285b26eb559f7b7e8","modified":1735972348204},{"_id":"public/js/img-lazyload.js","hash":"cbdeca434ec4da51f488c821d51b4d23c73294af","modified":1735972348204},{"_id":"public/2024/12/28/digital-human-render-1/老黄驱动.png","hash":"15d95c9e56afed5c007f286ceb289bd8d0e8864c","modified":1735972348204},{"_id":"public/2024/12/28/digital-human-render-1/林明美.png","hash":"495e8e6a0bf5de115d2aecc64c0ec78977666ee9","modified":1735972348204},{"_id":"public/2024/12/28/digital-human-render-1/老黄建模.png","hash":"5fc3ea52711df5b194732514cf9987a0d89421ad","modified":1735972348204},{"_id":"public/2025/01/04/digital-human-render-2/BSSRDF.png","hash":"cbd4bf8293e461e85f7c70e0f5e9bf76fc309551","modified":1735972348204},{"_id":"public/2025/01/04/digital-human-render-2/BTDF.png","hash":"4653f52fc9136d61d74133c56c93d1c52ab99e60","modified":1735972348204},{"_id":"public/2025/01/04/digital-human-render-2/texture-space-blur.png","hash":"3de671f74ee27649284ff301c34ba92744ca67c2","modified":1735972348204},{"_id":"public/2025/01/04/digital-human-render-2/screen-space-blur.png","hash":"2252754914ddbe14f09d136e9bd99b2ff0b54666","modified":1735972348204},{"_id":"public/2024/11/24/reflective-shadow-map/rsm_info.png","hash":"9e4355069be01e0ff6af9bc41a2d0e3d9ae966ab","modified":1735972348204},{"_id":"public/2024/12/10/screen-space-reflection/ssr_ss_sample.png","hash":"215ec66b0f42f10afb4e798f142a88e829dc7c1c","modified":1735972348204},{"_id":"public/2024/10/11/ProceduralTerrainGeneration/shadertoy_oc5_noshadow.png","hash":"64c2e7528dfe87ea1e450decd23c569918479c18","modified":1735972348204},{"_id":"public/2024/10/11/ProceduralTerrainGeneration/shadertoy_terrain_oct5.png","hash":"0f4daabfa7d24bf09919beb71c0701d1c9c7e524","modified":1735972348204},{"_id":"public/2024/12/28/digital-human-render-1/老黄模型渲染.png","hash":"5ead1685c64d87b187b52e37cf1cfd63f9b75644","modified":1735972348204},{"_id":"public/2025/01/04/digital-human-render-2/BSSRDF-1.png","hash":"5a296e452304352d836d596b9fc9290b3f5152bb","modified":1735972348204},{"_id":"public/2025/01/04/digital-human-render-2/Cook-Torrance-BRDF.png","hash":"89f038aa54b93f5225b0d5bb57492b174aa4968a","modified":1735972348204},{"_id":"public/2025/01/04/digital-human-render-2/skin-specular.png","hash":"1cd412abb03b7a925e43b3f54b61f44220f88386","modified":1735972348204},{"_id":"public/2024/11/24/reflective-shadow-map/rsm_off_sphere.png","hash":"7f1d0e24a55b55f1b91e6b3eb919f6b444abec54","modified":1735972348204},{"_id":"public/2024/12/08/soft-shadow/penumbra.png","hash":"954283c20a01c51408cf2b1d762e96ccd89bbb3a","modified":1735972348204},{"_id":"public/2024/12/28/digital-human-render-1/siren.png","hash":"787586bf5f2a20f97674bbc62697cfc8f16adbaa","modified":1735972348204},{"_id":"public/2024/10/28/depth-of-field/game1.jpg","hash":"0034e1ebc0e270dca6ea9c6fc878f3889d54e091","modified":1735972348204},{"_id":"public/2024/11/24/reflective-shadow-map/rsm_off_suit.png","hash":"3c6d18b25f8f84dba48b1ef8911bc9a745e222ce","modified":1735972348204},{"_id":"public/2024/11/24/reflective-shadow-map/rsm_on_sphere.png","hash":"72ea15cd39fff2d6d343268bede9e1c95bab94e8","modified":1735972348204},{"_id":"public/2024/12/08/soft-shadow/normal-shadow.png","hash":"7ab8c46a962e071b2bfb011ff9572369f070df1e","modified":1735972348204},{"_id":"public/2024/11/24/reflective-shadow-map/rsm_sample.png","hash":"be7dffe23b9e4e42b7612026466fb70fc0c1dca2","modified":1735972348204},{"_id":"public/img/image.png","hash":"77d6c488036d5caa852b645e753011476668d168","modified":1735972348204},{"_id":"public/2024/10/11/ProceduralTerrainGeneration/shadertoy_terrain.png","hash":"56cc9ca4c38d9dea545c3fc71e930ff792e46fac","modified":1735972348204},{"_id":"public/2024/10/11/ProceduralTerrainGeneration/shadertoy_oc11_hardshadow.png","hash":"4c4b64ea717fa659999b44c3218fe77f304e1d12","modified":1735972348204},{"_id":"public/2024/11/04/ProceduralTerrainGeneration2/terrain_no_grass.png","hash":"6fd3d29360d763f99afc976dbec8ca77c4edcf2c","modified":1735972348204},{"_id":"public/2024/12/28/digital-human-render-1/老黄扫描.png","hash":"93cbeb7dc2b112e31ac849509c4a8cb5ed2831d3","modified":1735972348204},{"_id":"public/2025/01/04/digital-human-render-2/blur-camparison.png","hash":"3cc296ca2e21844a0c7f430a31142dcfc43cf355","modified":1735972348204},{"_id":"public/2025/01/04/digital-human-render-2/ue-skin.png","hash":"212e6ff77cfd78d459a0f7878a46b4f47677c51b","modified":1735972348204},{"_id":"public/2024/11/24/reflective-shadow-map/rsm_on_suit.png","hash":"aebb00ccc8dab9d657d10a260d069ddf51c4d455","modified":1735972348204},{"_id":"public/2024/11/24/reflective-shadow-map/rsm_principle_1.png","hash":"198f7461830637664bb7ebb7fc40100fe9ed41d1","modified":1735972348204},{"_id":"public/2024/12/08/soft-shadow/real_shadow.png","hash":"c2e7f5a8f5eb40448b3b1028d4369fdc5a471bc6","modified":1735972348204},{"_id":"public/2024/12/08/soft-shadow/pcf-shadow-3.png","hash":"5b0473c8a5106943b16ee20c19e6b5b028015ce9","modified":1735972348204},{"_id":"public/img/default_.png","hash":"167a12978d80371cf578c8a2e45c24a2eb25b6fb","modified":1735972348204},{"_id":"public/2024/10/11/ProceduralTerrainGeneration/shadertoy_oc11_noshadow.png","hash":"48d0f8a6b7cd0289041b3bea39c24782aaf930f7","modified":1735972348204},{"_id":"public/2024/11/04/ProceduralTerrainGeneration2/terrain_with_grass.png","hash":"ecbed0af037da143c48b1dc2a52c3ed6d1569808","modified":1735972348204},{"_id":"public/2024/12/28/digital-human-render-1/metahuman面部驱动.png","hash":"ae02dd9c189215b55e799746d92bc3963c2865b4","modified":1735972348204},{"_id":"public/2024/11/04/ProceduralTerrainGeneration2/terrain_with_sky_fog.png","hash":"dc9fa37962207b51ec4b3659057a2eccc49f6bb0","modified":1735972348204},{"_id":"public/2024/11/04/ProceduralTerrainGeneration2/terrain_with_sky_fog_less_ambient.png","hash":"819633b9a30cd7a285f4a8f3fecd454bee8468d4","modified":1735972348204},{"_id":"public/2024/11/04/ProceduralTerrainGeneration2/terrain_with_sky.png","hash":"25bc38e72b9bde4ea69bbde7a25029becdf3a757","modified":1735972348204},{"_id":"public/2024/12/08/soft-shadow/pcf-shadow-1.png","hash":"ca79acf3405964cf1c55d79dc70247d9253296aa","modified":1735972348204},{"_id":"public/2024/12/10/screen-space-reflection/ssr-step4.gif","hash":"99869235c0ebaee933152c538e8aca432e0e7bca","modified":1735972348204},{"_id":"public/2024/10/15/single-scatter-atmosphere/single-scatter-atmosphere.png","hash":"7266482600228204c32e0fa281e85f626baf7773","modified":1735972348204},{"_id":"public/2024/10/19/defer-render/defer_render_banner.png","hash":"1962262b49a5eff662ac3a76846d4f6414660bc8","modified":1735972348204},{"_id":"public/2024/12/10/screen-space-reflection/ssr_result.png","hash":"6c9bffb3ff2b88a4d5ea119642a40350445a8df2","modified":1735972348204},{"_id":"public/2024/12/10/screen-space-reflection/ssr_normal_s32.png","hash":"cec7935113cf6206898b8b5dfe539e46d9e497eb","modified":1735972348204},{"_id":"public/2024/12/10/screen-space-reflection/ssr_sample_twice.png","hash":"b18cab4ec8ab1d9147b04aefd2caa85372561f2e","modified":1735972348204},{"_id":"public/2024/11/04/ProceduralTerrainGeneration2/terrain_with_sky_fog_diffuse_from_sky.png","hash":"8b3ddaad30616a7b0b5c606aa4bf89b43afb7192","modified":1735972348204},{"_id":"public/2024/11/04/ProceduralTerrainGeneration2/terrain_with_sky_fog_diffuse_from_mountain.png","hash":"82daa3b5403c88308c8fae59330d9a9c008b24aa","modified":1735972348204},{"_id":"public/2024/12/22/ue-ai-texture-generation/texgen_bp.png","hash":"f2e1fa65b29332d04311094be7422a9a9edbc660","modified":1735972348204},{"_id":"public/2024/12/22/ue-ai-texture-generation/texgen_cmd.png","hash":"de1b6ef7191ee713ba3e4e769437db4e1dce0a2a","modified":1735972348204},{"_id":"public/2024/12/10/screen-space-reflection/ssr_normal_s128.png","hash":"513a2cb22ce5c11e230984bff14d5388a0495da2","modified":1735972348204},{"_id":"public/2024/12/08/soft-shadow/PCSS_OFF.png","hash":"0694fc4bf6a06f3f52ec4329ceefc874efca5c54","modified":1735972348204},{"_id":"public/2024/11/04/ProceduralTerrainGeneration2/terrain_with_all_cloud.png","hash":"89318f0a3a168ac5b9cbfde0a729f45e6d918d80","modified":1735972348204},{"_id":"public/2024/11/04/ProceduralTerrainGeneration2/terrain_with_high_cloud.png","hash":"17f1b24ce2e0e425166da356780fdd6e9c38343c","modified":1735972348204},{"_id":"public/2024/12/08/soft-shadow/PCSS_ON.png","hash":"b5379d416aab14e2ce74b7530bcb5bf1ba803c7d","modified":1735972348204},{"_id":"public/2025/01/04/digital-human-render-2/pbr_sample.png","hash":"e2691bdd78175373dc93f5a9598f13a210bd2430","modified":1735972348204},{"_id":"public/2024/10/28/depth-of-field/dilate_after.png","hash":"c5e1bc9574c43cf344e177a7a0bbb501bc6eddb9","modified":1735972348204},{"_id":"public/2024/10/15/single-scatter-atmosphere/single-scatter-atmosphere.mp4","hash":"cf48cd4f4e5bf60302882e4ab93cca403793b8eb","modified":1735972348204},{"_id":"public/2024/12/10/screen-space-reflection/screen-space-reflection-in-kong.png","hash":"c6316de441e1ff2cc028805cf085b6425a364aea","modified":1735972348204},{"_id":"public/2024/12/10/screen-space-reflection/ssr_mugshot.png","hash":"7461c3996e0e6ccd85868a2deb1a1dd95babb6f4","modified":1735972348204},{"_id":"public/display-cabinet/og_tinyGL.png","hash":"e64a65a8e7f65f77498e678481bfe0cc65645caf","modified":1735972348204},{"_id":"public/img/og_tinyGL.png","hash":"e64a65a8e7f65f77498e678481bfe0cc65645caf","modified":1735972348204},{"_id":"public/2024/10/28/depth-of-field/dilate_before.png","hash":"c37d5697d706fcf09ae6ef0ac82f4b3abb278cf4","modified":1735972348204},{"_id":"public/2024/10/13/cascade-shadow-map/sm_near.png","hash":"3a2e2ba049b19ee345a7944d5eed95bc9bec4d38","modified":1735972348204},{"_id":"public/2024/10/28/depth-of-field/dof_far.png","hash":"a158cb25f92b20004de9a54390d55cbb760f114d","modified":1735972348204},{"_id":"public/2024/10/28/depth-of-field/dof_near.png","hash":"13cf595da86662b7836a26beb4230a5fb2cdf58a","modified":1735972348204},{"_id":"public/img/default.png","hash":"2e799d8027ae3d05c60d6d5d5876a2e180da79be","modified":1735972348204},{"_id":"public/2024/12/08/soft-shadow/soft-shadow-thumbnail.png","hash":"e5dab3b924f72a07ba555bb00fea5e0abf4421df","modified":1735972348204},{"_id":"public/2024/10/13/cascade-shadow-map/sm_far.png","hash":"4ebea9aeec6dfbdc98466a6fc2813949f8687f65","modified":1735972348204},{"_id":"public/2024/12/22/ue-ai-texture-generation/run_texgen.png","hash":"91696ace11fe8b8a3e880dd354f277eb5748aab6","modified":1735972348204},{"_id":"public/2024/10/19/defer-render/defer_render.png","hash":"674b2ebba5e97d292f2459382ebee9322476993b","modified":1735972348204},{"_id":"public/2024/10/19/defer-render/no_defer_render.png","hash":"19969d958ab565da447389bae3bd72abfb8bd34f","modified":1735972348204},{"_id":"public/2024/10/15/single-scatter-atmosphere/kong-screen-shot.png","hash":"f54c4ce1299d4e9f4ad89f97fdb86eafaa35ee3f","modified":1735972348204},{"_id":"public/2024/10/13/cascade-shadow-map/csm_result.png","hash":"a29eba82d35e09260b7ec148c9c55c17cbfd05b4","modified":1735972348204},{"_id":"public/2024/12/10/screen-space-reflection/ssr-step3.gif","hash":"c052a44123cffd14b0ced4ce1df8b566b4c66afe","modified":1735972348204},{"_id":"public/2024/10/28/depth-of-field/dof_butterfly.JPG","hash":"171654a74a208937641a465da8c3d08453d734a7","modified":1735972348204}],"Category":[{"name":"技术漫谈","_id":"cm5ht44ve00043457e31s9pfj"},{"name":"生活杂谈","_id":"cm5ht44vj000e3457gn1x7npn"}],"Data":[],"Page":[{"title":"about","layout":"about","_content":"\n你好，这里是RC。\n\n我是一位有着多年游戏行业工作经验的“小白”，做过不少自研引擎和UE相关的工作。\n\n在这个小小的网站空间里，我想把自己在生活和学习中的一些小碎片记录下来，可能是技术上的实践和思考，也可能只是生活中的小小杂念或者是感动。\n\n希望在如白驹过隙的时光，偷偷留下我的几份痕迹。","source":"about/index.md","raw":"---\ntitle: about\nlayout: about\n---\n\n你好，这里是RC。\n\n我是一位有着多年游戏行业工作经验的“小白”，做过不少自研引擎和UE相关的工作。\n\n在这个小小的网站空间里，我想把自己在生活和学习中的一些小碎片记录下来，可能是技术上的实践和思考，也可能只是生活中的小小杂念或者是感动。\n\n希望在如白驹过隙的时光，偷偷留下我的几份痕迹。","date":"2024-10-29T14:32:24.736Z","updated":"2024-10-29T14:32:24.736Z","path":"about/index.html","comments":1,"_id":"cm5ht44v8000034579ms377jp","content":"<p>你好，这里是RC。</p>\n<p>我是一位有着多年游戏行业工作经验的“小白”，做过不少自研引擎和UE相关的工作。</p>\n<p>在这个小小的网站空间里，我想把自己在生活和学习中的一些小碎片记录下来，可能是技术上的实践和思考，也可能只是生活中的小小杂念或者是感动。</p>\n<p>希望在如白驹过隙的时光，偷偷留下我的几份痕迹。</p>\n","excerpt":"","more":"<p>你好，这里是RC。</p>\n<p>我是一位有着多年游戏行业工作经验的“小白”，做过不少自研引擎和UE相关的工作。</p>\n<p>在这个小小的网站空间里，我想把自己在生活和学习中的一些小碎片记录下来，可能是技术上的实践和思考，也可能只是生活中的小小杂念或者是感动。</p>\n<p>希望在如白驹过隙的时光，偷偷留下我的几份痕迹。</p>\n"},{"title":"展示窗","subtitle":"我的作品展示","date":"2024-10-29T14:01:23.000Z","_content":"\n这里是展示窗，是我showcase一些自己的成果的地方，包括代码、github链接等等各种各样的小玩意。\n\n<div class=\"markdown-body\">\n\n# ShaderToy\nShaderToy是一个很有趣的网站，它上面分享了很多实现很酷炫效果的着色器代码。我在上面也有一些分享，下面展示的是我认为可能还算不错作品，或者是对我有意义的学习内容。\n\n## 大气渲染\n\n<iframe width=\"640\" height=\"360\" frameborder=\"0\" src=\"https://www.shadertoy.com/embed/XXBcRR?gui=true&t=10&paused=true&muted=false\" allowfullscreen></iframe>\n\n这个demo是天空大气的渲染，基于单次散射模型（有一篇文章有提到过）。是个挺有趣的小例子，修改Shader代码可以尝试将相机抬升，将会展示在外太空看向地球的大气效果。\n\n\n## 简单海平面\n\n<iframe width=\"640\" height=\"360\" frameborder=\"0\" src=\"https://www.shadertoy.com/embed/X3ByDD?gui=true&t=10&paused=true&muted=false\" allowfullscreen></iframe>\n\n这个demo稍微复杂了那么一丢丢，它是由上面的天空大气的demo增加了一些细节而来。云和海水的纹理都是通过柏林噪声来生成的（这就是图形编程的魅力所在吧，仅仅通过代码就能生成逼真的画面）。\n\n流程大概是先计算天空大气的颜色；然后通过噪音生成3D空间的云的密度函数，通过采样一定范围的空气密度来在画面的上半部分渲染云。渲染完成后将云沿着横轴翻转，再配合上噪音生成的海水波纹。\n\n\n## 程序化地形\n\n<iframe width=\"640\" height=\"360\" frameborder=\"0\" src=\"https://www.shadertoy.com/embed/4XByRV?gui=true&t=10&paused=true&muted=false\" allowfullscreen></iframe>\n\n这个demo的灵感来源于Inigo大佬的一篇教程：https://www.shadertoy.com/view/4ttSWf。\n\n和教程有所差异的是我选择了使用无线地形的生成，而不是一个固定的角度。\n\n从效果来看是比较震撼的，不过性能上还是有诸多问题，raymarch地形在现在看来还是比较难做到高性能和高质量检具的结果。\n\n另外云（高层和低层）的效果我并不是特别满意，后续还想改成更为复杂一点的效果，并且加上阴影等。\n\n## domain warp\n\n<iframe width=\"640\" height=\"360\" frameborder=\"0\" src=\"https://www.shadertoy.com/embed/lXfBWX?gui=true&t=10&paused=true&muted=false\" allowfullscreen></iframe>\n\n这个demo是用于学习domain warp的例子。domain warp实际上就是将FBM的输出作为另一个FBM的输入，调整合适的参数和迭代后得到的很特殊的效果。这个demo里面我默认将domain wrap的结果作为raymarch sphere的材质。\n\n## flow effect\n\n<iframe width=\"640\" height=\"360\" frameborder=\"0\" src=\"https://www.shadertoy.com/embed/MXffDX?gui=true&t=10&paused=true&muted=false\" allowfullscreen></iframe>\n\n这个demo和上面的类似，是我学习flow效果的一个练手作品。\n其实现过程是用FBM计算出平面上某个点的流动方向f，并在0帧初始化最初的颜色后，然后从往后的每一帧，每个像素的颜色都遵循f的方向流动并记录成材质，循环往复。\n\n另外这个flow上的阴影效果是根据颜色的blue值来决定的，只是调了一个好看的效果而已。\n\n\n# GitHub\n这里是我做的一些放在GitHub上面的工程。\n\n## KongEngine\nhttps://github.com/ruochenhua/KongEngine\n\nKongEngine是我在2019年开始的一个基于OpenGL的渲染项目，但是后面由于各种原因更新了没有多少功能就弃置了，在2020年提交了最后一个commit就没有动静，最终的效果也如下图所示。\n\n![2020年的最后一次提交](/img/og_tinyGL.png)\n\n\n2024年我重新拾起这个项目的开发，并在原来非常简单的渲染效果上增加了很多额外的功能，这也是我希望我能够一直进行下去的项目。\n\n</div>","source":"display-cabinet/index.md","raw":"---\ntitle: 展示窗\nsubtitle: 我的作品展示\ndate: 2024-10-29 22:01:23\n---\n\n这里是展示窗，是我showcase一些自己的成果的地方，包括代码、github链接等等各种各样的小玩意。\n\n<div class=\"markdown-body\">\n\n# ShaderToy\nShaderToy是一个很有趣的网站，它上面分享了很多实现很酷炫效果的着色器代码。我在上面也有一些分享，下面展示的是我认为可能还算不错作品，或者是对我有意义的学习内容。\n\n## 大气渲染\n\n<iframe width=\"640\" height=\"360\" frameborder=\"0\" src=\"https://www.shadertoy.com/embed/XXBcRR?gui=true&t=10&paused=true&muted=false\" allowfullscreen></iframe>\n\n这个demo是天空大气的渲染，基于单次散射模型（有一篇文章有提到过）。是个挺有趣的小例子，修改Shader代码可以尝试将相机抬升，将会展示在外太空看向地球的大气效果。\n\n\n## 简单海平面\n\n<iframe width=\"640\" height=\"360\" frameborder=\"0\" src=\"https://www.shadertoy.com/embed/X3ByDD?gui=true&t=10&paused=true&muted=false\" allowfullscreen></iframe>\n\n这个demo稍微复杂了那么一丢丢，它是由上面的天空大气的demo增加了一些细节而来。云和海水的纹理都是通过柏林噪声来生成的（这就是图形编程的魅力所在吧，仅仅通过代码就能生成逼真的画面）。\n\n流程大概是先计算天空大气的颜色；然后通过噪音生成3D空间的云的密度函数，通过采样一定范围的空气密度来在画面的上半部分渲染云。渲染完成后将云沿着横轴翻转，再配合上噪音生成的海水波纹。\n\n\n## 程序化地形\n\n<iframe width=\"640\" height=\"360\" frameborder=\"0\" src=\"https://www.shadertoy.com/embed/4XByRV?gui=true&t=10&paused=true&muted=false\" allowfullscreen></iframe>\n\n这个demo的灵感来源于Inigo大佬的一篇教程：https://www.shadertoy.com/view/4ttSWf。\n\n和教程有所差异的是我选择了使用无线地形的生成，而不是一个固定的角度。\n\n从效果来看是比较震撼的，不过性能上还是有诸多问题，raymarch地形在现在看来还是比较难做到高性能和高质量检具的结果。\n\n另外云（高层和低层）的效果我并不是特别满意，后续还想改成更为复杂一点的效果，并且加上阴影等。\n\n## domain warp\n\n<iframe width=\"640\" height=\"360\" frameborder=\"0\" src=\"https://www.shadertoy.com/embed/lXfBWX?gui=true&t=10&paused=true&muted=false\" allowfullscreen></iframe>\n\n这个demo是用于学习domain warp的例子。domain warp实际上就是将FBM的输出作为另一个FBM的输入，调整合适的参数和迭代后得到的很特殊的效果。这个demo里面我默认将domain wrap的结果作为raymarch sphere的材质。\n\n## flow effect\n\n<iframe width=\"640\" height=\"360\" frameborder=\"0\" src=\"https://www.shadertoy.com/embed/MXffDX?gui=true&t=10&paused=true&muted=false\" allowfullscreen></iframe>\n\n这个demo和上面的类似，是我学习flow效果的一个练手作品。\n其实现过程是用FBM计算出平面上某个点的流动方向f，并在0帧初始化最初的颜色后，然后从往后的每一帧，每个像素的颜色都遵循f的方向流动并记录成材质，循环往复。\n\n另外这个flow上的阴影效果是根据颜色的blue值来决定的，只是调了一个好看的效果而已。\n\n\n# GitHub\n这里是我做的一些放在GitHub上面的工程。\n\n## KongEngine\nhttps://github.com/ruochenhua/KongEngine\n\nKongEngine是我在2019年开始的一个基于OpenGL的渲染项目，但是后面由于各种原因更新了没有多少功能就弃置了，在2020年提交了最后一个commit就没有动静，最终的效果也如下图所示。\n\n![2020年的最后一次提交](/img/og_tinyGL.png)\n\n\n2024年我重新拾起这个项目的开发，并在原来非常简单的渲染效果上增加了很多额外的功能，这也是我希望我能够一直进行下去的项目。\n\n</div>","updated":"2024-10-29T14:48:40.119Z","path":"display-cabinet/index.html","comments":1,"layout":"page","_id":"cm5ht44vc00023457hh8i16sl","content":"<p>这里是展示窗，是我showcase一些自己的成果的地方，包括代码、github链接等等各种各样的小玩意。</p>\n<div class=\"markdown-body\">\n\n<h1 id=\"ShaderToy\"><a href=\"#ShaderToy\" class=\"headerlink\" title=\"ShaderToy\"></a>ShaderToy</h1><p>ShaderToy是一个很有趣的网站，它上面分享了很多实现很酷炫效果的着色器代码。我在上面也有一些分享，下面展示的是我认为可能还算不错作品，或者是对我有意义的学习内容。</p>\n<h2 id=\"大气渲染\"><a href=\"#大气渲染\" class=\"headerlink\" title=\"大气渲染\"></a>大气渲染</h2><iframe width=\"640\" height=\"360\" frameborder=\"0\" src=\"https://www.shadertoy.com/embed/XXBcRR?gui=true&t=10&paused=true&muted=false\" allowfullscreen></iframe>\n\n<p>这个demo是天空大气的渲染，基于单次散射模型（有一篇文章有提到过）。是个挺有趣的小例子，修改Shader代码可以尝试将相机抬升，将会展示在外太空看向地球的大气效果。</p>\n<h2 id=\"简单海平面\"><a href=\"#简单海平面\" class=\"headerlink\" title=\"简单海平面\"></a>简单海平面</h2><iframe width=\"640\" height=\"360\" frameborder=\"0\" src=\"https://www.shadertoy.com/embed/X3ByDD?gui=true&t=10&paused=true&muted=false\" allowfullscreen></iframe>\n\n<p>这个demo稍微复杂了那么一丢丢，它是由上面的天空大气的demo增加了一些细节而来。云和海水的纹理都是通过柏林噪声来生成的（这就是图形编程的魅力所在吧，仅仅通过代码就能生成逼真的画面）。</p>\n<p>流程大概是先计算天空大气的颜色；然后通过噪音生成3D空间的云的密度函数，通过采样一定范围的空气密度来在画面的上半部分渲染云。渲染完成后将云沿着横轴翻转，再配合上噪音生成的海水波纹。</p>\n<h2 id=\"程序化地形\"><a href=\"#程序化地形\" class=\"headerlink\" title=\"程序化地形\"></a>程序化地形</h2><iframe width=\"640\" height=\"360\" frameborder=\"0\" src=\"https://www.shadertoy.com/embed/4XByRV?gui=true&t=10&paused=true&muted=false\" allowfullscreen></iframe>\n\n<p>这个demo的灵感来源于Inigo大佬的一篇教程：<a href=\"https://www.shadertoy.com/view/4ttSWf%E3%80%82\">https://www.shadertoy.com/view/4ttSWf。</a></p>\n<p>和教程有所差异的是我选择了使用无线地形的生成，而不是一个固定的角度。</p>\n<p>从效果来看是比较震撼的，不过性能上还是有诸多问题，raymarch地形在现在看来还是比较难做到高性能和高质量检具的结果。</p>\n<p>另外云（高层和低层）的效果我并不是特别满意，后续还想改成更为复杂一点的效果，并且加上阴影等。</p>\n<h2 id=\"domain-warp\"><a href=\"#domain-warp\" class=\"headerlink\" title=\"domain warp\"></a>domain warp</h2><iframe width=\"640\" height=\"360\" frameborder=\"0\" src=\"https://www.shadertoy.com/embed/lXfBWX?gui=true&t=10&paused=true&muted=false\" allowfullscreen></iframe>\n\n<p>这个demo是用于学习domain warp的例子。domain warp实际上就是将FBM的输出作为另一个FBM的输入，调整合适的参数和迭代后得到的很特殊的效果。这个demo里面我默认将domain wrap的结果作为raymarch sphere的材质。</p>\n<h2 id=\"flow-effect\"><a href=\"#flow-effect\" class=\"headerlink\" title=\"flow effect\"></a>flow effect</h2><iframe width=\"640\" height=\"360\" frameborder=\"0\" src=\"https://www.shadertoy.com/embed/MXffDX?gui=true&t=10&paused=true&muted=false\" allowfullscreen></iframe>\n\n<p>这个demo和上面的类似，是我学习flow效果的一个练手作品。<br>其实现过程是用FBM计算出平面上某个点的流动方向f，并在0帧初始化最初的颜色后，然后从往后的每一帧，每个像素的颜色都遵循f的方向流动并记录成材质，循环往复。</p>\n<p>另外这个flow上的阴影效果是根据颜色的blue值来决定的，只是调了一个好看的效果而已。</p>\n<h1 id=\"GitHub\"><a href=\"#GitHub\" class=\"headerlink\" title=\"GitHub\"></a>GitHub</h1><p>这里是我做的一些放在GitHub上面的工程。</p>\n<h2 id=\"KongEngine\"><a href=\"#KongEngine\" class=\"headerlink\" title=\"KongEngine\"></a>KongEngine</h2><p><a href=\"https://github.com/ruochenhua/KongEngine\">https://github.com/ruochenhua/KongEngine</a></p>\n<p>KongEngine是我在2019年开始的一个基于OpenGL的渲染项目，但是后面由于各种原因更新了没有多少功能就弃置了，在2020年提交了最后一个commit就没有动静，最终的效果也如下图所示。</p>\n<p><img src=\"/img/og_tinyGL.png\" alt=\"2020年的最后一次提交\"></p>\n<p>2024年我重新拾起这个项目的开发，并在原来非常简单的渲染效果上增加了很多额外的功能，这也是我希望我能够一直进行下去的项目。</p>\n</div>","excerpt":"","more":"<p>这里是展示窗，是我showcase一些自己的成果的地方，包括代码、github链接等等各种各样的小玩意。</p>\n<div class=\"markdown-body\">\n\n<h1 id=\"ShaderToy\"><a href=\"#ShaderToy\" class=\"headerlink\" title=\"ShaderToy\"></a>ShaderToy</h1><p>ShaderToy是一个很有趣的网站，它上面分享了很多实现很酷炫效果的着色器代码。我在上面也有一些分享，下面展示的是我认为可能还算不错作品，或者是对我有意义的学习内容。</p>\n<h2 id=\"大气渲染\"><a href=\"#大气渲染\" class=\"headerlink\" title=\"大气渲染\"></a>大气渲染</h2><iframe width=\"640\" height=\"360\" frameborder=\"0\" src=\"https://www.shadertoy.com/embed/XXBcRR?gui=true&t=10&paused=true&muted=false\" allowfullscreen></iframe>\n\n<p>这个demo是天空大气的渲染，基于单次散射模型（有一篇文章有提到过）。是个挺有趣的小例子，修改Shader代码可以尝试将相机抬升，将会展示在外太空看向地球的大气效果。</p>\n<h2 id=\"简单海平面\"><a href=\"#简单海平面\" class=\"headerlink\" title=\"简单海平面\"></a>简单海平面</h2><iframe width=\"640\" height=\"360\" frameborder=\"0\" src=\"https://www.shadertoy.com/embed/X3ByDD?gui=true&t=10&paused=true&muted=false\" allowfullscreen></iframe>\n\n<p>这个demo稍微复杂了那么一丢丢，它是由上面的天空大气的demo增加了一些细节而来。云和海水的纹理都是通过柏林噪声来生成的（这就是图形编程的魅力所在吧，仅仅通过代码就能生成逼真的画面）。</p>\n<p>流程大概是先计算天空大气的颜色；然后通过噪音生成3D空间的云的密度函数，通过采样一定范围的空气密度来在画面的上半部分渲染云。渲染完成后将云沿着横轴翻转，再配合上噪音生成的海水波纹。</p>\n<h2 id=\"程序化地形\"><a href=\"#程序化地形\" class=\"headerlink\" title=\"程序化地形\"></a>程序化地形</h2><iframe width=\"640\" height=\"360\" frameborder=\"0\" src=\"https://www.shadertoy.com/embed/4XByRV?gui=true&t=10&paused=true&muted=false\" allowfullscreen></iframe>\n\n<p>这个demo的灵感来源于Inigo大佬的一篇教程：<a href=\"https://www.shadertoy.com/view/4ttSWf%E3%80%82\">https://www.shadertoy.com/view/4ttSWf。</a></p>\n<p>和教程有所差异的是我选择了使用无线地形的生成，而不是一个固定的角度。</p>\n<p>从效果来看是比较震撼的，不过性能上还是有诸多问题，raymarch地形在现在看来还是比较难做到高性能和高质量检具的结果。</p>\n<p>另外云（高层和低层）的效果我并不是特别满意，后续还想改成更为复杂一点的效果，并且加上阴影等。</p>\n<h2 id=\"domain-warp\"><a href=\"#domain-warp\" class=\"headerlink\" title=\"domain warp\"></a>domain warp</h2><iframe width=\"640\" height=\"360\" frameborder=\"0\" src=\"https://www.shadertoy.com/embed/lXfBWX?gui=true&t=10&paused=true&muted=false\" allowfullscreen></iframe>\n\n<p>这个demo是用于学习domain warp的例子。domain warp实际上就是将FBM的输出作为另一个FBM的输入，调整合适的参数和迭代后得到的很特殊的效果。这个demo里面我默认将domain wrap的结果作为raymarch sphere的材质。</p>\n<h2 id=\"flow-effect\"><a href=\"#flow-effect\" class=\"headerlink\" title=\"flow effect\"></a>flow effect</h2><iframe width=\"640\" height=\"360\" frameborder=\"0\" src=\"https://www.shadertoy.com/embed/MXffDX?gui=true&t=10&paused=true&muted=false\" allowfullscreen></iframe>\n\n<p>这个demo和上面的类似，是我学习flow效果的一个练手作品。<br>其实现过程是用FBM计算出平面上某个点的流动方向f，并在0帧初始化最初的颜色后，然后从往后的每一帧，每个像素的颜色都遵循f的方向流动并记录成材质，循环往复。</p>\n<p>另外这个flow上的阴影效果是根据颜色的blue值来决定的，只是调了一个好看的效果而已。</p>\n<h1 id=\"GitHub\"><a href=\"#GitHub\" class=\"headerlink\" title=\"GitHub\"></a>GitHub</h1><p>这里是我做的一些放在GitHub上面的工程。</p>\n<h2 id=\"KongEngine\"><a href=\"#KongEngine\" class=\"headerlink\" title=\"KongEngine\"></a>KongEngine</h2><p><a href=\"https://github.com/ruochenhua/KongEngine\">https://github.com/ruochenhua/KongEngine</a></p>\n<p>KongEngine是我在2019年开始的一个基于OpenGL的渲染项目，但是后面由于各种原因更新了没有多少功能就弃置了，在2020年提交了最后一个commit就没有动静，最终的效果也如下图所示。</p>\n<p><img src=\"/img/og_tinyGL.png\" alt=\"2020年的最后一次提交\"></p>\n<p>2024年我重新拾起这个项目的开发，并在原来非常简单的渲染效果上增加了很多额外的功能，这也是我希望我能够一直进行下去的项目。</p>\n</div>"}],"Post":[{"title":"程序化地形生成-2-性能优化","date":"2024-11-19T14:34:36.000Z","_content":"\n# 性能优化的需求\n自从实现了程序化地形生成的那个[ShaderToy上的Demo](https://www.shadertoy.com/view/4XByRV)之后，我对它的性能表现一直不太满意，随随便便跑一下我的GPU就直接拉到100%了，电脑风扇呼呼的。做了很多次大大小小的优化，最后发现瓶颈还是在对地形的光线步进计算上，不把这个问题解决掉的话这个场景的性能怎么样都无法达到令我满意的程度。\n\n于是我一直在寻找类似的场景，寻找有什么光线步进的方法能够满足我的要求：首先它必须是要针对实时随机生成的地形，也就是说不能是针对高度图或者其他预处理过的地形数据；其次它需要快，至少能够在我这台笔记本上（3070ti显卡）能够保持50%以下的占用率；最后就是这个光线步进算法需要有一定的精度，但是要求不会很高。\n\n最后我在ShaderToy上找到了一个非常棒的[例子](https://www.shadertoy.com/view/4slGD4)，来自Dave_Hoskins。\n\nDave的Demo也是做了地形的渲染，他的场景比我复杂很多，但是这个更为复杂的场景在我的电脑上运行的时候，它的GPU占用率（分辨率768X432）只有35%左右，远低于我的demo让我大为震撼。\n\n于是我开始研究它的光线步进的逻辑，如下：\n```glsl\n// source:https://www.shadertoy.com/view/4slGD4\nfloat BinarySubdivision(in vec3 rO, in vec3 rD, vec2 t)\n{\n\t// Home in on the surface by dividing by two and split...\n    float halfwayT;\n  \n    for (int i = 0; i < 5; i++)\n    {\n\n        halfwayT = dot(t, vec2(.5));\n        vec3 p = rO + halfwayT*rD;\n        float d = p.y - getTerrainHeight(p.xz, perlinOctaves); \n        // float d = Map(rO + halfwayT*rD); \n         t = mix(vec2(t.x, halfwayT), vec2(halfwayT, t.y), step(0.5, d));\n\n    }\n\treturn halfwayT;\n}\n\nbool rayMarchingTerrain(vec3 ro, vec3 rd, float max_dist, out float res_t)\n{\n    float t = 1. + Hash12(g_frag_coord)*1.;\n\tfloat oldT = 0.0;\n\tfloat delta = 0.0;\n\tbool fin = false;\n\tbool res = false;\n\tvec2 distances;\n\tfor( int j=0; j< 150; j++ )\n\t{\n\t\tif (fin || t > 240.0) break;\n\t\tvec3 p = ro + t*rd;\n\t\t//if (t > 240.0 || p.y > 195.0) break;\n\t\tfloat h = p.y - getTerrainHeight(p.xz, perlinOctaves); // ...Get this positions height mapping.\n\t\t// Are we inside, and close enough to fudge a hit?...\n\t\tif( h < 0.5)\n\t\t{\n\t\t\tfin = true;\n\t\t\tdistances = vec2(oldT, t);\n\t\t\tbreak;\n\t\t}\n\t\t// Delta ray advance - a fudge between the height returned\n\t\t// and the distance already travelled.\n\t\t// It's a really fiddly compromise between speed and accuracy\n\t\t// Too large a step and the tops of ridges get missed.\n\t\tdelta = max(0.01, 0.3*h) + (t*0.0065);\n\t\toldT = t;\n\t\tt += delta;\n\t}\n\tif (fin) res_t = BinarySubdivision(ro, rd, distances);\n\n\treturn fin;\n}\n```\n\n其实代码逻辑很简单，就是光线步进到的位置和当前XZ坐标的地形高度做比对，当光线步进的位置的高度和地形足够近的时候，记为击中。记录当前和上一步的t的位置，在得到最终结果的时候做一个取中间值的操作。\n\n这个方法的精华部分是这个：**delta = max(0.01, 0.3\\*h) + (t\\*0.0065);**，它被用于计算光线步进下一步的距离。如果光线步进每一步距离太近，会严重影响性能；而如果一步太远，则会导致地形的精度不足，出现地表抖动甚至断裂的情况。\n\nDave的方法，结合了当前位置和地形的高度差h和光线步进已经经过的长度t。高度差越小，说明可能越接近地表，需要较小的步长（反之亦然）；t的影响则表示远处的地形的精度需求可以逐步降低。\n\n下面是我原来的计算方式。\n```glsl\nbool rayMarchingTerrain(vec3 ro, vec3 rd, float max_dist, out float res_t)\n{\n    // float terrain_height = sin(iTime) + 1.;    \n    float dt_min = 0.1f;\n    float dt_max = 3.0f;\n\n    float dt = 1.0;\n    res_t = 0.0;\n    // first pass, step 1\n    for(float t = mint; t < max_dist; t+=dt)\n    {\n        vec3 p = ro+t*rd;\n        float terrain_height = getTerrainHeight(p.xz, perlinOctaves);\n        if(p.y < terrain_height )\n        {        \n            // res_t = t - dt + dt*(last_h - last_p.y) / (p.y - last_p.y-terrain_height+last_h); \n            res_t = t;\n            break;\n        }        \n        // // closer terrain use higher accuracy        \n        // last_h = terrain_height;        \n        // last_p = p;\n        dt = mix(dt_min, dt_max, pow(t / max_dist, 2.0));\n    }\n\n    // hit terrain\n    if(res_t > 0.)\n    {\n        float last_h = 0.0;\n        vec3 last_p = vec3(0);\n        float mini_dt =  max(0.01, dt * 0.02);\n        for(float t = res_t - dt; t < res_t + .01; t+=mini_dt)\n        {\n            vec3 p = ro+t*rd;\n            float terrain_height = getTerrainHeight(p.xz, perlinOctaves);\n            if(p.y < terrain_height)\n            {        \n                res_t = t - mini_dt + mini_dt*(last_h - last_p.y) / (p.y - last_p.y-terrain_height+last_h); \n                return true;\n            }        \n            // closer terrain use higher accuracy        \n            last_h = terrain_height;        \n            last_p = p;\n        }\n    }\n\n    return false;    \n}\n```\n\n我原来的方法的思想是做两遍测试，先以一个较大步长做一次初步筛选，找到大概的光线穿过地形的区间；然后再在那个区间用较小的步长做另外因此光线步进。\n\n这个方法的问题在于如果初筛的时候步长太大，可能会穿过一个厚度较小的地形（比如说山峰），所以初筛的步长也不能太小；第二次筛选似乎取值也偏小了，导致还是做了很多次的光线步进检测。\n\n# 优化结果\n现在我将新的光线步进方法更新到了我原来的ShaderToy Demo上，在768X432的分辨率60fps的情况下，我的demo在我的电脑上的GPU占用率由80%左右降低到了35%左右，可谓是巨大的提升。\n\n在demo的代码中，我在第一行添加了代码\n```glsl\n#define OLD_METHOD 0\n```\n将**OLD_METHOD**改为1的话可以改为使用老方法，各位有兴趣的话可以实际修改一下代码来对比一下这两种方法的性能差异。\n\n","source":"_posts/ProceduralTerrainGeneration2-optimize.md","raw":"---\ntitle: 程序化地形生成-2-性能优化\ndate: 2024-11-19 22:34:36\ncategories: \n\t- 技术漫谈\ntags: [3D, render, 渲染, 编程, 程序化生成]\n\t\n---\n\n# 性能优化的需求\n自从实现了程序化地形生成的那个[ShaderToy上的Demo](https://www.shadertoy.com/view/4XByRV)之后，我对它的性能表现一直不太满意，随随便便跑一下我的GPU就直接拉到100%了，电脑风扇呼呼的。做了很多次大大小小的优化，最后发现瓶颈还是在对地形的光线步进计算上，不把这个问题解决掉的话这个场景的性能怎么样都无法达到令我满意的程度。\n\n于是我一直在寻找类似的场景，寻找有什么光线步进的方法能够满足我的要求：首先它必须是要针对实时随机生成的地形，也就是说不能是针对高度图或者其他预处理过的地形数据；其次它需要快，至少能够在我这台笔记本上（3070ti显卡）能够保持50%以下的占用率；最后就是这个光线步进算法需要有一定的精度，但是要求不会很高。\n\n最后我在ShaderToy上找到了一个非常棒的[例子](https://www.shadertoy.com/view/4slGD4)，来自Dave_Hoskins。\n\nDave的Demo也是做了地形的渲染，他的场景比我复杂很多，但是这个更为复杂的场景在我的电脑上运行的时候，它的GPU占用率（分辨率768X432）只有35%左右，远低于我的demo让我大为震撼。\n\n于是我开始研究它的光线步进的逻辑，如下：\n```glsl\n// source:https://www.shadertoy.com/view/4slGD4\nfloat BinarySubdivision(in vec3 rO, in vec3 rD, vec2 t)\n{\n\t// Home in on the surface by dividing by two and split...\n    float halfwayT;\n  \n    for (int i = 0; i < 5; i++)\n    {\n\n        halfwayT = dot(t, vec2(.5));\n        vec3 p = rO + halfwayT*rD;\n        float d = p.y - getTerrainHeight(p.xz, perlinOctaves); \n        // float d = Map(rO + halfwayT*rD); \n         t = mix(vec2(t.x, halfwayT), vec2(halfwayT, t.y), step(0.5, d));\n\n    }\n\treturn halfwayT;\n}\n\nbool rayMarchingTerrain(vec3 ro, vec3 rd, float max_dist, out float res_t)\n{\n    float t = 1. + Hash12(g_frag_coord)*1.;\n\tfloat oldT = 0.0;\n\tfloat delta = 0.0;\n\tbool fin = false;\n\tbool res = false;\n\tvec2 distances;\n\tfor( int j=0; j< 150; j++ )\n\t{\n\t\tif (fin || t > 240.0) break;\n\t\tvec3 p = ro + t*rd;\n\t\t//if (t > 240.0 || p.y > 195.0) break;\n\t\tfloat h = p.y - getTerrainHeight(p.xz, perlinOctaves); // ...Get this positions height mapping.\n\t\t// Are we inside, and close enough to fudge a hit?...\n\t\tif( h < 0.5)\n\t\t{\n\t\t\tfin = true;\n\t\t\tdistances = vec2(oldT, t);\n\t\t\tbreak;\n\t\t}\n\t\t// Delta ray advance - a fudge between the height returned\n\t\t// and the distance already travelled.\n\t\t// It's a really fiddly compromise between speed and accuracy\n\t\t// Too large a step and the tops of ridges get missed.\n\t\tdelta = max(0.01, 0.3*h) + (t*0.0065);\n\t\toldT = t;\n\t\tt += delta;\n\t}\n\tif (fin) res_t = BinarySubdivision(ro, rd, distances);\n\n\treturn fin;\n}\n```\n\n其实代码逻辑很简单，就是光线步进到的位置和当前XZ坐标的地形高度做比对，当光线步进的位置的高度和地形足够近的时候，记为击中。记录当前和上一步的t的位置，在得到最终结果的时候做一个取中间值的操作。\n\n这个方法的精华部分是这个：**delta = max(0.01, 0.3\\*h) + (t\\*0.0065);**，它被用于计算光线步进下一步的距离。如果光线步进每一步距离太近，会严重影响性能；而如果一步太远，则会导致地形的精度不足，出现地表抖动甚至断裂的情况。\n\nDave的方法，结合了当前位置和地形的高度差h和光线步进已经经过的长度t。高度差越小，说明可能越接近地表，需要较小的步长（反之亦然）；t的影响则表示远处的地形的精度需求可以逐步降低。\n\n下面是我原来的计算方式。\n```glsl\nbool rayMarchingTerrain(vec3 ro, vec3 rd, float max_dist, out float res_t)\n{\n    // float terrain_height = sin(iTime) + 1.;    \n    float dt_min = 0.1f;\n    float dt_max = 3.0f;\n\n    float dt = 1.0;\n    res_t = 0.0;\n    // first pass, step 1\n    for(float t = mint; t < max_dist; t+=dt)\n    {\n        vec3 p = ro+t*rd;\n        float terrain_height = getTerrainHeight(p.xz, perlinOctaves);\n        if(p.y < terrain_height )\n        {        \n            // res_t = t - dt + dt*(last_h - last_p.y) / (p.y - last_p.y-terrain_height+last_h); \n            res_t = t;\n            break;\n        }        \n        // // closer terrain use higher accuracy        \n        // last_h = terrain_height;        \n        // last_p = p;\n        dt = mix(dt_min, dt_max, pow(t / max_dist, 2.0));\n    }\n\n    // hit terrain\n    if(res_t > 0.)\n    {\n        float last_h = 0.0;\n        vec3 last_p = vec3(0);\n        float mini_dt =  max(0.01, dt * 0.02);\n        for(float t = res_t - dt; t < res_t + .01; t+=mini_dt)\n        {\n            vec3 p = ro+t*rd;\n            float terrain_height = getTerrainHeight(p.xz, perlinOctaves);\n            if(p.y < terrain_height)\n            {        \n                res_t = t - mini_dt + mini_dt*(last_h - last_p.y) / (p.y - last_p.y-terrain_height+last_h); \n                return true;\n            }        \n            // closer terrain use higher accuracy        \n            last_h = terrain_height;        \n            last_p = p;\n        }\n    }\n\n    return false;    \n}\n```\n\n我原来的方法的思想是做两遍测试，先以一个较大步长做一次初步筛选，找到大概的光线穿过地形的区间；然后再在那个区间用较小的步长做另外因此光线步进。\n\n这个方法的问题在于如果初筛的时候步长太大，可能会穿过一个厚度较小的地形（比如说山峰），所以初筛的步长也不能太小；第二次筛选似乎取值也偏小了，导致还是做了很多次的光线步进检测。\n\n# 优化结果\n现在我将新的光线步进方法更新到了我原来的ShaderToy Demo上，在768X432的分辨率60fps的情况下，我的demo在我的电脑上的GPU占用率由80%左右降低到了35%左右，可谓是巨大的提升。\n\n在demo的代码中，我在第一行添加了代码\n```glsl\n#define OLD_METHOD 0\n```\n将**OLD_METHOD**改为1的话可以改为使用老方法，各位有兴趣的话可以实际修改一下代码来对比一下这两种方法的性能差异。\n\n","slug":"ProceduralTerrainGeneration2-optimize","published":1,"updated":"2024-11-19T15:17:27.531Z","comments":1,"layout":"post","photos":[],"_id":"cm5ht44va00013457cnap5ss6","content":"<h1 id=\"性能优化的需求\"><a href=\"#性能优化的需求\" class=\"headerlink\" title=\"性能优化的需求\"></a>性能优化的需求</h1><p>自从实现了程序化地形生成的那个<a href=\"https://www.shadertoy.com/view/4XByRV\">ShaderToy上的Demo</a>之后，我对它的性能表现一直不太满意，随随便便跑一下我的GPU就直接拉到100%了，电脑风扇呼呼的。做了很多次大大小小的优化，最后发现瓶颈还是在对地形的光线步进计算上，不把这个问题解决掉的话这个场景的性能怎么样都无法达到令我满意的程度。</p>\n<p>于是我一直在寻找类似的场景，寻找有什么光线步进的方法能够满足我的要求：首先它必须是要针对实时随机生成的地形，也就是说不能是针对高度图或者其他预处理过的地形数据；其次它需要快，至少能够在我这台笔记本上（3070ti显卡）能够保持50%以下的占用率；最后就是这个光线步进算法需要有一定的精度，但是要求不会很高。</p>\n<p>最后我在ShaderToy上找到了一个非常棒的<a href=\"https://www.shadertoy.com/view/4slGD4\">例子</a>，来自Dave_Hoskins。</p>\n<p>Dave的Demo也是做了地形的渲染，他的场景比我复杂很多，但是这个更为复杂的场景在我的电脑上运行的时候，它的GPU占用率（分辨率768X432）只有35%左右，远低于我的demo让我大为震撼。</p>\n<p>于是我开始研究它的光线步进的逻辑，如下：</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-comment\">// source:https://www.shadertoy.com/view/4slGD4</span><br><span class=\"hljs-type\">float</span> BinarySubdivision(<span class=\"hljs-keyword\">in</span> <span class=\"hljs-type\">vec3</span> rO, <span class=\"hljs-keyword\">in</span> <span class=\"hljs-type\">vec3</span> rD, <span class=\"hljs-type\">vec2</span> t)<br>&#123;<br>\t<span class=\"hljs-comment\">// Home in on the surface by dividing by two and split...</span><br>    <span class=\"hljs-type\">float</span> halfwayT;<br>  <br>    <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> i = <span class=\"hljs-number\">0</span>; i &lt; <span class=\"hljs-number\">5</span>; i++)<br>    &#123;<br><br>        halfwayT = <span class=\"hljs-built_in\">dot</span>(t, <span class=\"hljs-type\">vec2</span>(<span class=\"hljs-number\">.5</span>));<br>        <span class=\"hljs-type\">vec3</span> p = rO + halfwayT*rD;<br>        <span class=\"hljs-type\">float</span> d = p.y - getTerrainHeight(p.xz, perlinOctaves); <br>        <span class=\"hljs-comment\">// float d = Map(rO + halfwayT*rD); </span><br>         t = <span class=\"hljs-built_in\">mix</span>(<span class=\"hljs-type\">vec2</span>(t.x, halfwayT), <span class=\"hljs-type\">vec2</span>(halfwayT, t.y), <span class=\"hljs-built_in\">step</span>(<span class=\"hljs-number\">0.5</span>, d));<br><br>    &#125;<br>\t<span class=\"hljs-keyword\">return</span> halfwayT;<br>&#125;<br><br><span class=\"hljs-type\">bool</span> rayMarchingTerrain(<span class=\"hljs-type\">vec3</span> ro, <span class=\"hljs-type\">vec3</span> rd, <span class=\"hljs-type\">float</span> max_dist, <span class=\"hljs-keyword\">out</span> <span class=\"hljs-type\">float</span> res_t)<br>&#123;<br>    <span class=\"hljs-type\">float</span> t = <span class=\"hljs-number\">1.</span> + Hash12(g_frag_coord)*<span class=\"hljs-number\">1.</span>;<br>\t<span class=\"hljs-type\">float</span> oldT = <span class=\"hljs-number\">0.0</span>;<br>\t<span class=\"hljs-type\">float</span> delta = <span class=\"hljs-number\">0.0</span>;<br>\t<span class=\"hljs-type\">bool</span> fin = <span class=\"hljs-literal\">false</span>;<br>\t<span class=\"hljs-type\">bool</span> res = <span class=\"hljs-literal\">false</span>;<br>\t<span class=\"hljs-type\">vec2</span> distances;<br>\t<span class=\"hljs-keyword\">for</span>( <span class=\"hljs-type\">int</span> j=<span class=\"hljs-number\">0</span>; j&lt; <span class=\"hljs-number\">150</span>; j++ )<br>\t&#123;<br>\t\t<span class=\"hljs-keyword\">if</span> (fin || t &gt; <span class=\"hljs-number\">240.0</span>) <span class=\"hljs-keyword\">break</span>;<br>\t\t<span class=\"hljs-type\">vec3</span> p = ro + t*rd;<br>\t\t<span class=\"hljs-comment\">//if (t &gt; 240.0 || p.y &gt; 195.0) break;</span><br>\t\t<span class=\"hljs-type\">float</span> h = p.y - getTerrainHeight(p.xz, perlinOctaves); <span class=\"hljs-comment\">// ...Get this positions height mapping.</span><br>\t\t<span class=\"hljs-comment\">// Are we inside, and close enough to fudge a hit?...</span><br>\t\t<span class=\"hljs-keyword\">if</span>( h &lt; <span class=\"hljs-number\">0.5</span>)<br>\t\t&#123;<br>\t\t\tfin = <span class=\"hljs-literal\">true</span>;<br>\t\t\tdistances = <span class=\"hljs-type\">vec2</span>(oldT, t);<br>\t\t\t<span class=\"hljs-keyword\">break</span>;<br>\t\t&#125;<br>\t\t<span class=\"hljs-comment\">// Delta ray advance - a fudge between the height returned</span><br>\t\t<span class=\"hljs-comment\">// and the distance already travelled.</span><br>\t\t<span class=\"hljs-comment\">// It&#x27;s a really fiddly compromise between speed and accuracy</span><br>\t\t<span class=\"hljs-comment\">// Too large a step and the tops of ridges get missed.</span><br>\t\tdelta = <span class=\"hljs-built_in\">max</span>(<span class=\"hljs-number\">0.01</span>, <span class=\"hljs-number\">0.3</span>*h) + (t*<span class=\"hljs-number\">0.0065</span>);<br>\t\toldT = t;<br>\t\tt += delta;<br>\t&#125;<br>\t<span class=\"hljs-keyword\">if</span> (fin) res_t = BinarySubdivision(ro, rd, distances);<br><br>\t<span class=\"hljs-keyword\">return</span> fin;<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<p>其实代码逻辑很简单，就是光线步进到的位置和当前XZ坐标的地形高度做比对，当光线步进的位置的高度和地形足够近的时候，记为击中。记录当前和上一步的t的位置，在得到最终结果的时候做一个取中间值的操作。</p>\n<p>这个方法的精华部分是这个：**delta &#x3D; max(0.01, 0.3*h) + (t*0.0065);**，它被用于计算光线步进下一步的距离。如果光线步进每一步距离太近，会严重影响性能；而如果一步太远，则会导致地形的精度不足，出现地表抖动甚至断裂的情况。</p>\n<p>Dave的方法，结合了当前位置和地形的高度差h和光线步进已经经过的长度t。高度差越小，说明可能越接近地表，需要较小的步长（反之亦然）；t的影响则表示远处的地形的精度需求可以逐步降低。</p>\n<p>下面是我原来的计算方式。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-type\">bool</span> rayMarchingTerrain(<span class=\"hljs-type\">vec3</span> ro, <span class=\"hljs-type\">vec3</span> rd, <span class=\"hljs-type\">float</span> max_dist, <span class=\"hljs-keyword\">out</span> <span class=\"hljs-type\">float</span> res_t)<br>&#123;<br>    <span class=\"hljs-comment\">// float terrain_height = sin(iTime) + 1.;    </span><br>    <span class=\"hljs-type\">float</span> dt_min = <span class=\"hljs-number\">0.1</span>f;<br>    <span class=\"hljs-type\">float</span> dt_max = <span class=\"hljs-number\">3.0</span>f;<br><br>    <span class=\"hljs-type\">float</span> dt = <span class=\"hljs-number\">1.0</span>;<br>    res_t = <span class=\"hljs-number\">0.0</span>;<br>    <span class=\"hljs-comment\">// first pass, step 1</span><br>    <span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">float</span> t = mint; t &lt; max_dist; t+=dt)<br>    &#123;<br>        <span class=\"hljs-type\">vec3</span> p = ro+t*rd;<br>        <span class=\"hljs-type\">float</span> terrain_height = getTerrainHeight(p.xz, perlinOctaves);<br>        <span class=\"hljs-keyword\">if</span>(p.y &lt; terrain_height )<br>        &#123;        <br>            <span class=\"hljs-comment\">// res_t = t - dt + dt*(last_h - last_p.y) / (p.y - last_p.y-terrain_height+last_h); </span><br>            res_t = t;<br>            <span class=\"hljs-keyword\">break</span>;<br>        &#125;        <br>        <span class=\"hljs-comment\">// // closer terrain use higher accuracy        </span><br>        <span class=\"hljs-comment\">// last_h = terrain_height;        </span><br>        <span class=\"hljs-comment\">// last_p = p;</span><br>        dt = <span class=\"hljs-built_in\">mix</span>(dt_min, dt_max, <span class=\"hljs-built_in\">pow</span>(t / max_dist, <span class=\"hljs-number\">2.0</span>));<br>    &#125;<br><br>    <span class=\"hljs-comment\">// hit terrain</span><br>    <span class=\"hljs-keyword\">if</span>(res_t &gt; <span class=\"hljs-number\">0.</span>)<br>    &#123;<br>        <span class=\"hljs-type\">float</span> last_h = <span class=\"hljs-number\">0.0</span>;<br>        <span class=\"hljs-type\">vec3</span> last_p = <span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">0</span>);<br>        <span class=\"hljs-type\">float</span> mini_dt =  <span class=\"hljs-built_in\">max</span>(<span class=\"hljs-number\">0.01</span>, dt * <span class=\"hljs-number\">0.02</span>);<br>        <span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">float</span> t = res_t - dt; t &lt; res_t + <span class=\"hljs-number\">.01</span>; t+=mini_dt)<br>        &#123;<br>            <span class=\"hljs-type\">vec3</span> p = ro+t*rd;<br>            <span class=\"hljs-type\">float</span> terrain_height = getTerrainHeight(p.xz, perlinOctaves);<br>            <span class=\"hljs-keyword\">if</span>(p.y &lt; terrain_height)<br>            &#123;        <br>                res_t = t - mini_dt + mini_dt*(last_h - last_p.y) / (p.y - last_p.y-terrain_height+last_h); <br>                <span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">true</span>;<br>            &#125;        <br>            <span class=\"hljs-comment\">// closer terrain use higher accuracy        </span><br>            last_h = terrain_height;        <br>            last_p = p;<br>        &#125;<br>    &#125;<br><br>    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">false</span>;    <br>&#125;<br></code></pre></td></tr></table></figure>\n\n<p>我原来的方法的思想是做两遍测试，先以一个较大步长做一次初步筛选，找到大概的光线穿过地形的区间；然后再在那个区间用较小的步长做另外因此光线步进。</p>\n<p>这个方法的问题在于如果初筛的时候步长太大，可能会穿过一个厚度较小的地形（比如说山峰），所以初筛的步长也不能太小；第二次筛选似乎取值也偏小了，导致还是做了很多次的光线步进检测。</p>\n<h1 id=\"优化结果\"><a href=\"#优化结果\" class=\"headerlink\" title=\"优化结果\"></a>优化结果</h1><p>现在我将新的光线步进方法更新到了我原来的ShaderToy Demo上，在768X432的分辨率60fps的情况下，我的demo在我的电脑上的GPU占用率由80%左右降低到了35%左右，可谓是巨大的提升。</p>\n<p>在demo的代码中，我在第一行添加了代码</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-meta\">#define OLD_METHOD 0</span><br></code></pre></td></tr></table></figure>\n<p>将<strong>OLD_METHOD</strong>改为1的话可以改为使用老方法，各位有兴趣的话可以实际修改一下代码来对比一下这两种方法的性能差异。</p>\n","excerpt":"","more":"<h1 id=\"性能优化的需求\"><a href=\"#性能优化的需求\" class=\"headerlink\" title=\"性能优化的需求\"></a>性能优化的需求</h1><p>自从实现了程序化地形生成的那个<a href=\"https://www.shadertoy.com/view/4XByRV\">ShaderToy上的Demo</a>之后，我对它的性能表现一直不太满意，随随便便跑一下我的GPU就直接拉到100%了，电脑风扇呼呼的。做了很多次大大小小的优化，最后发现瓶颈还是在对地形的光线步进计算上，不把这个问题解决掉的话这个场景的性能怎么样都无法达到令我满意的程度。</p>\n<p>于是我一直在寻找类似的场景，寻找有什么光线步进的方法能够满足我的要求：首先它必须是要针对实时随机生成的地形，也就是说不能是针对高度图或者其他预处理过的地形数据；其次它需要快，至少能够在我这台笔记本上（3070ti显卡）能够保持50%以下的占用率；最后就是这个光线步进算法需要有一定的精度，但是要求不会很高。</p>\n<p>最后我在ShaderToy上找到了一个非常棒的<a href=\"https://www.shadertoy.com/view/4slGD4\">例子</a>，来自Dave_Hoskins。</p>\n<p>Dave的Demo也是做了地形的渲染，他的场景比我复杂很多，但是这个更为复杂的场景在我的电脑上运行的时候，它的GPU占用率（分辨率768X432）只有35%左右，远低于我的demo让我大为震撼。</p>\n<p>于是我开始研究它的光线步进的逻辑，如下：</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-comment\">// source:https://www.shadertoy.com/view/4slGD4</span><br><span class=\"hljs-type\">float</span> BinarySubdivision(<span class=\"hljs-keyword\">in</span> <span class=\"hljs-type\">vec3</span> rO, <span class=\"hljs-keyword\">in</span> <span class=\"hljs-type\">vec3</span> rD, <span class=\"hljs-type\">vec2</span> t)<br>&#123;<br>\t<span class=\"hljs-comment\">// Home in on the surface by dividing by two and split...</span><br>    <span class=\"hljs-type\">float</span> halfwayT;<br>  <br>    <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> i = <span class=\"hljs-number\">0</span>; i &lt; <span class=\"hljs-number\">5</span>; i++)<br>    &#123;<br><br>        halfwayT = <span class=\"hljs-built_in\">dot</span>(t, <span class=\"hljs-type\">vec2</span>(<span class=\"hljs-number\">.5</span>));<br>        <span class=\"hljs-type\">vec3</span> p = rO + halfwayT*rD;<br>        <span class=\"hljs-type\">float</span> d = p.y - getTerrainHeight(p.xz, perlinOctaves); <br>        <span class=\"hljs-comment\">// float d = Map(rO + halfwayT*rD); </span><br>         t = <span class=\"hljs-built_in\">mix</span>(<span class=\"hljs-type\">vec2</span>(t.x, halfwayT), <span class=\"hljs-type\">vec2</span>(halfwayT, t.y), <span class=\"hljs-built_in\">step</span>(<span class=\"hljs-number\">0.5</span>, d));<br><br>    &#125;<br>\t<span class=\"hljs-keyword\">return</span> halfwayT;<br>&#125;<br><br><span class=\"hljs-type\">bool</span> rayMarchingTerrain(<span class=\"hljs-type\">vec3</span> ro, <span class=\"hljs-type\">vec3</span> rd, <span class=\"hljs-type\">float</span> max_dist, <span class=\"hljs-keyword\">out</span> <span class=\"hljs-type\">float</span> res_t)<br>&#123;<br>    <span class=\"hljs-type\">float</span> t = <span class=\"hljs-number\">1.</span> + Hash12(g_frag_coord)*<span class=\"hljs-number\">1.</span>;<br>\t<span class=\"hljs-type\">float</span> oldT = <span class=\"hljs-number\">0.0</span>;<br>\t<span class=\"hljs-type\">float</span> delta = <span class=\"hljs-number\">0.0</span>;<br>\t<span class=\"hljs-type\">bool</span> fin = <span class=\"hljs-literal\">false</span>;<br>\t<span class=\"hljs-type\">bool</span> res = <span class=\"hljs-literal\">false</span>;<br>\t<span class=\"hljs-type\">vec2</span> distances;<br>\t<span class=\"hljs-keyword\">for</span>( <span class=\"hljs-type\">int</span> j=<span class=\"hljs-number\">0</span>; j&lt; <span class=\"hljs-number\">150</span>; j++ )<br>\t&#123;<br>\t\t<span class=\"hljs-keyword\">if</span> (fin || t &gt; <span class=\"hljs-number\">240.0</span>) <span class=\"hljs-keyword\">break</span>;<br>\t\t<span class=\"hljs-type\">vec3</span> p = ro + t*rd;<br>\t\t<span class=\"hljs-comment\">//if (t &gt; 240.0 || p.y &gt; 195.0) break;</span><br>\t\t<span class=\"hljs-type\">float</span> h = p.y - getTerrainHeight(p.xz, perlinOctaves); <span class=\"hljs-comment\">// ...Get this positions height mapping.</span><br>\t\t<span class=\"hljs-comment\">// Are we inside, and close enough to fudge a hit?...</span><br>\t\t<span class=\"hljs-keyword\">if</span>( h &lt; <span class=\"hljs-number\">0.5</span>)<br>\t\t&#123;<br>\t\t\tfin = <span class=\"hljs-literal\">true</span>;<br>\t\t\tdistances = <span class=\"hljs-type\">vec2</span>(oldT, t);<br>\t\t\t<span class=\"hljs-keyword\">break</span>;<br>\t\t&#125;<br>\t\t<span class=\"hljs-comment\">// Delta ray advance - a fudge between the height returned</span><br>\t\t<span class=\"hljs-comment\">// and the distance already travelled.</span><br>\t\t<span class=\"hljs-comment\">// It&#x27;s a really fiddly compromise between speed and accuracy</span><br>\t\t<span class=\"hljs-comment\">// Too large a step and the tops of ridges get missed.</span><br>\t\tdelta = <span class=\"hljs-built_in\">max</span>(<span class=\"hljs-number\">0.01</span>, <span class=\"hljs-number\">0.3</span>*h) + (t*<span class=\"hljs-number\">0.0065</span>);<br>\t\toldT = t;<br>\t\tt += delta;<br>\t&#125;<br>\t<span class=\"hljs-keyword\">if</span> (fin) res_t = BinarySubdivision(ro, rd, distances);<br><br>\t<span class=\"hljs-keyword\">return</span> fin;<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<p>其实代码逻辑很简单，就是光线步进到的位置和当前XZ坐标的地形高度做比对，当光线步进的位置的高度和地形足够近的时候，记为击中。记录当前和上一步的t的位置，在得到最终结果的时候做一个取中间值的操作。</p>\n<p>这个方法的精华部分是这个：**delta &#x3D; max(0.01, 0.3*h) + (t*0.0065);**，它被用于计算光线步进下一步的距离。如果光线步进每一步距离太近，会严重影响性能；而如果一步太远，则会导致地形的精度不足，出现地表抖动甚至断裂的情况。</p>\n<p>Dave的方法，结合了当前位置和地形的高度差h和光线步进已经经过的长度t。高度差越小，说明可能越接近地表，需要较小的步长（反之亦然）；t的影响则表示远处的地形的精度需求可以逐步降低。</p>\n<p>下面是我原来的计算方式。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-type\">bool</span> rayMarchingTerrain(<span class=\"hljs-type\">vec3</span> ro, <span class=\"hljs-type\">vec3</span> rd, <span class=\"hljs-type\">float</span> max_dist, <span class=\"hljs-keyword\">out</span> <span class=\"hljs-type\">float</span> res_t)<br>&#123;<br>    <span class=\"hljs-comment\">// float terrain_height = sin(iTime) + 1.;    </span><br>    <span class=\"hljs-type\">float</span> dt_min = <span class=\"hljs-number\">0.1</span>f;<br>    <span class=\"hljs-type\">float</span> dt_max = <span class=\"hljs-number\">3.0</span>f;<br><br>    <span class=\"hljs-type\">float</span> dt = <span class=\"hljs-number\">1.0</span>;<br>    res_t = <span class=\"hljs-number\">0.0</span>;<br>    <span class=\"hljs-comment\">// first pass, step 1</span><br>    <span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">float</span> t = mint; t &lt; max_dist; t+=dt)<br>    &#123;<br>        <span class=\"hljs-type\">vec3</span> p = ro+t*rd;<br>        <span class=\"hljs-type\">float</span> terrain_height = getTerrainHeight(p.xz, perlinOctaves);<br>        <span class=\"hljs-keyword\">if</span>(p.y &lt; terrain_height )<br>        &#123;        <br>            <span class=\"hljs-comment\">// res_t = t - dt + dt*(last_h - last_p.y) / (p.y - last_p.y-terrain_height+last_h); </span><br>            res_t = t;<br>            <span class=\"hljs-keyword\">break</span>;<br>        &#125;        <br>        <span class=\"hljs-comment\">// // closer terrain use higher accuracy        </span><br>        <span class=\"hljs-comment\">// last_h = terrain_height;        </span><br>        <span class=\"hljs-comment\">// last_p = p;</span><br>        dt = <span class=\"hljs-built_in\">mix</span>(dt_min, dt_max, <span class=\"hljs-built_in\">pow</span>(t / max_dist, <span class=\"hljs-number\">2.0</span>));<br>    &#125;<br><br>    <span class=\"hljs-comment\">// hit terrain</span><br>    <span class=\"hljs-keyword\">if</span>(res_t &gt; <span class=\"hljs-number\">0.</span>)<br>    &#123;<br>        <span class=\"hljs-type\">float</span> last_h = <span class=\"hljs-number\">0.0</span>;<br>        <span class=\"hljs-type\">vec3</span> last_p = <span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">0</span>);<br>        <span class=\"hljs-type\">float</span> mini_dt =  <span class=\"hljs-built_in\">max</span>(<span class=\"hljs-number\">0.01</span>, dt * <span class=\"hljs-number\">0.02</span>);<br>        <span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">float</span> t = res_t - dt; t &lt; res_t + <span class=\"hljs-number\">.01</span>; t+=mini_dt)<br>        &#123;<br>            <span class=\"hljs-type\">vec3</span> p = ro+t*rd;<br>            <span class=\"hljs-type\">float</span> terrain_height = getTerrainHeight(p.xz, perlinOctaves);<br>            <span class=\"hljs-keyword\">if</span>(p.y &lt; terrain_height)<br>            &#123;        <br>                res_t = t - mini_dt + mini_dt*(last_h - last_p.y) / (p.y - last_p.y-terrain_height+last_h); <br>                <span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">true</span>;<br>            &#125;        <br>            <span class=\"hljs-comment\">// closer terrain use higher accuracy        </span><br>            last_h = terrain_height;        <br>            last_p = p;<br>        &#125;<br>    &#125;<br><br>    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">false</span>;    <br>&#125;<br></code></pre></td></tr></table></figure>\n\n<p>我原来的方法的思想是做两遍测试，先以一个较大步长做一次初步筛选，找到大概的光线穿过地形的区间；然后再在那个区间用较小的步长做另外因此光线步进。</p>\n<p>这个方法的问题在于如果初筛的时候步长太大，可能会穿过一个厚度较小的地形（比如说山峰），所以初筛的步长也不能太小；第二次筛选似乎取值也偏小了，导致还是做了很多次的光线步进检测。</p>\n<h1 id=\"优化结果\"><a href=\"#优化结果\" class=\"headerlink\" title=\"优化结果\"></a>优化结果</h1><p>现在我将新的光线步进方法更新到了我原来的ShaderToy Demo上，在768X432的分辨率60fps的情况下，我的demo在我的电脑上的GPU占用率由80%左右降低到了35%左右，可谓是巨大的提升。</p>\n<p>在demo的代码中，我在第一行添加了代码</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-meta\">#define OLD_METHOD 0</span><br></code></pre></td></tr></table></figure>\n<p>将<strong>OLD_METHOD</strong>改为1的话可以改为使用老方法，各位有兴趣的话可以实际修改一下代码来对比一下这两种方法的性能差异。</p>\n"},{"title":"程序化地形生成-1","date":"2024-10-11T14:59:27.000Z","index_img":"/2024/10/11/ProceduralTerrainGeneration/shadertoy_terrain.png","banner_img":"/2024/10/11/ProceduralTerrainGeneration/shadertoy_terrain.png","_content":"\n[ShaderToy](https://www.shadertoy.com/)是一个很有趣的网站，它上面有着非常多的渲染案例分享，最近一段时间我也是沉迷了。在看了不少大佬的作品之后，不禁手痒。前一段时间看了Inigo大佬的一个[教程案例](https://www.shadertoy.com/view/4ttSWf)，想着把这个效果自己来实现一次，因此就有了今天的这篇文章。\n\n我最终的成品也放到了shadertoy上面，有兴趣的同学可以一起讨论参考一下。看起来还不错对吧，虽然还有不少地方需要完善，但这个demo已经实现了我心中的大部分效果，包括无限的基于噪音的地形生成、地形阴影、雾气、云等等。\n\n<iframe width=\"640\" height=\"360\" frameborder=\"0\" src=\"https://www.shadertoy.com/embed/4XByRV?gui=true&t=10&paused=true&muted=false\" allowfullscreen></iframe>\n\n那么下面，就让我来一步步说明这个demo的实现过程吧。\n\n# 基础知识\n## 在ST上渲染地形\n对ShaderToy上运行的Shader代码，对应着可编程渲染管线的片段着色器(或者叫像素着色器)。片段着色器主要是是图形光栅化后的像素信息，所以渲染3D场景需要进行一些额外的步骤。\n\nShaderToy的程序一般是这样的：\n```c\nvoid mainImage(out vec4 fragColor, in vec2 fragCoord)\n{\n\t...\n}\n```\n**fragColor**是输出，代表这这个像素的最终颜色；**fragCoord**是输入，代表这个像素点的xy坐标。ShaderToy提供了固定变量**iResolution**用来表示整个屏幕的xy的分辨率。\n\n为了渲染3D物体，我们需要采用ray cast/marching的方法，构建一个相机的位置作为光线射出的起点**ro**，再根据当前像素点的坐标和ro的差获得光线射出的方向**rd**。\n```c\nvoid mainImage(out vec4 fragColor, in vec2 fragCoord)\n{\n    vec2 uv = fragCoord / iResolution.xy;\n\t// 以屏幕中心为（0,0）\n    uv = uv * 2.0 - 1.0;\n\t// 缩放x，在画面拉伸的时候保证比例正确\n    uv.x *= iResolution.x/iResolution.y;\n\t// 原点位置\n    vec3 ro = vec3(0, 0, -1);\n    // 射线方向\n    vec3 rd = normalize(vec3(uv, 2));\n\n\tfragColor = rayMarching(ro, rd);\n}\n```\n\n## 和地形相交\n在shadertoy中渲染3D物体，一般是使用raymarching方法配合SDFs来渲染3D的物体。SDF（Signed Distance Field）是一种物体的隐式表达，用于存储和计算点到图形表面的最近距离。经由一个起点和一个方向，可以用SDF来达到低消耗的射线检测效果。\n\n这里可以参考Inigo对SDF的介绍的介绍：https://iquilezles.org/articles/distfunctions/\n\n地形的渲染也是类似的，我们通过ray marching方法来找到距离地形最近的点，以此来获取地形的形状。但是和SDF不同的是，我们无法很轻易的判断射线当前距离地形的最近距离，尤其是当我们的地形完全是通过噪音来随机生成的时候，这变成了一个不可能完成的任务。所以在判断地形相交的时候，只能回归到笨办法，一步一步慢慢的往前“挪”，*若当前的顶点在地形之下，而之前的一个迭代在地形之上的话*，那我们就找到了击中地表的区间段。\n![射线和地表相交](https://iquilezles.org/articles/terrainmarching/gfx02.png)\n``` c\nbool rayMarch(vec3 ro, vec3 rd, out float hit_t)\n{\n\tconst float dt = 0.01f;\n\tconst float min_t = 1e-3;\n\tconst float max_t = 1e3;\n\tfor(float t = min_t; t < max_t; t+=dt)\n\t{\n\t\tconst vec3 p = ro+rd*t;\n\t\tif(p.y < f(p.x, p.z));\n\t\t{\n\t\t\t// 取中间点减小误差\n\t\t\thit_t = t - 0.5f*dt;\n\t\t\treturn true;\n\t\t}\n\t}\n\treturn false;\n}\n```\n这个方法简单易懂，但显而易见在性能上并不是最优的，尤其是涉及到范围很大的地形的时候，dt的值如果取得太小，那么渲染完成一个场景的时间将会非常的长，消耗巨大；而若是dt的值取得太大，则很有可能会出现取值错误的情况。\n\n当场景距离我们足够远的时候，由于透视的原因，近大远小，远处的场景精度对于观察者来说是越来越不重要了，因此dt的值可以随着光线步近而逐渐组建增大，动态变化。在合适的dt取值和变化曲线下，能够满足精度和性能的要求。Inigo给出的方法是类似这样的：\n```c\n//其他和上方代码一致\nfor(float t = min_t; t<max_t; t+=dt)\n{\n    const vec3 p = ro+rd*t;\n    const float h = f(p.xz);\n    if(p.y<h)\n    {\n        hit_t = t - 0.5f*dt;\n        return true;\n    }\n    dt=0.01f*t;\n}\nreturn false;\n```\nt的起始值和dt的增长倍数可以自己尝试选择一个合适的值。\n\n另外，如果我们能对最终渲染的效果有所了解的话，可以通过过滤掉很多不需要做射线检测的情况来极大的提升性能。如果我们最终的效果是一个在空中的相机，天空和地面占据画面各一半的话，那么上半部分的画面（通过rd.y>0判断）是可以完全跳过射线检测的。或者通过增加min_t的值来减少前期昂贵且不必要的性能消耗。\n\n在相交点的取值上，也可以进一步优化。原来仅仅是取两次光线步近的平均值，我们可以额外获取两次步近时位置的地形高度，用高度变化的连线和光线步近的线段做相交的判定取交点。这样得到的值将会更加精确。\n```c\n//其他和上方代码一致\nfloat lh = 0.0f;\nfloat ly = 0.0f;\nfor(float t = min_t; t<max_t; t+=dt)\n{\n    const vec3 p = ro+rd*t;\n    const float h = f(p.xz);\n    if(p.y<h)\n    {\n        // 计算两个线段的相交点\n        hit_t = t - dt + dt*(lh-ly)/(p.y-ly-h+lh);\n        return true;\n    }\n    dt=0.01f*t;\n    lh = h;\n    ly = p.y;\n}\nreturn false;\n```\n至此，我们就可以在ShaderToy渲染出地形了。\n\n# 地形生成\n## 生成的基础：噪音\n当我们提到噪音，往往会很生活化的把噪音和声音连接起来，从声学的角度来说是正确的。噪音其实可以用来表示所有通过振幅（amplitude）和频率（frequency）描述的波动，它可以是声音，它可以是辐射，也可以是其他的任意一种波动。\n\n在数学课上，我们学过正弦、余弦等三角函数，sin和cos其实就是一种噪音的表现方式。\n```c\nfloat amplitude = 1.0;\nfloat frequencey = 1.0;\nfloat y = amplitude * sin(frequency * x);\n```\n就像上面的代码所示，通过改变amplitude和frequency，我们可以改变sin波形的状态。\n\n噪音在很多程序化生成算法中都有着举足轻重的地位。\n\n## 分形布朗运动\n噪音是一种波，它是可以相互叠加的。两个相同的sin波形叠加会形成振幅更加强大的sin波形，而频率相差π/2的两个sin波形叠加后会相互抵消。\n\n在地形随机生成中，为了最终的结果噪音有着更好的随机性和更好的细节，将会循环多次计算噪音，循环的次数为我们称之为octave。每次循环的同一个噪音以一定倍数（lacunarity）升高频率，同时以一定比例（gain）降低振幅，最终将每个噪音计算的结果叠加得到一个最终的噪音，这个噪音的生成技术叫做“分形布朗运动”（fractal brownian motion，fbm）。\n\n下面是分形布朗运动的一个简单的代码演示：\n```c\nfloat fbm(vec2 uv, float frequency, float amplitude, int octave)\n{\n\tfloat lacunarity = 2.0;\n\tfloat gain = 0.5;\n\tfloat noise_val = 0.0;\n\tfloat amp = amplitude;\n\tfor(int index = 0; index < octave; ++index)\n\t{\n\t\tnose_val += noiseInterpolate(uv * frequency) * amp;\n\t\tamp *= gain;\n\t\tfrequency *= lacunarity;\n\t}\n\t\n\treturn noise_val;\n}\n```\n其中noiseInterpolate可以是perlin noise或者是simplex noise等任意一种噪音算法。\ndemo中的地形生成和云层的生成，也使用了该技术。关于FBM除了上面简单的使用还有很多其他的变种，这里我们就不扩展了，后面有机会的话可以专门介绍一下。\n\n## 地形的基础表现\n这里我将地形部分拆解出来。demo的地形计算使用了perlin noise，octave数量达到了11。更多的octave数量会给地形带来更多的细节，但是一般来说后面的效果收益会越来越少。下方是octave数量分布为5和11下的地形的形状对比。\n![](shadertoy_oc5_noshadow.png)\n![](shadertoy_oc11_noshadow.png)\n\n除了每次叠加噪音会进行频率和振幅的变化，为了获得更好的随机性，以及进一步减少噪音可能出现的重复pattern，可以将噪音进行旋转（也就是将传入的uv或者是坐标乘以一个默认的旋转矩阵）后再叠加到原来的噪音上。\n\n我们也需要地形的法线来和光源结合，渲染出地形的明暗部分。获得法线的方法有很多种，可以采样当前计算的地形上点的x轴和z轴（这里假定y轴为up）方向不远的一两个点，和目标点相减得到切线和副切线方向，通过叉乘得到目标点的法线。亦或是采样其他点后通过中心差分法求得目标点的法线。\n\n## 阴影\n仅仅通过法线来渲染地形的明部和暗部是不够的，我们还需要计算地形投射在地表上的阴影。地形的阴影计算原理非常简单，就是将地形上渲染的目标点，沿着光源方向进行射线检测，如果和地形相交的话，那该点就是处于阴影之下。理想情况下，射线检测的距离当然是实际上光源和地形上的点的距离，但是往往由于性能的原因，我们需要缩短这个距离。*实际的检测距离可以结合当前点的高度以及地形可能的最高位置进行计算*。\n\n在判断当前点处于阴影的时候，计算最终颜色的时候需要再乘以一个阴影的系数。\n![硬阴影](shadertoy_oc11_hardshadow.png)\n\n为了提升效果，我们通常不希望阴影的边缘非常生硬，而是希望有一种柔软的过度，这种更加符合现实的表现。实现这种软阴影的方法可能有很多种，这里采用的是Inigo教程的一种方法。\n\n上面提到判定阴影是通过从地形上面的点向光源方向做射线检测得到的，如果和地形相交则该点处于阴影当中，若不相交，则需要再取一个值，这值是地形向着光源方向移动距离t长度的位置，它和地形高度的差值d和距离t的比值的最小值，乘以某个常数X（10~32等等，可以自己尝试合适的范围）后经过smoothstep限制在（0,1）范围内。这个值作为阴影系数放入光照计算后就可以得到不错的软阴影效果。\n![软阴影](calc_soft_shadow.png)\n\n通过下面的对比图我们可以看到，在加入了软阴影计算后，地形阴影的边缘有了一种较为平滑的过度，显得没那么生硬了。想要更改软阴影的表现的话可以通过修改常数X。\n![](shadertoy_terrain.png)\n\n# 结语\n好了，我们已经得到了一个基础的程序化生成地形的效果了，但是它看起来还是有些单调。地形的深度表现、天空、云彩等等应该如何表现呢？\n\n无需着急，我们将会在后面的文章中对它进行进一步的优化。\n\n## 参考资料\nhttps://thebookofshaders.com/13/?lan=ch\nhttps://iquilezles.org/articles/morenoise\nhttps://youtu.be/BFld4EBO2RE?si=HWQMSNx5TBsOG_6g\n","source":"_posts/ProceduralTerrainGeneration.md","raw":"---\ntitle: 程序化地形生成-1\ndate: 2024-10-11 22:59:27\ncategories: \n\t- 技术漫谈\ntags: [3D, render, 渲染, 编程, 程序化生成]\n\t\nindex_img: /2024/10/11/ProceduralTerrainGeneration/shadertoy_terrain.png\nbanner_img: /2024/10/11/ProceduralTerrainGeneration/shadertoy_terrain.png\n---\n\n[ShaderToy](https://www.shadertoy.com/)是一个很有趣的网站，它上面有着非常多的渲染案例分享，最近一段时间我也是沉迷了。在看了不少大佬的作品之后，不禁手痒。前一段时间看了Inigo大佬的一个[教程案例](https://www.shadertoy.com/view/4ttSWf)，想着把这个效果自己来实现一次，因此就有了今天的这篇文章。\n\n我最终的成品也放到了shadertoy上面，有兴趣的同学可以一起讨论参考一下。看起来还不错对吧，虽然还有不少地方需要完善，但这个demo已经实现了我心中的大部分效果，包括无限的基于噪音的地形生成、地形阴影、雾气、云等等。\n\n<iframe width=\"640\" height=\"360\" frameborder=\"0\" src=\"https://www.shadertoy.com/embed/4XByRV?gui=true&t=10&paused=true&muted=false\" allowfullscreen></iframe>\n\n那么下面，就让我来一步步说明这个demo的实现过程吧。\n\n# 基础知识\n## 在ST上渲染地形\n对ShaderToy上运行的Shader代码，对应着可编程渲染管线的片段着色器(或者叫像素着色器)。片段着色器主要是是图形光栅化后的像素信息，所以渲染3D场景需要进行一些额外的步骤。\n\nShaderToy的程序一般是这样的：\n```c\nvoid mainImage(out vec4 fragColor, in vec2 fragCoord)\n{\n\t...\n}\n```\n**fragColor**是输出，代表这这个像素的最终颜色；**fragCoord**是输入，代表这个像素点的xy坐标。ShaderToy提供了固定变量**iResolution**用来表示整个屏幕的xy的分辨率。\n\n为了渲染3D物体，我们需要采用ray cast/marching的方法，构建一个相机的位置作为光线射出的起点**ro**，再根据当前像素点的坐标和ro的差获得光线射出的方向**rd**。\n```c\nvoid mainImage(out vec4 fragColor, in vec2 fragCoord)\n{\n    vec2 uv = fragCoord / iResolution.xy;\n\t// 以屏幕中心为（0,0）\n    uv = uv * 2.0 - 1.0;\n\t// 缩放x，在画面拉伸的时候保证比例正确\n    uv.x *= iResolution.x/iResolution.y;\n\t// 原点位置\n    vec3 ro = vec3(0, 0, -1);\n    // 射线方向\n    vec3 rd = normalize(vec3(uv, 2));\n\n\tfragColor = rayMarching(ro, rd);\n}\n```\n\n## 和地形相交\n在shadertoy中渲染3D物体，一般是使用raymarching方法配合SDFs来渲染3D的物体。SDF（Signed Distance Field）是一种物体的隐式表达，用于存储和计算点到图形表面的最近距离。经由一个起点和一个方向，可以用SDF来达到低消耗的射线检测效果。\n\n这里可以参考Inigo对SDF的介绍的介绍：https://iquilezles.org/articles/distfunctions/\n\n地形的渲染也是类似的，我们通过ray marching方法来找到距离地形最近的点，以此来获取地形的形状。但是和SDF不同的是，我们无法很轻易的判断射线当前距离地形的最近距离，尤其是当我们的地形完全是通过噪音来随机生成的时候，这变成了一个不可能完成的任务。所以在判断地形相交的时候，只能回归到笨办法，一步一步慢慢的往前“挪”，*若当前的顶点在地形之下，而之前的一个迭代在地形之上的话*，那我们就找到了击中地表的区间段。\n![射线和地表相交](https://iquilezles.org/articles/terrainmarching/gfx02.png)\n``` c\nbool rayMarch(vec3 ro, vec3 rd, out float hit_t)\n{\n\tconst float dt = 0.01f;\n\tconst float min_t = 1e-3;\n\tconst float max_t = 1e3;\n\tfor(float t = min_t; t < max_t; t+=dt)\n\t{\n\t\tconst vec3 p = ro+rd*t;\n\t\tif(p.y < f(p.x, p.z));\n\t\t{\n\t\t\t// 取中间点减小误差\n\t\t\thit_t = t - 0.5f*dt;\n\t\t\treturn true;\n\t\t}\n\t}\n\treturn false;\n}\n```\n这个方法简单易懂，但显而易见在性能上并不是最优的，尤其是涉及到范围很大的地形的时候，dt的值如果取得太小，那么渲染完成一个场景的时间将会非常的长，消耗巨大；而若是dt的值取得太大，则很有可能会出现取值错误的情况。\n\n当场景距离我们足够远的时候，由于透视的原因，近大远小，远处的场景精度对于观察者来说是越来越不重要了，因此dt的值可以随着光线步近而逐渐组建增大，动态变化。在合适的dt取值和变化曲线下，能够满足精度和性能的要求。Inigo给出的方法是类似这样的：\n```c\n//其他和上方代码一致\nfor(float t = min_t; t<max_t; t+=dt)\n{\n    const vec3 p = ro+rd*t;\n    const float h = f(p.xz);\n    if(p.y<h)\n    {\n        hit_t = t - 0.5f*dt;\n        return true;\n    }\n    dt=0.01f*t;\n}\nreturn false;\n```\nt的起始值和dt的增长倍数可以自己尝试选择一个合适的值。\n\n另外，如果我们能对最终渲染的效果有所了解的话，可以通过过滤掉很多不需要做射线检测的情况来极大的提升性能。如果我们最终的效果是一个在空中的相机，天空和地面占据画面各一半的话，那么上半部分的画面（通过rd.y>0判断）是可以完全跳过射线检测的。或者通过增加min_t的值来减少前期昂贵且不必要的性能消耗。\n\n在相交点的取值上，也可以进一步优化。原来仅仅是取两次光线步近的平均值，我们可以额外获取两次步近时位置的地形高度，用高度变化的连线和光线步近的线段做相交的判定取交点。这样得到的值将会更加精确。\n```c\n//其他和上方代码一致\nfloat lh = 0.0f;\nfloat ly = 0.0f;\nfor(float t = min_t; t<max_t; t+=dt)\n{\n    const vec3 p = ro+rd*t;\n    const float h = f(p.xz);\n    if(p.y<h)\n    {\n        // 计算两个线段的相交点\n        hit_t = t - dt + dt*(lh-ly)/(p.y-ly-h+lh);\n        return true;\n    }\n    dt=0.01f*t;\n    lh = h;\n    ly = p.y;\n}\nreturn false;\n```\n至此，我们就可以在ShaderToy渲染出地形了。\n\n# 地形生成\n## 生成的基础：噪音\n当我们提到噪音，往往会很生活化的把噪音和声音连接起来，从声学的角度来说是正确的。噪音其实可以用来表示所有通过振幅（amplitude）和频率（frequency）描述的波动，它可以是声音，它可以是辐射，也可以是其他的任意一种波动。\n\n在数学课上，我们学过正弦、余弦等三角函数，sin和cos其实就是一种噪音的表现方式。\n```c\nfloat amplitude = 1.0;\nfloat frequencey = 1.0;\nfloat y = amplitude * sin(frequency * x);\n```\n就像上面的代码所示，通过改变amplitude和frequency，我们可以改变sin波形的状态。\n\n噪音在很多程序化生成算法中都有着举足轻重的地位。\n\n## 分形布朗运动\n噪音是一种波，它是可以相互叠加的。两个相同的sin波形叠加会形成振幅更加强大的sin波形，而频率相差π/2的两个sin波形叠加后会相互抵消。\n\n在地形随机生成中，为了最终的结果噪音有着更好的随机性和更好的细节，将会循环多次计算噪音，循环的次数为我们称之为octave。每次循环的同一个噪音以一定倍数（lacunarity）升高频率，同时以一定比例（gain）降低振幅，最终将每个噪音计算的结果叠加得到一个最终的噪音，这个噪音的生成技术叫做“分形布朗运动”（fractal brownian motion，fbm）。\n\n下面是分形布朗运动的一个简单的代码演示：\n```c\nfloat fbm(vec2 uv, float frequency, float amplitude, int octave)\n{\n\tfloat lacunarity = 2.0;\n\tfloat gain = 0.5;\n\tfloat noise_val = 0.0;\n\tfloat amp = amplitude;\n\tfor(int index = 0; index < octave; ++index)\n\t{\n\t\tnose_val += noiseInterpolate(uv * frequency) * amp;\n\t\tamp *= gain;\n\t\tfrequency *= lacunarity;\n\t}\n\t\n\treturn noise_val;\n}\n```\n其中noiseInterpolate可以是perlin noise或者是simplex noise等任意一种噪音算法。\ndemo中的地形生成和云层的生成，也使用了该技术。关于FBM除了上面简单的使用还有很多其他的变种，这里我们就不扩展了，后面有机会的话可以专门介绍一下。\n\n## 地形的基础表现\n这里我将地形部分拆解出来。demo的地形计算使用了perlin noise，octave数量达到了11。更多的octave数量会给地形带来更多的细节，但是一般来说后面的效果收益会越来越少。下方是octave数量分布为5和11下的地形的形状对比。\n![](shadertoy_oc5_noshadow.png)\n![](shadertoy_oc11_noshadow.png)\n\n除了每次叠加噪音会进行频率和振幅的变化，为了获得更好的随机性，以及进一步减少噪音可能出现的重复pattern，可以将噪音进行旋转（也就是将传入的uv或者是坐标乘以一个默认的旋转矩阵）后再叠加到原来的噪音上。\n\n我们也需要地形的法线来和光源结合，渲染出地形的明暗部分。获得法线的方法有很多种，可以采样当前计算的地形上点的x轴和z轴（这里假定y轴为up）方向不远的一两个点，和目标点相减得到切线和副切线方向，通过叉乘得到目标点的法线。亦或是采样其他点后通过中心差分法求得目标点的法线。\n\n## 阴影\n仅仅通过法线来渲染地形的明部和暗部是不够的，我们还需要计算地形投射在地表上的阴影。地形的阴影计算原理非常简单，就是将地形上渲染的目标点，沿着光源方向进行射线检测，如果和地形相交的话，那该点就是处于阴影之下。理想情况下，射线检测的距离当然是实际上光源和地形上的点的距离，但是往往由于性能的原因，我们需要缩短这个距离。*实际的检测距离可以结合当前点的高度以及地形可能的最高位置进行计算*。\n\n在判断当前点处于阴影的时候，计算最终颜色的时候需要再乘以一个阴影的系数。\n![硬阴影](shadertoy_oc11_hardshadow.png)\n\n为了提升效果，我们通常不希望阴影的边缘非常生硬，而是希望有一种柔软的过度，这种更加符合现实的表现。实现这种软阴影的方法可能有很多种，这里采用的是Inigo教程的一种方法。\n\n上面提到判定阴影是通过从地形上面的点向光源方向做射线检测得到的，如果和地形相交则该点处于阴影当中，若不相交，则需要再取一个值，这值是地形向着光源方向移动距离t长度的位置，它和地形高度的差值d和距离t的比值的最小值，乘以某个常数X（10~32等等，可以自己尝试合适的范围）后经过smoothstep限制在（0,1）范围内。这个值作为阴影系数放入光照计算后就可以得到不错的软阴影效果。\n![软阴影](calc_soft_shadow.png)\n\n通过下面的对比图我们可以看到，在加入了软阴影计算后，地形阴影的边缘有了一种较为平滑的过度，显得没那么生硬了。想要更改软阴影的表现的话可以通过修改常数X。\n![](shadertoy_terrain.png)\n\n# 结语\n好了，我们已经得到了一个基础的程序化生成地形的效果了，但是它看起来还是有些单调。地形的深度表现、天空、云彩等等应该如何表现呢？\n\n无需着急，我们将会在后面的文章中对它进行进一步的优化。\n\n## 参考资料\nhttps://thebookofshaders.com/13/?lan=ch\nhttps://iquilezles.org/articles/morenoise\nhttps://youtu.be/BFld4EBO2RE?si=HWQMSNx5TBsOG_6g\n","slug":"ProceduralTerrainGeneration","published":1,"updated":"2024-11-05T15:30:48.824Z","comments":1,"layout":"post","photos":[],"_id":"cm5ht44vd000334574tjcdf63","content":"<p><a href=\"https://www.shadertoy.com/\">ShaderToy</a>是一个很有趣的网站，它上面有着非常多的渲染案例分享，最近一段时间我也是沉迷了。在看了不少大佬的作品之后，不禁手痒。前一段时间看了Inigo大佬的一个<a href=\"https://www.shadertoy.com/view/4ttSWf\">教程案例</a>，想着把这个效果自己来实现一次，因此就有了今天的这篇文章。</p>\n<p>我最终的成品也放到了shadertoy上面，有兴趣的同学可以一起讨论参考一下。看起来还不错对吧，虽然还有不少地方需要完善，但这个demo已经实现了我心中的大部分效果，包括无限的基于噪音的地形生成、地形阴影、雾气、云等等。</p>\n<iframe width=\"640\" height=\"360\" frameborder=\"0\" src=\"https://www.shadertoy.com/embed/4XByRV?gui=true&t=10&paused=true&muted=false\" allowfullscreen></iframe>\n\n<p>那么下面，就让我来一步步说明这个demo的实现过程吧。</p>\n<h1 id=\"基础知识\"><a href=\"#基础知识\" class=\"headerlink\" title=\"基础知识\"></a>基础知识</h1><h2 id=\"在ST上渲染地形\"><a href=\"#在ST上渲染地形\" class=\"headerlink\" title=\"在ST上渲染地形\"></a>在ST上渲染地形</h2><p>对ShaderToy上运行的Shader代码，对应着可编程渲染管线的片段着色器(或者叫像素着色器)。片段着色器主要是是图形光栅化后的像素信息，所以渲染3D场景需要进行一些额外的步骤。</p>\n<p>ShaderToy的程序一般是这样的：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c\"><span class=\"hljs-type\">void</span> <span class=\"hljs-title function_\">mainImage</span><span class=\"hljs-params\">(out vec4 fragColor, in vec2 fragCoord)</span><br>&#123;<br>\t...<br>&#125;<br></code></pre></td></tr></table></figure>\n<p><strong>fragColor</strong>是输出，代表这这个像素的最终颜色；<strong>fragCoord</strong>是输入，代表这个像素点的xy坐标。ShaderToy提供了固定变量<strong>iResolution</strong>用来表示整个屏幕的xy的分辨率。</p>\n<p>为了渲染3D物体，我们需要采用ray cast&#x2F;marching的方法，构建一个相机的位置作为光线射出的起点<strong>ro</strong>，再根据当前像素点的坐标和ro的差获得光线射出的方向<strong>rd</strong>。</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c\"><span class=\"hljs-type\">void</span> <span class=\"hljs-title function_\">mainImage</span><span class=\"hljs-params\">(out vec4 fragColor, in vec2 fragCoord)</span><br>&#123;<br>    vec2 uv = fragCoord / iResolution.xy;<br>\t<span class=\"hljs-comment\">// 以屏幕中心为（0,0）</span><br>    uv = uv * <span class=\"hljs-number\">2.0</span> - <span class=\"hljs-number\">1.0</span>;<br>\t<span class=\"hljs-comment\">// 缩放x，在画面拉伸的时候保证比例正确</span><br>    uv.x *= iResolution.x/iResolution.y;<br>\t<span class=\"hljs-comment\">// 原点位置</span><br>    vec3 ro = vec3(<span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">-1</span>);<br>    <span class=\"hljs-comment\">// 射线方向</span><br>    vec3 rd = normalize(vec3(uv, <span class=\"hljs-number\">2</span>));<br><br>\tfragColor = rayMarching(ro, rd);<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<h2 id=\"和地形相交\"><a href=\"#和地形相交\" class=\"headerlink\" title=\"和地形相交\"></a>和地形相交</h2><p>在shadertoy中渲染3D物体，一般是使用raymarching方法配合SDFs来渲染3D的物体。SDF（Signed Distance Field）是一种物体的隐式表达，用于存储和计算点到图形表面的最近距离。经由一个起点和一个方向，可以用SDF来达到低消耗的射线检测效果。</p>\n<p>这里可以参考Inigo对SDF的介绍的介绍：<a href=\"https://iquilezles.org/articles/distfunctions/\">https://iquilezles.org/articles/distfunctions/</a></p>\n<p>地形的渲染也是类似的，我们通过ray marching方法来找到距离地形最近的点，以此来获取地形的形状。但是和SDF不同的是，我们无法很轻易的判断射线当前距离地形的最近距离，尤其是当我们的地形完全是通过噪音来随机生成的时候，这变成了一个不可能完成的任务。所以在判断地形相交的时候，只能回归到笨办法，一步一步慢慢的往前“挪”，<em>若当前的顶点在地形之下，而之前的一个迭代在地形之上的话</em>，那我们就找到了击中地表的区间段。<br><img src=\"https://iquilezles.org/articles/terrainmarching/gfx02.png\" alt=\"射线和地表相交\"></p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c\"><span class=\"hljs-type\">bool</span> <span class=\"hljs-title function_\">rayMarch</span><span class=\"hljs-params\">(vec3 ro, vec3 rd, out <span class=\"hljs-type\">float</span> <span class=\"hljs-type\">hit_t</span>)</span><br>&#123;<br>\t<span class=\"hljs-type\">const</span> <span class=\"hljs-type\">float</span> dt = <span class=\"hljs-number\">0.01f</span>;<br>\t<span class=\"hljs-type\">const</span> <span class=\"hljs-type\">float</span> <span class=\"hljs-type\">min_t</span> = <span class=\"hljs-number\">1e-3</span>;<br>\t<span class=\"hljs-type\">const</span> <span class=\"hljs-type\">float</span> <span class=\"hljs-type\">max_t</span> = <span class=\"hljs-number\">1e3</span>;<br>\t<span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">float</span> t = <span class=\"hljs-type\">min_t</span>; t &lt; <span class=\"hljs-type\">max_t</span>; t+=dt)<br>\t&#123;<br>\t\t<span class=\"hljs-type\">const</span> vec3 p = ro+rd*t;<br>\t\t<span class=\"hljs-keyword\">if</span>(p.y &lt; f(p.x, p.z));<br>\t\t&#123;<br>\t\t\t<span class=\"hljs-comment\">// 取中间点减小误差</span><br>\t\t\t<span class=\"hljs-type\">hit_t</span> = t - <span class=\"hljs-number\">0.5f</span>*dt;<br>\t\t\t<span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">true</span>;<br>\t\t&#125;<br>\t&#125;<br>\t<span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">false</span>;<br>&#125;<br></code></pre></td></tr></table></figure>\n<p>这个方法简单易懂，但显而易见在性能上并不是最优的，尤其是涉及到范围很大的地形的时候，dt的值如果取得太小，那么渲染完成一个场景的时间将会非常的长，消耗巨大；而若是dt的值取得太大，则很有可能会出现取值错误的情况。</p>\n<p>当场景距离我们足够远的时候，由于透视的原因，近大远小，远处的场景精度对于观察者来说是越来越不重要了，因此dt的值可以随着光线步近而逐渐组建增大，动态变化。在合适的dt取值和变化曲线下，能够满足精度和性能的要求。Inigo给出的方法是类似这样的：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c\"><span class=\"hljs-comment\">//其他和上方代码一致</span><br><span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">float</span> t = <span class=\"hljs-type\">min_t</span>; t&lt;<span class=\"hljs-type\">max_t</span>; t+=dt)<br>&#123;<br>    <span class=\"hljs-type\">const</span> vec3 p = ro+rd*t;<br>    <span class=\"hljs-type\">const</span> <span class=\"hljs-type\">float</span> h = f(p.xz);<br>    <span class=\"hljs-keyword\">if</span>(p.y&lt;h)<br>    &#123;<br>        <span class=\"hljs-type\">hit_t</span> = t - <span class=\"hljs-number\">0.5f</span>*dt;<br>        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">true</span>;<br>    &#125;<br>    dt=<span class=\"hljs-number\">0.01f</span>*t;<br>&#125;<br><span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">false</span>;<br></code></pre></td></tr></table></figure>\n<p>t的起始值和dt的增长倍数可以自己尝试选择一个合适的值。</p>\n<p>另外，如果我们能对最终渲染的效果有所了解的话，可以通过过滤掉很多不需要做射线检测的情况来极大的提升性能。如果我们最终的效果是一个在空中的相机，天空和地面占据画面各一半的话，那么上半部分的画面（通过rd.y&gt;0判断）是可以完全跳过射线检测的。或者通过增加min_t的值来减少前期昂贵且不必要的性能消耗。</p>\n<p>在相交点的取值上，也可以进一步优化。原来仅仅是取两次光线步近的平均值，我们可以额外获取两次步近时位置的地形高度，用高度变化的连线和光线步近的线段做相交的判定取交点。这样得到的值将会更加精确。</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c\"><span class=\"hljs-comment\">//其他和上方代码一致</span><br><span class=\"hljs-type\">float</span> lh = <span class=\"hljs-number\">0.0f</span>;<br><span class=\"hljs-type\">float</span> ly = <span class=\"hljs-number\">0.0f</span>;<br><span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">float</span> t = <span class=\"hljs-type\">min_t</span>; t&lt;<span class=\"hljs-type\">max_t</span>; t+=dt)<br>&#123;<br>    <span class=\"hljs-type\">const</span> vec3 p = ro+rd*t;<br>    <span class=\"hljs-type\">const</span> <span class=\"hljs-type\">float</span> h = f(p.xz);<br>    <span class=\"hljs-keyword\">if</span>(p.y&lt;h)<br>    &#123;<br>        <span class=\"hljs-comment\">// 计算两个线段的相交点</span><br>        <span class=\"hljs-type\">hit_t</span> = t - dt + dt*(lh-ly)/(p.y-ly-h+lh);<br>        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">true</span>;<br>    &#125;<br>    dt=<span class=\"hljs-number\">0.01f</span>*t;<br>    lh = h;<br>    ly = p.y;<br>&#125;<br><span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">false</span>;<br></code></pre></td></tr></table></figure>\n<p>至此，我们就可以在ShaderToy渲染出地形了。</p>\n<h1 id=\"地形生成\"><a href=\"#地形生成\" class=\"headerlink\" title=\"地形生成\"></a>地形生成</h1><h2 id=\"生成的基础：噪音\"><a href=\"#生成的基础：噪音\" class=\"headerlink\" title=\"生成的基础：噪音\"></a>生成的基础：噪音</h2><p>当我们提到噪音，往往会很生活化的把噪音和声音连接起来，从声学的角度来说是正确的。噪音其实可以用来表示所有通过振幅（amplitude）和频率（frequency）描述的波动，它可以是声音，它可以是辐射，也可以是其他的任意一种波动。</p>\n<p>在数学课上，我们学过正弦、余弦等三角函数，sin和cos其实就是一种噪音的表现方式。</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c\"><span class=\"hljs-type\">float</span> amplitude = <span class=\"hljs-number\">1.0</span>;<br><span class=\"hljs-type\">float</span> frequencey = <span class=\"hljs-number\">1.0</span>;<br><span class=\"hljs-type\">float</span> y = amplitude * <span class=\"hljs-built_in\">sin</span>(frequency * x);<br></code></pre></td></tr></table></figure>\n<p>就像上面的代码所示，通过改变amplitude和frequency，我们可以改变sin波形的状态。</p>\n<p>噪音在很多程序化生成算法中都有着举足轻重的地位。</p>\n<h2 id=\"分形布朗运动\"><a href=\"#分形布朗运动\" class=\"headerlink\" title=\"分形布朗运动\"></a>分形布朗运动</h2><p>噪音是一种波，它是可以相互叠加的。两个相同的sin波形叠加会形成振幅更加强大的sin波形，而频率相差π&#x2F;2的两个sin波形叠加后会相互抵消。</p>\n<p>在地形随机生成中，为了最终的结果噪音有着更好的随机性和更好的细节，将会循环多次计算噪音，循环的次数为我们称之为octave。每次循环的同一个噪音以一定倍数（lacunarity）升高频率，同时以一定比例（gain）降低振幅，最终将每个噪音计算的结果叠加得到一个最终的噪音，这个噪音的生成技术叫做“分形布朗运动”（fractal brownian motion，fbm）。</p>\n<p>下面是分形布朗运动的一个简单的代码演示：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c\"><span class=\"hljs-type\">float</span> <span class=\"hljs-title function_\">fbm</span><span class=\"hljs-params\">(vec2 uv, <span class=\"hljs-type\">float</span> frequency, <span class=\"hljs-type\">float</span> amplitude, <span class=\"hljs-type\">int</span> octave)</span><br>&#123;<br>\t<span class=\"hljs-type\">float</span> lacunarity = <span class=\"hljs-number\">2.0</span>;<br>\t<span class=\"hljs-type\">float</span> gain = <span class=\"hljs-number\">0.5</span>;<br>\t<span class=\"hljs-type\">float</span> noise_val = <span class=\"hljs-number\">0.0</span>;<br>\t<span class=\"hljs-type\">float</span> amp = amplitude;<br>\t<span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">int</span> index = <span class=\"hljs-number\">0</span>; index &lt; octave; ++index)<br>\t&#123;<br>\t\tnose_val += noiseInterpolate(uv * frequency) * amp;<br>\t\tamp *= gain;<br>\t\tfrequency *= lacunarity;<br>\t&#125;<br>\t<br>\t<span class=\"hljs-keyword\">return</span> noise_val;<br>&#125;<br></code></pre></td></tr></table></figure>\n<p>其中noiseInterpolate可以是perlin noise或者是simplex noise等任意一种噪音算法。<br>demo中的地形生成和云层的生成，也使用了该技术。关于FBM除了上面简单的使用还有很多其他的变种，这里我们就不扩展了，后面有机会的话可以专门介绍一下。</p>\n<h2 id=\"地形的基础表现\"><a href=\"#地形的基础表现\" class=\"headerlink\" title=\"地形的基础表现\"></a>地形的基础表现</h2><p>这里我将地形部分拆解出来。demo的地形计算使用了perlin noise，octave数量达到了11。更多的octave数量会给地形带来更多的细节，但是一般来说后面的效果收益会越来越少。下方是octave数量分布为5和11下的地形的形状对比。<br><img src=\"/2024/10/11/ProceduralTerrainGeneration/shadertoy_oc5_noshadow.png\"><br><img src=\"/2024/10/11/ProceduralTerrainGeneration/shadertoy_oc11_noshadow.png\"></p>\n<p>除了每次叠加噪音会进行频率和振幅的变化，为了获得更好的随机性，以及进一步减少噪音可能出现的重复pattern，可以将噪音进行旋转（也就是将传入的uv或者是坐标乘以一个默认的旋转矩阵）后再叠加到原来的噪音上。</p>\n<p>我们也需要地形的法线来和光源结合，渲染出地形的明暗部分。获得法线的方法有很多种，可以采样当前计算的地形上点的x轴和z轴（这里假定y轴为up）方向不远的一两个点，和目标点相减得到切线和副切线方向，通过叉乘得到目标点的法线。亦或是采样其他点后通过中心差分法求得目标点的法线。</p>\n<h2 id=\"阴影\"><a href=\"#阴影\" class=\"headerlink\" title=\"阴影\"></a>阴影</h2><p>仅仅通过法线来渲染地形的明部和暗部是不够的，我们还需要计算地形投射在地表上的阴影。地形的阴影计算原理非常简单，就是将地形上渲染的目标点，沿着光源方向进行射线检测，如果和地形相交的话，那该点就是处于阴影之下。理想情况下，射线检测的距离当然是实际上光源和地形上的点的距离，但是往往由于性能的原因，我们需要缩短这个距离。<em>实际的检测距离可以结合当前点的高度以及地形可能的最高位置进行计算</em>。</p>\n<p>在判断当前点处于阴影的时候，计算最终颜色的时候需要再乘以一个阴影的系数。<br><img src=\"/2024/10/11/ProceduralTerrainGeneration/shadertoy_oc11_hardshadow.png\" alt=\"硬阴影\"></p>\n<p>为了提升效果，我们通常不希望阴影的边缘非常生硬，而是希望有一种柔软的过度，这种更加符合现实的表现。实现这种软阴影的方法可能有很多种，这里采用的是Inigo教程的一种方法。</p>\n<p>上面提到判定阴影是通过从地形上面的点向光源方向做射线检测得到的，如果和地形相交则该点处于阴影当中，若不相交，则需要再取一个值，这值是地形向着光源方向移动距离t长度的位置，它和地形高度的差值d和距离t的比值的最小值，乘以某个常数X（10~32等等，可以自己尝试合适的范围）后经过smoothstep限制在（0,1）范围内。这个值作为阴影系数放入光照计算后就可以得到不错的软阴影效果。<br><img src=\"/2024/10/11/ProceduralTerrainGeneration/calc_soft_shadow.png\" alt=\"软阴影\"></p>\n<p>通过下面的对比图我们可以看到，在加入了软阴影计算后，地形阴影的边缘有了一种较为平滑的过度，显得没那么生硬了。想要更改软阴影的表现的话可以通过修改常数X。<br><img src=\"/2024/10/11/ProceduralTerrainGeneration/shadertoy_terrain.png\"></p>\n<h1 id=\"结语\"><a href=\"#结语\" class=\"headerlink\" title=\"结语\"></a>结语</h1><p>好了，我们已经得到了一个基础的程序化生成地形的效果了，但是它看起来还是有些单调。地形的深度表现、天空、云彩等等应该如何表现呢？</p>\n<p>无需着急，我们将会在后面的文章中对它进行进一步的优化。</p>\n<h2 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h2><p><a href=\"https://thebookofshaders.com/13/?lan=ch\">https://thebookofshaders.com/13/?lan=ch</a><br><a href=\"https://iquilezles.org/articles/morenoise\">https://iquilezles.org/articles/morenoise</a><br><a href=\"https://youtu.be/BFld4EBO2RE?si=HWQMSNx5TBsOG_6g\">https://youtu.be/BFld4EBO2RE?si=HWQMSNx5TBsOG_6g</a></p>\n","excerpt":"","more":"<p><a href=\"https://www.shadertoy.com/\">ShaderToy</a>是一个很有趣的网站，它上面有着非常多的渲染案例分享，最近一段时间我也是沉迷了。在看了不少大佬的作品之后，不禁手痒。前一段时间看了Inigo大佬的一个<a href=\"https://www.shadertoy.com/view/4ttSWf\">教程案例</a>，想着把这个效果自己来实现一次，因此就有了今天的这篇文章。</p>\n<p>我最终的成品也放到了shadertoy上面，有兴趣的同学可以一起讨论参考一下。看起来还不错对吧，虽然还有不少地方需要完善，但这个demo已经实现了我心中的大部分效果，包括无限的基于噪音的地形生成、地形阴影、雾气、云等等。</p>\n<iframe width=\"640\" height=\"360\" frameborder=\"0\" src=\"https://www.shadertoy.com/embed/4XByRV?gui=true&t=10&paused=true&muted=false\" allowfullscreen></iframe>\n\n<p>那么下面，就让我来一步步说明这个demo的实现过程吧。</p>\n<h1 id=\"基础知识\"><a href=\"#基础知识\" class=\"headerlink\" title=\"基础知识\"></a>基础知识</h1><h2 id=\"在ST上渲染地形\"><a href=\"#在ST上渲染地形\" class=\"headerlink\" title=\"在ST上渲染地形\"></a>在ST上渲染地形</h2><p>对ShaderToy上运行的Shader代码，对应着可编程渲染管线的片段着色器(或者叫像素着色器)。片段着色器主要是是图形光栅化后的像素信息，所以渲染3D场景需要进行一些额外的步骤。</p>\n<p>ShaderToy的程序一般是这样的：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c\"><span class=\"hljs-type\">void</span> <span class=\"hljs-title function_\">mainImage</span><span class=\"hljs-params\">(out vec4 fragColor, in vec2 fragCoord)</span><br>&#123;<br>\t...<br>&#125;<br></code></pre></td></tr></table></figure>\n<p><strong>fragColor</strong>是输出，代表这这个像素的最终颜色；<strong>fragCoord</strong>是输入，代表这个像素点的xy坐标。ShaderToy提供了固定变量<strong>iResolution</strong>用来表示整个屏幕的xy的分辨率。</p>\n<p>为了渲染3D物体，我们需要采用ray cast&#x2F;marching的方法，构建一个相机的位置作为光线射出的起点<strong>ro</strong>，再根据当前像素点的坐标和ro的差获得光线射出的方向<strong>rd</strong>。</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c\"><span class=\"hljs-type\">void</span> <span class=\"hljs-title function_\">mainImage</span><span class=\"hljs-params\">(out vec4 fragColor, in vec2 fragCoord)</span><br>&#123;<br>    vec2 uv = fragCoord / iResolution.xy;<br>\t<span class=\"hljs-comment\">// 以屏幕中心为（0,0）</span><br>    uv = uv * <span class=\"hljs-number\">2.0</span> - <span class=\"hljs-number\">1.0</span>;<br>\t<span class=\"hljs-comment\">// 缩放x，在画面拉伸的时候保证比例正确</span><br>    uv.x *= iResolution.x/iResolution.y;<br>\t<span class=\"hljs-comment\">// 原点位置</span><br>    vec3 ro = vec3(<span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">-1</span>);<br>    <span class=\"hljs-comment\">// 射线方向</span><br>    vec3 rd = normalize(vec3(uv, <span class=\"hljs-number\">2</span>));<br><br>\tfragColor = rayMarching(ro, rd);<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<h2 id=\"和地形相交\"><a href=\"#和地形相交\" class=\"headerlink\" title=\"和地形相交\"></a>和地形相交</h2><p>在shadertoy中渲染3D物体，一般是使用raymarching方法配合SDFs来渲染3D的物体。SDF（Signed Distance Field）是一种物体的隐式表达，用于存储和计算点到图形表面的最近距离。经由一个起点和一个方向，可以用SDF来达到低消耗的射线检测效果。</p>\n<p>这里可以参考Inigo对SDF的介绍的介绍：<a href=\"https://iquilezles.org/articles/distfunctions/\">https://iquilezles.org/articles/distfunctions/</a></p>\n<p>地形的渲染也是类似的，我们通过ray marching方法来找到距离地形最近的点，以此来获取地形的形状。但是和SDF不同的是，我们无法很轻易的判断射线当前距离地形的最近距离，尤其是当我们的地形完全是通过噪音来随机生成的时候，这变成了一个不可能完成的任务。所以在判断地形相交的时候，只能回归到笨办法，一步一步慢慢的往前“挪”，<em>若当前的顶点在地形之下，而之前的一个迭代在地形之上的话</em>，那我们就找到了击中地表的区间段。<br><img src=\"https://iquilezles.org/articles/terrainmarching/gfx02.png\" alt=\"射线和地表相交\"></p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c\"><span class=\"hljs-type\">bool</span> <span class=\"hljs-title function_\">rayMarch</span><span class=\"hljs-params\">(vec3 ro, vec3 rd, out <span class=\"hljs-type\">float</span> <span class=\"hljs-type\">hit_t</span>)</span><br>&#123;<br>\t<span class=\"hljs-type\">const</span> <span class=\"hljs-type\">float</span> dt = <span class=\"hljs-number\">0.01f</span>;<br>\t<span class=\"hljs-type\">const</span> <span class=\"hljs-type\">float</span> <span class=\"hljs-type\">min_t</span> = <span class=\"hljs-number\">1e-3</span>;<br>\t<span class=\"hljs-type\">const</span> <span class=\"hljs-type\">float</span> <span class=\"hljs-type\">max_t</span> = <span class=\"hljs-number\">1e3</span>;<br>\t<span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">float</span> t = <span class=\"hljs-type\">min_t</span>; t &lt; <span class=\"hljs-type\">max_t</span>; t+=dt)<br>\t&#123;<br>\t\t<span class=\"hljs-type\">const</span> vec3 p = ro+rd*t;<br>\t\t<span class=\"hljs-keyword\">if</span>(p.y &lt; f(p.x, p.z));<br>\t\t&#123;<br>\t\t\t<span class=\"hljs-comment\">// 取中间点减小误差</span><br>\t\t\t<span class=\"hljs-type\">hit_t</span> = t - <span class=\"hljs-number\">0.5f</span>*dt;<br>\t\t\t<span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">true</span>;<br>\t\t&#125;<br>\t&#125;<br>\t<span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">false</span>;<br>&#125;<br></code></pre></td></tr></table></figure>\n<p>这个方法简单易懂，但显而易见在性能上并不是最优的，尤其是涉及到范围很大的地形的时候，dt的值如果取得太小，那么渲染完成一个场景的时间将会非常的长，消耗巨大；而若是dt的值取得太大，则很有可能会出现取值错误的情况。</p>\n<p>当场景距离我们足够远的时候，由于透视的原因，近大远小，远处的场景精度对于观察者来说是越来越不重要了，因此dt的值可以随着光线步近而逐渐组建增大，动态变化。在合适的dt取值和变化曲线下，能够满足精度和性能的要求。Inigo给出的方法是类似这样的：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c\"><span class=\"hljs-comment\">//其他和上方代码一致</span><br><span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">float</span> t = <span class=\"hljs-type\">min_t</span>; t&lt;<span class=\"hljs-type\">max_t</span>; t+=dt)<br>&#123;<br>    <span class=\"hljs-type\">const</span> vec3 p = ro+rd*t;<br>    <span class=\"hljs-type\">const</span> <span class=\"hljs-type\">float</span> h = f(p.xz);<br>    <span class=\"hljs-keyword\">if</span>(p.y&lt;h)<br>    &#123;<br>        <span class=\"hljs-type\">hit_t</span> = t - <span class=\"hljs-number\">0.5f</span>*dt;<br>        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">true</span>;<br>    &#125;<br>    dt=<span class=\"hljs-number\">0.01f</span>*t;<br>&#125;<br><span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">false</span>;<br></code></pre></td></tr></table></figure>\n<p>t的起始值和dt的增长倍数可以自己尝试选择一个合适的值。</p>\n<p>另外，如果我们能对最终渲染的效果有所了解的话，可以通过过滤掉很多不需要做射线检测的情况来极大的提升性能。如果我们最终的效果是一个在空中的相机，天空和地面占据画面各一半的话，那么上半部分的画面（通过rd.y&gt;0判断）是可以完全跳过射线检测的。或者通过增加min_t的值来减少前期昂贵且不必要的性能消耗。</p>\n<p>在相交点的取值上，也可以进一步优化。原来仅仅是取两次光线步近的平均值，我们可以额外获取两次步近时位置的地形高度，用高度变化的连线和光线步近的线段做相交的判定取交点。这样得到的值将会更加精确。</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c\"><span class=\"hljs-comment\">//其他和上方代码一致</span><br><span class=\"hljs-type\">float</span> lh = <span class=\"hljs-number\">0.0f</span>;<br><span class=\"hljs-type\">float</span> ly = <span class=\"hljs-number\">0.0f</span>;<br><span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">float</span> t = <span class=\"hljs-type\">min_t</span>; t&lt;<span class=\"hljs-type\">max_t</span>; t+=dt)<br>&#123;<br>    <span class=\"hljs-type\">const</span> vec3 p = ro+rd*t;<br>    <span class=\"hljs-type\">const</span> <span class=\"hljs-type\">float</span> h = f(p.xz);<br>    <span class=\"hljs-keyword\">if</span>(p.y&lt;h)<br>    &#123;<br>        <span class=\"hljs-comment\">// 计算两个线段的相交点</span><br>        <span class=\"hljs-type\">hit_t</span> = t - dt + dt*(lh-ly)/(p.y-ly-h+lh);<br>        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">true</span>;<br>    &#125;<br>    dt=<span class=\"hljs-number\">0.01f</span>*t;<br>    lh = h;<br>    ly = p.y;<br>&#125;<br><span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">false</span>;<br></code></pre></td></tr></table></figure>\n<p>至此，我们就可以在ShaderToy渲染出地形了。</p>\n<h1 id=\"地形生成\"><a href=\"#地形生成\" class=\"headerlink\" title=\"地形生成\"></a>地形生成</h1><h2 id=\"生成的基础：噪音\"><a href=\"#生成的基础：噪音\" class=\"headerlink\" title=\"生成的基础：噪音\"></a>生成的基础：噪音</h2><p>当我们提到噪音，往往会很生活化的把噪音和声音连接起来，从声学的角度来说是正确的。噪音其实可以用来表示所有通过振幅（amplitude）和频率（frequency）描述的波动，它可以是声音，它可以是辐射，也可以是其他的任意一种波动。</p>\n<p>在数学课上，我们学过正弦、余弦等三角函数，sin和cos其实就是一种噪音的表现方式。</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c\"><span class=\"hljs-type\">float</span> amplitude = <span class=\"hljs-number\">1.0</span>;<br><span class=\"hljs-type\">float</span> frequencey = <span class=\"hljs-number\">1.0</span>;<br><span class=\"hljs-type\">float</span> y = amplitude * <span class=\"hljs-built_in\">sin</span>(frequency * x);<br></code></pre></td></tr></table></figure>\n<p>就像上面的代码所示，通过改变amplitude和frequency，我们可以改变sin波形的状态。</p>\n<p>噪音在很多程序化生成算法中都有着举足轻重的地位。</p>\n<h2 id=\"分形布朗运动\"><a href=\"#分形布朗运动\" class=\"headerlink\" title=\"分形布朗运动\"></a>分形布朗运动</h2><p>噪音是一种波，它是可以相互叠加的。两个相同的sin波形叠加会形成振幅更加强大的sin波形，而频率相差π&#x2F;2的两个sin波形叠加后会相互抵消。</p>\n<p>在地形随机生成中，为了最终的结果噪音有着更好的随机性和更好的细节，将会循环多次计算噪音，循环的次数为我们称之为octave。每次循环的同一个噪音以一定倍数（lacunarity）升高频率，同时以一定比例（gain）降低振幅，最终将每个噪音计算的结果叠加得到一个最终的噪音，这个噪音的生成技术叫做“分形布朗运动”（fractal brownian motion，fbm）。</p>\n<p>下面是分形布朗运动的一个简单的代码演示：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c\"><span class=\"hljs-type\">float</span> <span class=\"hljs-title function_\">fbm</span><span class=\"hljs-params\">(vec2 uv, <span class=\"hljs-type\">float</span> frequency, <span class=\"hljs-type\">float</span> amplitude, <span class=\"hljs-type\">int</span> octave)</span><br>&#123;<br>\t<span class=\"hljs-type\">float</span> lacunarity = <span class=\"hljs-number\">2.0</span>;<br>\t<span class=\"hljs-type\">float</span> gain = <span class=\"hljs-number\">0.5</span>;<br>\t<span class=\"hljs-type\">float</span> noise_val = <span class=\"hljs-number\">0.0</span>;<br>\t<span class=\"hljs-type\">float</span> amp = amplitude;<br>\t<span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">int</span> index = <span class=\"hljs-number\">0</span>; index &lt; octave; ++index)<br>\t&#123;<br>\t\tnose_val += noiseInterpolate(uv * frequency) * amp;<br>\t\tamp *= gain;<br>\t\tfrequency *= lacunarity;<br>\t&#125;<br>\t<br>\t<span class=\"hljs-keyword\">return</span> noise_val;<br>&#125;<br></code></pre></td></tr></table></figure>\n<p>其中noiseInterpolate可以是perlin noise或者是simplex noise等任意一种噪音算法。<br>demo中的地形生成和云层的生成，也使用了该技术。关于FBM除了上面简单的使用还有很多其他的变种，这里我们就不扩展了，后面有机会的话可以专门介绍一下。</p>\n<h2 id=\"地形的基础表现\"><a href=\"#地形的基础表现\" class=\"headerlink\" title=\"地形的基础表现\"></a>地形的基础表现</h2><p>这里我将地形部分拆解出来。demo的地形计算使用了perlin noise，octave数量达到了11。更多的octave数量会给地形带来更多的细节，但是一般来说后面的效果收益会越来越少。下方是octave数量分布为5和11下的地形的形状对比。<br><img src=\"/2024/10/11/ProceduralTerrainGeneration/shadertoy_oc5_noshadow.png\"><br><img src=\"/2024/10/11/ProceduralTerrainGeneration/shadertoy_oc11_noshadow.png\"></p>\n<p>除了每次叠加噪音会进行频率和振幅的变化，为了获得更好的随机性，以及进一步减少噪音可能出现的重复pattern，可以将噪音进行旋转（也就是将传入的uv或者是坐标乘以一个默认的旋转矩阵）后再叠加到原来的噪音上。</p>\n<p>我们也需要地形的法线来和光源结合，渲染出地形的明暗部分。获得法线的方法有很多种，可以采样当前计算的地形上点的x轴和z轴（这里假定y轴为up）方向不远的一两个点，和目标点相减得到切线和副切线方向，通过叉乘得到目标点的法线。亦或是采样其他点后通过中心差分法求得目标点的法线。</p>\n<h2 id=\"阴影\"><a href=\"#阴影\" class=\"headerlink\" title=\"阴影\"></a>阴影</h2><p>仅仅通过法线来渲染地形的明部和暗部是不够的，我们还需要计算地形投射在地表上的阴影。地形的阴影计算原理非常简单，就是将地形上渲染的目标点，沿着光源方向进行射线检测，如果和地形相交的话，那该点就是处于阴影之下。理想情况下，射线检测的距离当然是实际上光源和地形上的点的距离，但是往往由于性能的原因，我们需要缩短这个距离。<em>实际的检测距离可以结合当前点的高度以及地形可能的最高位置进行计算</em>。</p>\n<p>在判断当前点处于阴影的时候，计算最终颜色的时候需要再乘以一个阴影的系数。<br><img src=\"/2024/10/11/ProceduralTerrainGeneration/shadertoy_oc11_hardshadow.png\" alt=\"硬阴影\"></p>\n<p>为了提升效果，我们通常不希望阴影的边缘非常生硬，而是希望有一种柔软的过度，这种更加符合现实的表现。实现这种软阴影的方法可能有很多种，这里采用的是Inigo教程的一种方法。</p>\n<p>上面提到判定阴影是通过从地形上面的点向光源方向做射线检测得到的，如果和地形相交则该点处于阴影当中，若不相交，则需要再取一个值，这值是地形向着光源方向移动距离t长度的位置，它和地形高度的差值d和距离t的比值的最小值，乘以某个常数X（10~32等等，可以自己尝试合适的范围）后经过smoothstep限制在（0,1）范围内。这个值作为阴影系数放入光照计算后就可以得到不错的软阴影效果。<br><img src=\"/2024/10/11/ProceduralTerrainGeneration/calc_soft_shadow.png\" alt=\"软阴影\"></p>\n<p>通过下面的对比图我们可以看到，在加入了软阴影计算后，地形阴影的边缘有了一种较为平滑的过度，显得没那么生硬了。想要更改软阴影的表现的话可以通过修改常数X。<br><img src=\"/2024/10/11/ProceduralTerrainGeneration/shadertoy_terrain.png\"></p>\n<h1 id=\"结语\"><a href=\"#结语\" class=\"headerlink\" title=\"结语\"></a>结语</h1><p>好了，我们已经得到了一个基础的程序化生成地形的效果了，但是它看起来还是有些单调。地形的深度表现、天空、云彩等等应该如何表现呢？</p>\n<p>无需着急，我们将会在后面的文章中对它进行进一步的优化。</p>\n<h2 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h2><p><a href=\"https://thebookofshaders.com/13/?lan=ch\">https://thebookofshaders.com/13/?lan=ch</a><br><a href=\"https://iquilezles.org/articles/morenoise\">https://iquilezles.org/articles/morenoise</a><br><a href=\"https://youtu.be/BFld4EBO2RE?si=HWQMSNx5TBsOG_6g\">https://youtu.be/BFld4EBO2RE?si=HWQMSNx5TBsOG_6g</a></p>\n"},{"title":"个人博客启动","date":"2024-10-09T15:33:59.000Z","_content":"\n晚上好。\n\n还是打算在个人的github.io继续更新自己的技术博客了。[原网站](qrc-eye.com)本来是打算用于和朋友一起写点东西上去的，结果现在倒是变成了只有我自己的碎碎念，着实不太好。\n\n最近一段时间会尽快的将我的部分文章搬运过来，一些琐碎的文章暂时就不管了。\n\n","source":"_posts/StartMyBlog.md","raw":"---\ntitle: 个人博客启动\ndate: 2024-10-09 23:33:59\ncategory: 生活杂谈\ntags: 生活\n---\n\n晚上好。\n\n还是打算在个人的github.io继续更新自己的技术博客了。[原网站](qrc-eye.com)本来是打算用于和朋友一起写点东西上去的，结果现在倒是变成了只有我自己的碎碎念，着实不太好。\n\n最近一段时间会尽快的将我的部分文章搬运过来，一些琐碎的文章暂时就不管了。\n\n","slug":"StartMyBlog","published":1,"updated":"2024-10-10T14:58:34.819Z","comments":1,"layout":"post","photos":[],"_id":"cm5ht44vf00063457063u804r","content":"<p>晚上好。</p>\n<p>还是打算在个人的github.io继续更新自己的技术博客了。<a href=\"qrc-eye.com\">原网站</a>本来是打算用于和朋友一起写点东西上去的，结果现在倒是变成了只有我自己的碎碎念，着实不太好。</p>\n<p>最近一段时间会尽快的将我的部分文章搬运过来，一些琐碎的文章暂时就不管了。</p>\n","excerpt":"","more":"<p>晚上好。</p>\n<p>还是打算在个人的github.io继续更新自己的技术博客了。<a href=\"qrc-eye.com\">原网站</a>本来是打算用于和朋友一起写点东西上去的，结果现在倒是变成了只有我自己的碎碎念，着实不太好。</p>\n<p>最近一段时间会尽快的将我的部分文章搬运过来，一些琐碎的文章暂时就不管了。</p>\n"},{"title":"程序化地形生成-2","date":"2024-11-04T13:50:18.000Z","index_img":"/2024/11/04/ProceduralTerrainGeneration2/terrain_with_all_cloud.png","banner_img":"/2024/11/04/ProceduralTerrainGeneration2/terrain_with_all_cloud.png","_content":"\n距离上一篇将程序化地形生成的[教程](https://ruochenhua.github.io/2024/10/11/ProceduralTerrainGeneration/)也已经过去了一段时间了。前段时间一直有其他事情需要忙，现在终于有时间继续之前未完成的工作了。\n\n# 丰富地形\n上一篇教程我们已经创建出了绵延的山脉，如下图所示：\n![上次渲染的结果](terrain_no_grass.png)\n看起来似乎是有那么点意思了，但是这样的山脉效果还是太单调了。\n\n接下来我们就计划丰富一下这个场景。\n\n## 增加绿植\n我们场景中的山光秃秃的，很像是沙漠，也很像火星上的地貌。\n\n我想给场景增加一些层次，一些生机，因此我打算将山的一部分渲染为绿植部分。\n\n那么如何确定哪些部分是绿植，哪些部分是裸露的山体呢？根据地形表面的法线可以做一个简单的判定：法线的y分量越大，也就是面向上的分量越大，表明这个面更平，因此有绿植是更合适的；反正则表明这个面更加陡峭，更适合作为山体表现。\n\n```glsl\nvec3 dirt_color = vec3(0.8549, 0.5255, 0.3098);\nvec3 grass_color = vec3(0.3137, 0.5412, 0.0157);\n// ..\n\nif(rd.y < 0.05 && rayMarchingTerrain(ro, rd, maxt, res_t))\n{\n    vec3 height_pos = ro+res_t*rd;\n\n    // calculate normla\n    vec3 normal = getNormal(height_pos);\n    float grass_ratio = smoothstep(0.7, 0.98, normal.y);\n    vec3 ground_color = mix(dirt_color, grass_color, grass_ratio);\n}\n\n```\n\n上面是一段实例代码，我们预先设定山体和绿植的颜色，在光线步进山体的时候，获取normal，根据法线的y方向的大小（也可以用dot来判定）做smoothstep取一个合适的值作为当前平面草地的比例，最后混合山体和草地的比例。得到的结果如下：\n\n![增加绿植](terrain_with_grass.png)\n这样一来场景的丰富程度一下子就提升了。\n\n# 天空\n现在最终结果还是挺奇怪的，很大一部分的原因是场景的背景还是黑咕隆咚的。我们的大场景急需增加天空的部分。\n\n## 天空的颜色\n天空的颜色我之前有写过一个[教程](https://ruochenhua.github.io/2024/10/15/single-scatter-atmosphere/)，是利用单次散射的方法来计算天空大气的颜色。当然这个方法在这依然是可用的。\n\n不过如果你说，我不想用那么复杂的方法，只想快速出效果呢？那也是很简单，给地形光线步近没有结果的像素设定为蓝色就行了，哈哈。\n\n如果想再复杂一点，可以做一个小细节的优化。也就是在晴朗的情况下，一般靠近地平线的天的蓝色是更加浅的，所以在给天空像素上色的时候，可以根据当前像素的方向的y值作为判定，在深蓝色和浅蓝色做线性插值。\n\n```glsl\nvec3 low_sky_blue = vec3(0.5137, 0.7804, 0.9608);\nvec3 high_sky_blue = vec3(0.3216, 0.4706, 0.9725);\n\nvec3 sky_color = mix(low_sky_blue, high_sky_blue, clamp(rd.y, 0.0, 1.0));\n```\n\n![增加天空](terrain_with_sky.png)\n\n## 雾气增强层次感\n在增加了天空之后，我们的渲染结果有了很显著的提升，终于像室外的场景了。\n\n不过仔细看后，还是觉得怪怪的，远处的山和天空根本不在一个图层上；山的远近层次感也不足，远处的山太清晰了。\n\n这是因为缺少雾气的结果，雾气能够很好的增加大场景体量感，能够很好的处理场景和天空衔接的感觉。\n\n增加雾气也十分简单，仅需根据地形光线步近的结果的大小（也就是地形离相机的距离）来计算一个比例，根据这个比例来做场景颜色和雾气颜色的插值即可。\n\n```glsl\n// cal fog density, different between r,g,b so the fog has a blue hue\nvec3 calcFog(float dist)\n{    \n    return exp(-5e-3*dist*vec3(1,2,4));\n}\n\n// ...\nvec3 fog_amount = calcFog(res_t);\ncolor = mix(vec3(0.7),color, fog_amount);\n```\n上面这段便是雾气的计算过程。雾气的比例通过**exp(-5e-3*dist*vec3(1,2,4))**计算，得到的雾的比例用于最终颜色的线性插值。\n\n![增加雾气](terrain_with_sky_fog.png)\n有了雾气的加持，我们场景的大体量感就有了，层次感也明显了很多。\n关于雾气的更多内容，可以参考Inigo大神的[这篇文章](https://iquilezles.org/articles/fog/)。\n\n## 漫反射光照细节\n接下来我想优化一下场景整体的漫反射细节。\n\n当前的渲染结果其实是有加一个默认的环境光**vec3(0.1)**的，主要是之前的场景太黑了。这是个临时的处理，把这个环境光去掉。\n![去掉写死的环境光补偿](terrain_with_sky_fog_less_ambient.png)\n\n好了，我们再来看一下我们的场景。有没有感觉山体的阴影部分有点太黑了？是的，在现实世界中，处于阴影的场景，在大白天也会被场景的漫反射光给提亮的，这种一片漆黑的感觉是有些难看。所以接下来我们来增加一些场景中应该有的漫反射光。\n\n首先山体之间是会有漫反射光的影响的，也就是阴影部分的山体会被不在阴影的其他山的漫反射光照亮。当然这种影响我们是无法实时计算的，要计算的东西实在是太多了。但是可以通过一个很简单的方式来提单：我们计算当前点的发现和指向光源方向的点乘，当这个结果大于0的时候，用这个乘积乘以一个小的光照并加到原本的颜色上。\n\n```glsl\ncolor += (max(0.0, dot(-light_dir, normal))*ground_color/10.0);\n```\n\n![增加地形的漫反射光](terrain_with_sky_fog_diffuse_from_mountain.png)\n\n加完地形的漫反射光后，天空其实也有各个方向的漫反射的光会影响地形的亮度。天空的漫反射光可以通过地形法线的y分量和天空颜色的一部分来计算，示例代码如下：\n```glsl\ncolor += (normal.y + 1.0)/2.0*low_sky_blue/10.0;\n```\n\n![增加天空的漫反射光](terrain_with_sky_fog_diffuse_from_sky.png)\n\n增加了这些漫反射细节，场景的真实度进一步得到了提升。\n\n# 云朵\n有了天空，再给天空上增加云的话会进一步增加场景的丰富度，下面就来介绍我给这个场景添加的两部分云的计算。\n\n## 高层云\n我给这个场景添加了两种云，高层云是我想模拟在高度很高的地方，云层看起来不厚并且相对静态的感觉。\n\n高层云的纹理也是利用perlin noise来计算，由于高层的云没有厚实感其实用2D的perlin noise也足够了。\n\n另外需要注意的是，云的纹理需要根据远近来做处理。远处的云的纹理如果太清晰的话在最终效果上会显得太过于密集。下面是高层云的示例代码：\n```glsl\n// 高层云的高度\nfloat top_sky_plane = 3000.;\n\nvec3 getSkyColor(vec3 ro, vec3 rd)\n{    \n    vec3 hit_sky;\n    hit_sky.y = top_sky_plane;\n        \n    hit_sky.xz = ro.xz + rd.xz * (top_sky_plane - ro.y)/rd.y;    \n    \n    //降低远处云的密度，看起来效果更好\n    float hit_dist = distance(hit_sky, ro);\n    float cloud_density_percentage = 1.0;\n    if(hit_dist > cloud_view_distance)\n    {\n        cloud_density_percentage *= exp(-(hit_dist - cloud_view_distance)/ cloud_view_distance);\n    }    \n\n    // 根据云层采样点的远近处理云层的密度\n    float cloud_density = smoothstep(getCloudDensity(hit_sky.xz/150.0, 3), -0.99, 1.9)*cloud_density_percentage * 0.5;\n    float res_t;\n\n    // sky color    \n    vec3 sky_color = mix(low_sky_blue, high_sky_blue, clamp(rd.y, 0.0, 1.0));\n    vec3 cloud_color = vec3(1.);\n    // 根据云层密度得到最终的高层云和天空颜色\n    return mix(sky_color, cloud_color, cloud_density);\n}\n```\n\n![增加天空高层云](terrain_with_high_cloud.png)\n\n增加了高层云之后，天空就显得不那么单调了。\n\n## 体积云\n最后就是体积云了，体积云的部分我不会在这里详细讨论，这是一个可以讨论的比较深的话题。我这个场景的体积云效果也并不是很理想，这里仅作为一个最后的点缀，在山峰上增加一点点动态效果。\n\n体积云的效果其实也是在一定区域内进行多个点的采样而来，这个区域内利用FBM来实现云的动态的随机效果。在这个场景内我使用了SDF来圈定体积云的范围，并且给SDF增加了随机现状来获取更好的随机效果。\n\n```glsl\nfloat scene(in vec3 pos)\n{    \n    vec3 cloud_pos = vec3(0.0, 5.0, 15.0);\n    \n    vec3 filter_pos = vec3(pos.x, pos.y+iTime, pos.z+iTime);\n    pos -= cloud_pos;\n    float rst = -(rm_box(pos)) + fbm_cloud(pos * 0.1+iTime*0.3, 5);\n    rst = rst / 25.0 * max(fbm_cloud(filter_pos*0.1, 1) - 1.2, 0.0); \n    return rst;\n}\n\nfloat max_cloud_dist = 80.;\nvec4 renderMidClouds(vec3 ro, vec3 rd)\n{    \n    vec4 res = vec4(0.0);\n    float depth = 0.0;    \n    \n    int sample_count = 64;\n    float dt = max_cloud_dist / float(sample_count);\n    \n    for(int i = 0; i < sample_count; ++i)\n    {\n        vec3 p = ro + depth*rd;\n        float density = scene(p);\n        if(density > 0.0)\n        {\n            float diffuse = clamp((scene(p) - scene(p + 0.3*light_dir))/0.3, 0.0, 1.0);\n            vec3 lin = vec3(0.8, 0.8, 0.8) * 1.1 + 0.8 * vec3(0.9333, 0.702, 0.5255)*diffuse;\n            vec4 color = vec4(mix(vec3(1.0), vec3(0.0), density), density);\n            color.rgb *= color.a;\n\n            res += color * (1.0 - res.a);\n        }\n\n        depth+=dt;\n    }\n\n    return res;\n}\n```\n\n![增加体积云](terrain_with_all_cloud.png)\n\n\n# 最终效果\n\n<iframe width=\"640\" height=\"360\" frameborder=\"0\" src=\"https://www.shadertoy.com/embed/4XByRV?gui=true&t=10&paused=true&muted=false\" allowfullscreen></iframe>\n\n以上便是我如何实现这个场景的简单介绍了。这是我初次尝试在ShaderToy上渲染大场景，也是第一次将整个步骤拆解讲解，可能还是会有不少地方讲的不过仔细和通透，也有些遗漏。\n\n后面我还是会继续尝试创作和讲解，希望能给人带来帮助，也有助于巩固我所学的知识。","source":"_posts/ProceduralTerrainGeneration2.md","raw":"---\ntitle: 程序化地形生成-2\ndate: 2024-11-04 21:50:18\ncategories: \n\t- 技术漫谈\ntags: [3D, render, 渲染, 编程, 程序化生成]\n\t\nindex_img: /2024/11/04/ProceduralTerrainGeneration2/terrain_with_all_cloud.png\nbanner_img: /2024/11/04/ProceduralTerrainGeneration2/terrain_with_all_cloud.png\n---\n\n距离上一篇将程序化地形生成的[教程](https://ruochenhua.github.io/2024/10/11/ProceduralTerrainGeneration/)也已经过去了一段时间了。前段时间一直有其他事情需要忙，现在终于有时间继续之前未完成的工作了。\n\n# 丰富地形\n上一篇教程我们已经创建出了绵延的山脉，如下图所示：\n![上次渲染的结果](terrain_no_grass.png)\n看起来似乎是有那么点意思了，但是这样的山脉效果还是太单调了。\n\n接下来我们就计划丰富一下这个场景。\n\n## 增加绿植\n我们场景中的山光秃秃的，很像是沙漠，也很像火星上的地貌。\n\n我想给场景增加一些层次，一些生机，因此我打算将山的一部分渲染为绿植部分。\n\n那么如何确定哪些部分是绿植，哪些部分是裸露的山体呢？根据地形表面的法线可以做一个简单的判定：法线的y分量越大，也就是面向上的分量越大，表明这个面更平，因此有绿植是更合适的；反正则表明这个面更加陡峭，更适合作为山体表现。\n\n```glsl\nvec3 dirt_color = vec3(0.8549, 0.5255, 0.3098);\nvec3 grass_color = vec3(0.3137, 0.5412, 0.0157);\n// ..\n\nif(rd.y < 0.05 && rayMarchingTerrain(ro, rd, maxt, res_t))\n{\n    vec3 height_pos = ro+res_t*rd;\n\n    // calculate normla\n    vec3 normal = getNormal(height_pos);\n    float grass_ratio = smoothstep(0.7, 0.98, normal.y);\n    vec3 ground_color = mix(dirt_color, grass_color, grass_ratio);\n}\n\n```\n\n上面是一段实例代码，我们预先设定山体和绿植的颜色，在光线步进山体的时候，获取normal，根据法线的y方向的大小（也可以用dot来判定）做smoothstep取一个合适的值作为当前平面草地的比例，最后混合山体和草地的比例。得到的结果如下：\n\n![增加绿植](terrain_with_grass.png)\n这样一来场景的丰富程度一下子就提升了。\n\n# 天空\n现在最终结果还是挺奇怪的，很大一部分的原因是场景的背景还是黑咕隆咚的。我们的大场景急需增加天空的部分。\n\n## 天空的颜色\n天空的颜色我之前有写过一个[教程](https://ruochenhua.github.io/2024/10/15/single-scatter-atmosphere/)，是利用单次散射的方法来计算天空大气的颜色。当然这个方法在这依然是可用的。\n\n不过如果你说，我不想用那么复杂的方法，只想快速出效果呢？那也是很简单，给地形光线步近没有结果的像素设定为蓝色就行了，哈哈。\n\n如果想再复杂一点，可以做一个小细节的优化。也就是在晴朗的情况下，一般靠近地平线的天的蓝色是更加浅的，所以在给天空像素上色的时候，可以根据当前像素的方向的y值作为判定，在深蓝色和浅蓝色做线性插值。\n\n```glsl\nvec3 low_sky_blue = vec3(0.5137, 0.7804, 0.9608);\nvec3 high_sky_blue = vec3(0.3216, 0.4706, 0.9725);\n\nvec3 sky_color = mix(low_sky_blue, high_sky_blue, clamp(rd.y, 0.0, 1.0));\n```\n\n![增加天空](terrain_with_sky.png)\n\n## 雾气增强层次感\n在增加了天空之后，我们的渲染结果有了很显著的提升，终于像室外的场景了。\n\n不过仔细看后，还是觉得怪怪的，远处的山和天空根本不在一个图层上；山的远近层次感也不足，远处的山太清晰了。\n\n这是因为缺少雾气的结果，雾气能够很好的增加大场景体量感，能够很好的处理场景和天空衔接的感觉。\n\n增加雾气也十分简单，仅需根据地形光线步近的结果的大小（也就是地形离相机的距离）来计算一个比例，根据这个比例来做场景颜色和雾气颜色的插值即可。\n\n```glsl\n// cal fog density, different between r,g,b so the fog has a blue hue\nvec3 calcFog(float dist)\n{    \n    return exp(-5e-3*dist*vec3(1,2,4));\n}\n\n// ...\nvec3 fog_amount = calcFog(res_t);\ncolor = mix(vec3(0.7),color, fog_amount);\n```\n上面这段便是雾气的计算过程。雾气的比例通过**exp(-5e-3*dist*vec3(1,2,4))**计算，得到的雾的比例用于最终颜色的线性插值。\n\n![增加雾气](terrain_with_sky_fog.png)\n有了雾气的加持，我们场景的大体量感就有了，层次感也明显了很多。\n关于雾气的更多内容，可以参考Inigo大神的[这篇文章](https://iquilezles.org/articles/fog/)。\n\n## 漫反射光照细节\n接下来我想优化一下场景整体的漫反射细节。\n\n当前的渲染结果其实是有加一个默认的环境光**vec3(0.1)**的，主要是之前的场景太黑了。这是个临时的处理，把这个环境光去掉。\n![去掉写死的环境光补偿](terrain_with_sky_fog_less_ambient.png)\n\n好了，我们再来看一下我们的场景。有没有感觉山体的阴影部分有点太黑了？是的，在现实世界中，处于阴影的场景，在大白天也会被场景的漫反射光给提亮的，这种一片漆黑的感觉是有些难看。所以接下来我们来增加一些场景中应该有的漫反射光。\n\n首先山体之间是会有漫反射光的影响的，也就是阴影部分的山体会被不在阴影的其他山的漫反射光照亮。当然这种影响我们是无法实时计算的，要计算的东西实在是太多了。但是可以通过一个很简单的方式来提单：我们计算当前点的发现和指向光源方向的点乘，当这个结果大于0的时候，用这个乘积乘以一个小的光照并加到原本的颜色上。\n\n```glsl\ncolor += (max(0.0, dot(-light_dir, normal))*ground_color/10.0);\n```\n\n![增加地形的漫反射光](terrain_with_sky_fog_diffuse_from_mountain.png)\n\n加完地形的漫反射光后，天空其实也有各个方向的漫反射的光会影响地形的亮度。天空的漫反射光可以通过地形法线的y分量和天空颜色的一部分来计算，示例代码如下：\n```glsl\ncolor += (normal.y + 1.0)/2.0*low_sky_blue/10.0;\n```\n\n![增加天空的漫反射光](terrain_with_sky_fog_diffuse_from_sky.png)\n\n增加了这些漫反射细节，场景的真实度进一步得到了提升。\n\n# 云朵\n有了天空，再给天空上增加云的话会进一步增加场景的丰富度，下面就来介绍我给这个场景添加的两部分云的计算。\n\n## 高层云\n我给这个场景添加了两种云，高层云是我想模拟在高度很高的地方，云层看起来不厚并且相对静态的感觉。\n\n高层云的纹理也是利用perlin noise来计算，由于高层的云没有厚实感其实用2D的perlin noise也足够了。\n\n另外需要注意的是，云的纹理需要根据远近来做处理。远处的云的纹理如果太清晰的话在最终效果上会显得太过于密集。下面是高层云的示例代码：\n```glsl\n// 高层云的高度\nfloat top_sky_plane = 3000.;\n\nvec3 getSkyColor(vec3 ro, vec3 rd)\n{    \n    vec3 hit_sky;\n    hit_sky.y = top_sky_plane;\n        \n    hit_sky.xz = ro.xz + rd.xz * (top_sky_plane - ro.y)/rd.y;    \n    \n    //降低远处云的密度，看起来效果更好\n    float hit_dist = distance(hit_sky, ro);\n    float cloud_density_percentage = 1.0;\n    if(hit_dist > cloud_view_distance)\n    {\n        cloud_density_percentage *= exp(-(hit_dist - cloud_view_distance)/ cloud_view_distance);\n    }    \n\n    // 根据云层采样点的远近处理云层的密度\n    float cloud_density = smoothstep(getCloudDensity(hit_sky.xz/150.0, 3), -0.99, 1.9)*cloud_density_percentage * 0.5;\n    float res_t;\n\n    // sky color    \n    vec3 sky_color = mix(low_sky_blue, high_sky_blue, clamp(rd.y, 0.0, 1.0));\n    vec3 cloud_color = vec3(1.);\n    // 根据云层密度得到最终的高层云和天空颜色\n    return mix(sky_color, cloud_color, cloud_density);\n}\n```\n\n![增加天空高层云](terrain_with_high_cloud.png)\n\n增加了高层云之后，天空就显得不那么单调了。\n\n## 体积云\n最后就是体积云了，体积云的部分我不会在这里详细讨论，这是一个可以讨论的比较深的话题。我这个场景的体积云效果也并不是很理想，这里仅作为一个最后的点缀，在山峰上增加一点点动态效果。\n\n体积云的效果其实也是在一定区域内进行多个点的采样而来，这个区域内利用FBM来实现云的动态的随机效果。在这个场景内我使用了SDF来圈定体积云的范围，并且给SDF增加了随机现状来获取更好的随机效果。\n\n```glsl\nfloat scene(in vec3 pos)\n{    \n    vec3 cloud_pos = vec3(0.0, 5.0, 15.0);\n    \n    vec3 filter_pos = vec3(pos.x, pos.y+iTime, pos.z+iTime);\n    pos -= cloud_pos;\n    float rst = -(rm_box(pos)) + fbm_cloud(pos * 0.1+iTime*0.3, 5);\n    rst = rst / 25.0 * max(fbm_cloud(filter_pos*0.1, 1) - 1.2, 0.0); \n    return rst;\n}\n\nfloat max_cloud_dist = 80.;\nvec4 renderMidClouds(vec3 ro, vec3 rd)\n{    \n    vec4 res = vec4(0.0);\n    float depth = 0.0;    \n    \n    int sample_count = 64;\n    float dt = max_cloud_dist / float(sample_count);\n    \n    for(int i = 0; i < sample_count; ++i)\n    {\n        vec3 p = ro + depth*rd;\n        float density = scene(p);\n        if(density > 0.0)\n        {\n            float diffuse = clamp((scene(p) - scene(p + 0.3*light_dir))/0.3, 0.0, 1.0);\n            vec3 lin = vec3(0.8, 0.8, 0.8) * 1.1 + 0.8 * vec3(0.9333, 0.702, 0.5255)*diffuse;\n            vec4 color = vec4(mix(vec3(1.0), vec3(0.0), density), density);\n            color.rgb *= color.a;\n\n            res += color * (1.0 - res.a);\n        }\n\n        depth+=dt;\n    }\n\n    return res;\n}\n```\n\n![增加体积云](terrain_with_all_cloud.png)\n\n\n# 最终效果\n\n<iframe width=\"640\" height=\"360\" frameborder=\"0\" src=\"https://www.shadertoy.com/embed/4XByRV?gui=true&t=10&paused=true&muted=false\" allowfullscreen></iframe>\n\n以上便是我如何实现这个场景的简单介绍了。这是我初次尝试在ShaderToy上渲染大场景，也是第一次将整个步骤拆解讲解，可能还是会有不少地方讲的不过仔细和通透，也有些遗漏。\n\n后面我还是会继续尝试创作和讲解，希望能给人带来帮助，也有助于巩固我所学的知识。","slug":"ProceduralTerrainGeneration2","published":1,"updated":"2024-11-05T15:29:34.319Z","comments":1,"layout":"post","photos":[],"_id":"cm5ht44vf000734572fxi9aa2","content":"<p>距离上一篇将程序化地形生成的<a href=\"https://ruochenhua.github.io/2024/10/11/ProceduralTerrainGeneration/\">教程</a>也已经过去了一段时间了。前段时间一直有其他事情需要忙，现在终于有时间继续之前未完成的工作了。</p>\n<h1 id=\"丰富地形\"><a href=\"#丰富地形\" class=\"headerlink\" title=\"丰富地形\"></a>丰富地形</h1><p>上一篇教程我们已经创建出了绵延的山脉，如下图所示：<br><img src=\"/2024/11/04/ProceduralTerrainGeneration2/terrain_no_grass.png\" alt=\"上次渲染的结果\"><br>看起来似乎是有那么点意思了，但是这样的山脉效果还是太单调了。</p>\n<p>接下来我们就计划丰富一下这个场景。</p>\n<h2 id=\"增加绿植\"><a href=\"#增加绿植\" class=\"headerlink\" title=\"增加绿植\"></a>增加绿植</h2><p>我们场景中的山光秃秃的，很像是沙漠，也很像火星上的地貌。</p>\n<p>我想给场景增加一些层次，一些生机，因此我打算将山的一部分渲染为绿植部分。</p>\n<p>那么如何确定哪些部分是绿植，哪些部分是裸露的山体呢？根据地形表面的法线可以做一个简单的判定：法线的y分量越大，也就是面向上的分量越大，表明这个面更平，因此有绿植是更合适的；反正则表明这个面更加陡峭，更适合作为山体表现。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-type\">vec3</span> dirt_color = <span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">0.8549</span>, <span class=\"hljs-number\">0.5255</span>, <span class=\"hljs-number\">0.3098</span>);<br><span class=\"hljs-type\">vec3</span> grass_color = <span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">0.3137</span>, <span class=\"hljs-number\">0.5412</span>, <span class=\"hljs-number\">0.0157</span>);<br><span class=\"hljs-comment\">// ..</span><br><br><span class=\"hljs-keyword\">if</span>(rd.y &lt; <span class=\"hljs-number\">0.05</span> &amp;&amp; rayMarchingTerrain(ro, rd, maxt, res_t))<br>&#123;<br>    <span class=\"hljs-type\">vec3</span> height_pos = ro+res_t*rd;<br><br>    <span class=\"hljs-comment\">// calculate normla</span><br>    <span class=\"hljs-type\">vec3</span> normal = getNormal(height_pos);<br>    <span class=\"hljs-type\">float</span> grass_ratio = <span class=\"hljs-built_in\">smoothstep</span>(<span class=\"hljs-number\">0.7</span>, <span class=\"hljs-number\">0.98</span>, normal.y);<br>    <span class=\"hljs-type\">vec3</span> ground_color = <span class=\"hljs-built_in\">mix</span>(dirt_color, grass_color, grass_ratio);<br>&#125;<br><br></code></pre></td></tr></table></figure>\n\n<p>上面是一段实例代码，我们预先设定山体和绿植的颜色，在光线步进山体的时候，获取normal，根据法线的y方向的大小（也可以用dot来判定）做smoothstep取一个合适的值作为当前平面草地的比例，最后混合山体和草地的比例。得到的结果如下：</p>\n<p><img src=\"/2024/11/04/ProceduralTerrainGeneration2/terrain_with_grass.png\" alt=\"增加绿植\"><br>这样一来场景的丰富程度一下子就提升了。</p>\n<h1 id=\"天空\"><a href=\"#天空\" class=\"headerlink\" title=\"天空\"></a>天空</h1><p>现在最终结果还是挺奇怪的，很大一部分的原因是场景的背景还是黑咕隆咚的。我们的大场景急需增加天空的部分。</p>\n<h2 id=\"天空的颜色\"><a href=\"#天空的颜色\" class=\"headerlink\" title=\"天空的颜色\"></a>天空的颜色</h2><p>天空的颜色我之前有写过一个<a href=\"https://ruochenhua.github.io/2024/10/15/single-scatter-atmosphere/\">教程</a>，是利用单次散射的方法来计算天空大气的颜色。当然这个方法在这依然是可用的。</p>\n<p>不过如果你说，我不想用那么复杂的方法，只想快速出效果呢？那也是很简单，给地形光线步近没有结果的像素设定为蓝色就行了，哈哈。</p>\n<p>如果想再复杂一点，可以做一个小细节的优化。也就是在晴朗的情况下，一般靠近地平线的天的蓝色是更加浅的，所以在给天空像素上色的时候，可以根据当前像素的方向的y值作为判定，在深蓝色和浅蓝色做线性插值。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-type\">vec3</span> low_sky_blue = <span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">0.5137</span>, <span class=\"hljs-number\">0.7804</span>, <span class=\"hljs-number\">0.9608</span>);<br><span class=\"hljs-type\">vec3</span> high_sky_blue = <span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">0.3216</span>, <span class=\"hljs-number\">0.4706</span>, <span class=\"hljs-number\">0.9725</span>);<br><br><span class=\"hljs-type\">vec3</span> sky_color = <span class=\"hljs-built_in\">mix</span>(low_sky_blue, high_sky_blue, <span class=\"hljs-built_in\">clamp</span>(rd.y, <span class=\"hljs-number\">0.0</span>, <span class=\"hljs-number\">1.0</span>));<br></code></pre></td></tr></table></figure>\n\n<p><img src=\"/2024/11/04/ProceduralTerrainGeneration2/terrain_with_sky.png\" alt=\"增加天空\"></p>\n<h2 id=\"雾气增强层次感\"><a href=\"#雾气增强层次感\" class=\"headerlink\" title=\"雾气增强层次感\"></a>雾气增强层次感</h2><p>在增加了天空之后，我们的渲染结果有了很显著的提升，终于像室外的场景了。</p>\n<p>不过仔细看后，还是觉得怪怪的，远处的山和天空根本不在一个图层上；山的远近层次感也不足，远处的山太清晰了。</p>\n<p>这是因为缺少雾气的结果，雾气能够很好的增加大场景体量感，能够很好的处理场景和天空衔接的感觉。</p>\n<p>增加雾气也十分简单，仅需根据地形光线步近的结果的大小（也就是地形离相机的距离）来计算一个比例，根据这个比例来做场景颜色和雾气颜色的插值即可。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-comment\">// cal fog density, different between r,g,b so the fog has a blue hue</span><br><span class=\"hljs-type\">vec3</span> calcFog(<span class=\"hljs-type\">float</span> dist)<br>&#123;    <br>    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-built_in\">exp</span>(<span class=\"hljs-number\">-5e-3</span>*dist*<span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">4</span>));<br>&#125;<br><br><span class=\"hljs-comment\">// ...</span><br><span class=\"hljs-type\">vec3</span> fog_amount = calcFog(res_t);<br>color = <span class=\"hljs-built_in\">mix</span>(<span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">0.7</span>),color, fog_amount);<br></code></pre></td></tr></table></figure>\n<p>上面这段便是雾气的计算过程。雾气的比例通过*<em>exp(-5e-3</em>dist*vec3(1,2,4))**计算，得到的雾的比例用于最终颜色的线性插值。</p>\n<p><img src=\"/2024/11/04/ProceduralTerrainGeneration2/terrain_with_sky_fog.png\" alt=\"增加雾气\"><br>有了雾气的加持，我们场景的大体量感就有了，层次感也明显了很多。<br>关于雾气的更多内容，可以参考Inigo大神的<a href=\"https://iquilezles.org/articles/fog/\">这篇文章</a>。</p>\n<h2 id=\"漫反射光照细节\"><a href=\"#漫反射光照细节\" class=\"headerlink\" title=\"漫反射光照细节\"></a>漫反射光照细节</h2><p>接下来我想优化一下场景整体的漫反射细节。</p>\n<p>当前的渲染结果其实是有加一个默认的环境光**vec3(0.1)**的，主要是之前的场景太黑了。这是个临时的处理，把这个环境光去掉。<br><img src=\"/2024/11/04/ProceduralTerrainGeneration2/terrain_with_sky_fog_less_ambient.png\" alt=\"去掉写死的环境光补偿\"></p>\n<p>好了，我们再来看一下我们的场景。有没有感觉山体的阴影部分有点太黑了？是的，在现实世界中，处于阴影的场景，在大白天也会被场景的漫反射光给提亮的，这种一片漆黑的感觉是有些难看。所以接下来我们来增加一些场景中应该有的漫反射光。</p>\n<p>首先山体之间是会有漫反射光的影响的，也就是阴影部分的山体会被不在阴影的其他山的漫反射光照亮。当然这种影响我们是无法实时计算的，要计算的东西实在是太多了。但是可以通过一个很简单的方式来提单：我们计算当前点的发现和指向光源方向的点乘，当这个结果大于0的时候，用这个乘积乘以一个小的光照并加到原本的颜色上。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\">color += (<span class=\"hljs-built_in\">max</span>(<span class=\"hljs-number\">0.0</span>, <span class=\"hljs-built_in\">dot</span>(-light_dir, normal))*ground_color/<span class=\"hljs-number\">10.0</span>);<br></code></pre></td></tr></table></figure>\n\n<p><img src=\"/2024/11/04/ProceduralTerrainGeneration2/terrain_with_sky_fog_diffuse_from_mountain.png\" alt=\"增加地形的漫反射光\"></p>\n<p>加完地形的漫反射光后，天空其实也有各个方向的漫反射的光会影响地形的亮度。天空的漫反射光可以通过地形法线的y分量和天空颜色的一部分来计算，示例代码如下：</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\">color += (normal.y + <span class=\"hljs-number\">1.0</span>)/<span class=\"hljs-number\">2.0</span>*low_sky_blue/<span class=\"hljs-number\">10.0</span>;<br></code></pre></td></tr></table></figure>\n\n<p><img src=\"/2024/11/04/ProceduralTerrainGeneration2/terrain_with_sky_fog_diffuse_from_sky.png\" alt=\"增加天空的漫反射光\"></p>\n<p>增加了这些漫反射细节，场景的真实度进一步得到了提升。</p>\n<h1 id=\"云朵\"><a href=\"#云朵\" class=\"headerlink\" title=\"云朵\"></a>云朵</h1><p>有了天空，再给天空上增加云的话会进一步增加场景的丰富度，下面就来介绍我给这个场景添加的两部分云的计算。</p>\n<h2 id=\"高层云\"><a href=\"#高层云\" class=\"headerlink\" title=\"高层云\"></a>高层云</h2><p>我给这个场景添加了两种云，高层云是我想模拟在高度很高的地方，云层看起来不厚并且相对静态的感觉。</p>\n<p>高层云的纹理也是利用perlin noise来计算，由于高层的云没有厚实感其实用2D的perlin noise也足够了。</p>\n<p>另外需要注意的是，云的纹理需要根据远近来做处理。远处的云的纹理如果太清晰的话在最终效果上会显得太过于密集。下面是高层云的示例代码：</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-comment\">// 高层云的高度</span><br><span class=\"hljs-type\">float</span> top_sky_plane = <span class=\"hljs-number\">3000.</span>;<br><br><span class=\"hljs-type\">vec3</span> getSkyColor(<span class=\"hljs-type\">vec3</span> ro, <span class=\"hljs-type\">vec3</span> rd)<br>&#123;    <br>    <span class=\"hljs-type\">vec3</span> hit_sky;<br>    hit_sky.y = top_sky_plane;<br>        <br>    hit_sky.xz = ro.xz + rd.xz * (top_sky_plane - ro.y)/rd.y;    <br>    <br>    <span class=\"hljs-comment\">//降低远处云的密度，看起来效果更好</span><br>    <span class=\"hljs-type\">float</span> hit_dist = <span class=\"hljs-built_in\">distance</span>(hit_sky, ro);<br>    <span class=\"hljs-type\">float</span> cloud_density_percentage = <span class=\"hljs-number\">1.0</span>;<br>    <span class=\"hljs-keyword\">if</span>(hit_dist &gt; cloud_view_distance)<br>    &#123;<br>        cloud_density_percentage *= <span class=\"hljs-built_in\">exp</span>(-(hit_dist - cloud_view_distance)/ cloud_view_distance);<br>    &#125;    <br><br>    <span class=\"hljs-comment\">// 根据云层采样点的远近处理云层的密度</span><br>    <span class=\"hljs-type\">float</span> cloud_density = <span class=\"hljs-built_in\">smoothstep</span>(getCloudDensity(hit_sky.xz/<span class=\"hljs-number\">150.0</span>, <span class=\"hljs-number\">3</span>), <span class=\"hljs-number\">-0.99</span>, <span class=\"hljs-number\">1.9</span>)*cloud_density_percentage * <span class=\"hljs-number\">0.5</span>;<br>    <span class=\"hljs-type\">float</span> res_t;<br><br>    <span class=\"hljs-comment\">// sky color    </span><br>    <span class=\"hljs-type\">vec3</span> sky_color = <span class=\"hljs-built_in\">mix</span>(low_sky_blue, high_sky_blue, <span class=\"hljs-built_in\">clamp</span>(rd.y, <span class=\"hljs-number\">0.0</span>, <span class=\"hljs-number\">1.0</span>));<br>    <span class=\"hljs-type\">vec3</span> cloud_color = <span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">1.</span>);<br>    <span class=\"hljs-comment\">// 根据云层密度得到最终的高层云和天空颜色</span><br>    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-built_in\">mix</span>(sky_color, cloud_color, cloud_density);<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<p><img src=\"/2024/11/04/ProceduralTerrainGeneration2/terrain_with_high_cloud.png\" alt=\"增加天空高层云\"></p>\n<p>增加了高层云之后，天空就显得不那么单调了。</p>\n<h2 id=\"体积云\"><a href=\"#体积云\" class=\"headerlink\" title=\"体积云\"></a>体积云</h2><p>最后就是体积云了，体积云的部分我不会在这里详细讨论，这是一个可以讨论的比较深的话题。我这个场景的体积云效果也并不是很理想，这里仅作为一个最后的点缀，在山峰上增加一点点动态效果。</p>\n<p>体积云的效果其实也是在一定区域内进行多个点的采样而来，这个区域内利用FBM来实现云的动态的随机效果。在这个场景内我使用了SDF来圈定体积云的范围，并且给SDF增加了随机现状来获取更好的随机效果。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-type\">float</span> scene(<span class=\"hljs-keyword\">in</span> <span class=\"hljs-type\">vec3</span> pos)<br>&#123;    <br>    <span class=\"hljs-type\">vec3</span> cloud_pos = <span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">0.0</span>, <span class=\"hljs-number\">5.0</span>, <span class=\"hljs-number\">15.0</span>);<br>    <br>    <span class=\"hljs-type\">vec3</span> filter_pos = <span class=\"hljs-type\">vec3</span>(pos.x, pos.y+iTime, pos.z+iTime);<br>    pos -= cloud_pos;<br>    <span class=\"hljs-type\">float</span> rst = -(rm_box(pos)) + fbm_cloud(pos * <span class=\"hljs-number\">0.1</span>+iTime*<span class=\"hljs-number\">0.3</span>, <span class=\"hljs-number\">5</span>);<br>    rst = rst / <span class=\"hljs-number\">25.0</span> * <span class=\"hljs-built_in\">max</span>(fbm_cloud(filter_pos*<span class=\"hljs-number\">0.1</span>, <span class=\"hljs-number\">1</span>) - <span class=\"hljs-number\">1.2</span>, <span class=\"hljs-number\">0.0</span>); <br>    <span class=\"hljs-keyword\">return</span> rst;<br>&#125;<br><br><span class=\"hljs-type\">float</span> max_cloud_dist = <span class=\"hljs-number\">80.</span>;<br><span class=\"hljs-type\">vec4</span> renderMidClouds(<span class=\"hljs-type\">vec3</span> ro, <span class=\"hljs-type\">vec3</span> rd)<br>&#123;    <br>    <span class=\"hljs-type\">vec4</span> res = <span class=\"hljs-type\">vec4</span>(<span class=\"hljs-number\">0.0</span>);<br>    <span class=\"hljs-type\">float</span> depth = <span class=\"hljs-number\">0.0</span>;    <br>    <br>    <span class=\"hljs-type\">int</span> sample_count = <span class=\"hljs-number\">64</span>;<br>    <span class=\"hljs-type\">float</span> dt = max_cloud_dist / <span class=\"hljs-type\">float</span>(sample_count);<br>    <br>    <span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">int</span> i = <span class=\"hljs-number\">0</span>; i &lt; sample_count; ++i)<br>    &#123;<br>        <span class=\"hljs-type\">vec3</span> p = ro + depth*rd;<br>        <span class=\"hljs-type\">float</span> density = scene(p);<br>        <span class=\"hljs-keyword\">if</span>(density &gt; <span class=\"hljs-number\">0.0</span>)<br>        &#123;<br>            <span class=\"hljs-type\">float</span> diffuse = <span class=\"hljs-built_in\">clamp</span>((scene(p) - scene(p + <span class=\"hljs-number\">0.3</span>*light_dir))/<span class=\"hljs-number\">0.3</span>, <span class=\"hljs-number\">0.0</span>, <span class=\"hljs-number\">1.0</span>);<br>            <span class=\"hljs-type\">vec3</span> lin = <span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">0.8</span>, <span class=\"hljs-number\">0.8</span>, <span class=\"hljs-number\">0.8</span>) * <span class=\"hljs-number\">1.1</span> + <span class=\"hljs-number\">0.8</span> * <span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">0.9333</span>, <span class=\"hljs-number\">0.702</span>, <span class=\"hljs-number\">0.5255</span>)*diffuse;<br>            <span class=\"hljs-type\">vec4</span> color = <span class=\"hljs-type\">vec4</span>(<span class=\"hljs-built_in\">mix</span>(<span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">1.0</span>), <span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">0.0</span>), density), density);<br>            color.rgb *= color.a;<br><br>            res += color * (<span class=\"hljs-number\">1.0</span> - res.a);<br>        &#125;<br><br>        depth+=dt;<br>    &#125;<br><br>    <span class=\"hljs-keyword\">return</span> res;<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<p><img src=\"/2024/11/04/ProceduralTerrainGeneration2/terrain_with_all_cloud.png\" alt=\"增加体积云\"></p>\n<h1 id=\"最终效果\"><a href=\"#最终效果\" class=\"headerlink\" title=\"最终效果\"></a>最终效果</h1><iframe width=\"640\" height=\"360\" frameborder=\"0\" src=\"https://www.shadertoy.com/embed/4XByRV?gui=true&t=10&paused=true&muted=false\" allowfullscreen></iframe>\n\n<p>以上便是我如何实现这个场景的简单介绍了。这是我初次尝试在ShaderToy上渲染大场景，也是第一次将整个步骤拆解讲解，可能还是会有不少地方讲的不过仔细和通透，也有些遗漏。</p>\n<p>后面我还是会继续尝试创作和讲解，希望能给人带来帮助，也有助于巩固我所学的知识。</p>\n","excerpt":"","more":"<p>距离上一篇将程序化地形生成的<a href=\"https://ruochenhua.github.io/2024/10/11/ProceduralTerrainGeneration/\">教程</a>也已经过去了一段时间了。前段时间一直有其他事情需要忙，现在终于有时间继续之前未完成的工作了。</p>\n<h1 id=\"丰富地形\"><a href=\"#丰富地形\" class=\"headerlink\" title=\"丰富地形\"></a>丰富地形</h1><p>上一篇教程我们已经创建出了绵延的山脉，如下图所示：<br><img src=\"/2024/11/04/ProceduralTerrainGeneration2/terrain_no_grass.png\" alt=\"上次渲染的结果\"><br>看起来似乎是有那么点意思了，但是这样的山脉效果还是太单调了。</p>\n<p>接下来我们就计划丰富一下这个场景。</p>\n<h2 id=\"增加绿植\"><a href=\"#增加绿植\" class=\"headerlink\" title=\"增加绿植\"></a>增加绿植</h2><p>我们场景中的山光秃秃的，很像是沙漠，也很像火星上的地貌。</p>\n<p>我想给场景增加一些层次，一些生机，因此我打算将山的一部分渲染为绿植部分。</p>\n<p>那么如何确定哪些部分是绿植，哪些部分是裸露的山体呢？根据地形表面的法线可以做一个简单的判定：法线的y分量越大，也就是面向上的分量越大，表明这个面更平，因此有绿植是更合适的；反正则表明这个面更加陡峭，更适合作为山体表现。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-type\">vec3</span> dirt_color = <span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">0.8549</span>, <span class=\"hljs-number\">0.5255</span>, <span class=\"hljs-number\">0.3098</span>);<br><span class=\"hljs-type\">vec3</span> grass_color = <span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">0.3137</span>, <span class=\"hljs-number\">0.5412</span>, <span class=\"hljs-number\">0.0157</span>);<br><span class=\"hljs-comment\">// ..</span><br><br><span class=\"hljs-keyword\">if</span>(rd.y &lt; <span class=\"hljs-number\">0.05</span> &amp;&amp; rayMarchingTerrain(ro, rd, maxt, res_t))<br>&#123;<br>    <span class=\"hljs-type\">vec3</span> height_pos = ro+res_t*rd;<br><br>    <span class=\"hljs-comment\">// calculate normla</span><br>    <span class=\"hljs-type\">vec3</span> normal = getNormal(height_pos);<br>    <span class=\"hljs-type\">float</span> grass_ratio = <span class=\"hljs-built_in\">smoothstep</span>(<span class=\"hljs-number\">0.7</span>, <span class=\"hljs-number\">0.98</span>, normal.y);<br>    <span class=\"hljs-type\">vec3</span> ground_color = <span class=\"hljs-built_in\">mix</span>(dirt_color, grass_color, grass_ratio);<br>&#125;<br><br></code></pre></td></tr></table></figure>\n\n<p>上面是一段实例代码，我们预先设定山体和绿植的颜色，在光线步进山体的时候，获取normal，根据法线的y方向的大小（也可以用dot来判定）做smoothstep取一个合适的值作为当前平面草地的比例，最后混合山体和草地的比例。得到的结果如下：</p>\n<p><img src=\"/2024/11/04/ProceduralTerrainGeneration2/terrain_with_grass.png\" alt=\"增加绿植\"><br>这样一来场景的丰富程度一下子就提升了。</p>\n<h1 id=\"天空\"><a href=\"#天空\" class=\"headerlink\" title=\"天空\"></a>天空</h1><p>现在最终结果还是挺奇怪的，很大一部分的原因是场景的背景还是黑咕隆咚的。我们的大场景急需增加天空的部分。</p>\n<h2 id=\"天空的颜色\"><a href=\"#天空的颜色\" class=\"headerlink\" title=\"天空的颜色\"></a>天空的颜色</h2><p>天空的颜色我之前有写过一个<a href=\"https://ruochenhua.github.io/2024/10/15/single-scatter-atmosphere/\">教程</a>，是利用单次散射的方法来计算天空大气的颜色。当然这个方法在这依然是可用的。</p>\n<p>不过如果你说，我不想用那么复杂的方法，只想快速出效果呢？那也是很简单，给地形光线步近没有结果的像素设定为蓝色就行了，哈哈。</p>\n<p>如果想再复杂一点，可以做一个小细节的优化。也就是在晴朗的情况下，一般靠近地平线的天的蓝色是更加浅的，所以在给天空像素上色的时候，可以根据当前像素的方向的y值作为判定，在深蓝色和浅蓝色做线性插值。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-type\">vec3</span> low_sky_blue = <span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">0.5137</span>, <span class=\"hljs-number\">0.7804</span>, <span class=\"hljs-number\">0.9608</span>);<br><span class=\"hljs-type\">vec3</span> high_sky_blue = <span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">0.3216</span>, <span class=\"hljs-number\">0.4706</span>, <span class=\"hljs-number\">0.9725</span>);<br><br><span class=\"hljs-type\">vec3</span> sky_color = <span class=\"hljs-built_in\">mix</span>(low_sky_blue, high_sky_blue, <span class=\"hljs-built_in\">clamp</span>(rd.y, <span class=\"hljs-number\">0.0</span>, <span class=\"hljs-number\">1.0</span>));<br></code></pre></td></tr></table></figure>\n\n<p><img src=\"/2024/11/04/ProceduralTerrainGeneration2/terrain_with_sky.png\" alt=\"增加天空\"></p>\n<h2 id=\"雾气增强层次感\"><a href=\"#雾气增强层次感\" class=\"headerlink\" title=\"雾气增强层次感\"></a>雾气增强层次感</h2><p>在增加了天空之后，我们的渲染结果有了很显著的提升，终于像室外的场景了。</p>\n<p>不过仔细看后，还是觉得怪怪的，远处的山和天空根本不在一个图层上；山的远近层次感也不足，远处的山太清晰了。</p>\n<p>这是因为缺少雾气的结果，雾气能够很好的增加大场景体量感，能够很好的处理场景和天空衔接的感觉。</p>\n<p>增加雾气也十分简单，仅需根据地形光线步近的结果的大小（也就是地形离相机的距离）来计算一个比例，根据这个比例来做场景颜色和雾气颜色的插值即可。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-comment\">// cal fog density, different between r,g,b so the fog has a blue hue</span><br><span class=\"hljs-type\">vec3</span> calcFog(<span class=\"hljs-type\">float</span> dist)<br>&#123;    <br>    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-built_in\">exp</span>(<span class=\"hljs-number\">-5e-3</span>*dist*<span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">4</span>));<br>&#125;<br><br><span class=\"hljs-comment\">// ...</span><br><span class=\"hljs-type\">vec3</span> fog_amount = calcFog(res_t);<br>color = <span class=\"hljs-built_in\">mix</span>(<span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">0.7</span>),color, fog_amount);<br></code></pre></td></tr></table></figure>\n<p>上面这段便是雾气的计算过程。雾气的比例通过*<em>exp(-5e-3</em>dist*vec3(1,2,4))**计算，得到的雾的比例用于最终颜色的线性插值。</p>\n<p><img src=\"/2024/11/04/ProceduralTerrainGeneration2/terrain_with_sky_fog.png\" alt=\"增加雾气\"><br>有了雾气的加持，我们场景的大体量感就有了，层次感也明显了很多。<br>关于雾气的更多内容，可以参考Inigo大神的<a href=\"https://iquilezles.org/articles/fog/\">这篇文章</a>。</p>\n<h2 id=\"漫反射光照细节\"><a href=\"#漫反射光照细节\" class=\"headerlink\" title=\"漫反射光照细节\"></a>漫反射光照细节</h2><p>接下来我想优化一下场景整体的漫反射细节。</p>\n<p>当前的渲染结果其实是有加一个默认的环境光**vec3(0.1)**的，主要是之前的场景太黑了。这是个临时的处理，把这个环境光去掉。<br><img src=\"/2024/11/04/ProceduralTerrainGeneration2/terrain_with_sky_fog_less_ambient.png\" alt=\"去掉写死的环境光补偿\"></p>\n<p>好了，我们再来看一下我们的场景。有没有感觉山体的阴影部分有点太黑了？是的，在现实世界中，处于阴影的场景，在大白天也会被场景的漫反射光给提亮的，这种一片漆黑的感觉是有些难看。所以接下来我们来增加一些场景中应该有的漫反射光。</p>\n<p>首先山体之间是会有漫反射光的影响的，也就是阴影部分的山体会被不在阴影的其他山的漫反射光照亮。当然这种影响我们是无法实时计算的，要计算的东西实在是太多了。但是可以通过一个很简单的方式来提单：我们计算当前点的发现和指向光源方向的点乘，当这个结果大于0的时候，用这个乘积乘以一个小的光照并加到原本的颜色上。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\">color += (<span class=\"hljs-built_in\">max</span>(<span class=\"hljs-number\">0.0</span>, <span class=\"hljs-built_in\">dot</span>(-light_dir, normal))*ground_color/<span class=\"hljs-number\">10.0</span>);<br></code></pre></td></tr></table></figure>\n\n<p><img src=\"/2024/11/04/ProceduralTerrainGeneration2/terrain_with_sky_fog_diffuse_from_mountain.png\" alt=\"增加地形的漫反射光\"></p>\n<p>加完地形的漫反射光后，天空其实也有各个方向的漫反射的光会影响地形的亮度。天空的漫反射光可以通过地形法线的y分量和天空颜色的一部分来计算，示例代码如下：</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\">color += (normal.y + <span class=\"hljs-number\">1.0</span>)/<span class=\"hljs-number\">2.0</span>*low_sky_blue/<span class=\"hljs-number\">10.0</span>;<br></code></pre></td></tr></table></figure>\n\n<p><img src=\"/2024/11/04/ProceduralTerrainGeneration2/terrain_with_sky_fog_diffuse_from_sky.png\" alt=\"增加天空的漫反射光\"></p>\n<p>增加了这些漫反射细节，场景的真实度进一步得到了提升。</p>\n<h1 id=\"云朵\"><a href=\"#云朵\" class=\"headerlink\" title=\"云朵\"></a>云朵</h1><p>有了天空，再给天空上增加云的话会进一步增加场景的丰富度，下面就来介绍我给这个场景添加的两部分云的计算。</p>\n<h2 id=\"高层云\"><a href=\"#高层云\" class=\"headerlink\" title=\"高层云\"></a>高层云</h2><p>我给这个场景添加了两种云，高层云是我想模拟在高度很高的地方，云层看起来不厚并且相对静态的感觉。</p>\n<p>高层云的纹理也是利用perlin noise来计算，由于高层的云没有厚实感其实用2D的perlin noise也足够了。</p>\n<p>另外需要注意的是，云的纹理需要根据远近来做处理。远处的云的纹理如果太清晰的话在最终效果上会显得太过于密集。下面是高层云的示例代码：</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-comment\">// 高层云的高度</span><br><span class=\"hljs-type\">float</span> top_sky_plane = <span class=\"hljs-number\">3000.</span>;<br><br><span class=\"hljs-type\">vec3</span> getSkyColor(<span class=\"hljs-type\">vec3</span> ro, <span class=\"hljs-type\">vec3</span> rd)<br>&#123;    <br>    <span class=\"hljs-type\">vec3</span> hit_sky;<br>    hit_sky.y = top_sky_plane;<br>        <br>    hit_sky.xz = ro.xz + rd.xz * (top_sky_plane - ro.y)/rd.y;    <br>    <br>    <span class=\"hljs-comment\">//降低远处云的密度，看起来效果更好</span><br>    <span class=\"hljs-type\">float</span> hit_dist = <span class=\"hljs-built_in\">distance</span>(hit_sky, ro);<br>    <span class=\"hljs-type\">float</span> cloud_density_percentage = <span class=\"hljs-number\">1.0</span>;<br>    <span class=\"hljs-keyword\">if</span>(hit_dist &gt; cloud_view_distance)<br>    &#123;<br>        cloud_density_percentage *= <span class=\"hljs-built_in\">exp</span>(-(hit_dist - cloud_view_distance)/ cloud_view_distance);<br>    &#125;    <br><br>    <span class=\"hljs-comment\">// 根据云层采样点的远近处理云层的密度</span><br>    <span class=\"hljs-type\">float</span> cloud_density = <span class=\"hljs-built_in\">smoothstep</span>(getCloudDensity(hit_sky.xz/<span class=\"hljs-number\">150.0</span>, <span class=\"hljs-number\">3</span>), <span class=\"hljs-number\">-0.99</span>, <span class=\"hljs-number\">1.9</span>)*cloud_density_percentage * <span class=\"hljs-number\">0.5</span>;<br>    <span class=\"hljs-type\">float</span> res_t;<br><br>    <span class=\"hljs-comment\">// sky color    </span><br>    <span class=\"hljs-type\">vec3</span> sky_color = <span class=\"hljs-built_in\">mix</span>(low_sky_blue, high_sky_blue, <span class=\"hljs-built_in\">clamp</span>(rd.y, <span class=\"hljs-number\">0.0</span>, <span class=\"hljs-number\">1.0</span>));<br>    <span class=\"hljs-type\">vec3</span> cloud_color = <span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">1.</span>);<br>    <span class=\"hljs-comment\">// 根据云层密度得到最终的高层云和天空颜色</span><br>    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-built_in\">mix</span>(sky_color, cloud_color, cloud_density);<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<p><img src=\"/2024/11/04/ProceduralTerrainGeneration2/terrain_with_high_cloud.png\" alt=\"增加天空高层云\"></p>\n<p>增加了高层云之后，天空就显得不那么单调了。</p>\n<h2 id=\"体积云\"><a href=\"#体积云\" class=\"headerlink\" title=\"体积云\"></a>体积云</h2><p>最后就是体积云了，体积云的部分我不会在这里详细讨论，这是一个可以讨论的比较深的话题。我这个场景的体积云效果也并不是很理想，这里仅作为一个最后的点缀，在山峰上增加一点点动态效果。</p>\n<p>体积云的效果其实也是在一定区域内进行多个点的采样而来，这个区域内利用FBM来实现云的动态的随机效果。在这个场景内我使用了SDF来圈定体积云的范围，并且给SDF增加了随机现状来获取更好的随机效果。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-type\">float</span> scene(<span class=\"hljs-keyword\">in</span> <span class=\"hljs-type\">vec3</span> pos)<br>&#123;    <br>    <span class=\"hljs-type\">vec3</span> cloud_pos = <span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">0.0</span>, <span class=\"hljs-number\">5.0</span>, <span class=\"hljs-number\">15.0</span>);<br>    <br>    <span class=\"hljs-type\">vec3</span> filter_pos = <span class=\"hljs-type\">vec3</span>(pos.x, pos.y+iTime, pos.z+iTime);<br>    pos -= cloud_pos;<br>    <span class=\"hljs-type\">float</span> rst = -(rm_box(pos)) + fbm_cloud(pos * <span class=\"hljs-number\">0.1</span>+iTime*<span class=\"hljs-number\">0.3</span>, <span class=\"hljs-number\">5</span>);<br>    rst = rst / <span class=\"hljs-number\">25.0</span> * <span class=\"hljs-built_in\">max</span>(fbm_cloud(filter_pos*<span class=\"hljs-number\">0.1</span>, <span class=\"hljs-number\">1</span>) - <span class=\"hljs-number\">1.2</span>, <span class=\"hljs-number\">0.0</span>); <br>    <span class=\"hljs-keyword\">return</span> rst;<br>&#125;<br><br><span class=\"hljs-type\">float</span> max_cloud_dist = <span class=\"hljs-number\">80.</span>;<br><span class=\"hljs-type\">vec4</span> renderMidClouds(<span class=\"hljs-type\">vec3</span> ro, <span class=\"hljs-type\">vec3</span> rd)<br>&#123;    <br>    <span class=\"hljs-type\">vec4</span> res = <span class=\"hljs-type\">vec4</span>(<span class=\"hljs-number\">0.0</span>);<br>    <span class=\"hljs-type\">float</span> depth = <span class=\"hljs-number\">0.0</span>;    <br>    <br>    <span class=\"hljs-type\">int</span> sample_count = <span class=\"hljs-number\">64</span>;<br>    <span class=\"hljs-type\">float</span> dt = max_cloud_dist / <span class=\"hljs-type\">float</span>(sample_count);<br>    <br>    <span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">int</span> i = <span class=\"hljs-number\">0</span>; i &lt; sample_count; ++i)<br>    &#123;<br>        <span class=\"hljs-type\">vec3</span> p = ro + depth*rd;<br>        <span class=\"hljs-type\">float</span> density = scene(p);<br>        <span class=\"hljs-keyword\">if</span>(density &gt; <span class=\"hljs-number\">0.0</span>)<br>        &#123;<br>            <span class=\"hljs-type\">float</span> diffuse = <span class=\"hljs-built_in\">clamp</span>((scene(p) - scene(p + <span class=\"hljs-number\">0.3</span>*light_dir))/<span class=\"hljs-number\">0.3</span>, <span class=\"hljs-number\">0.0</span>, <span class=\"hljs-number\">1.0</span>);<br>            <span class=\"hljs-type\">vec3</span> lin = <span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">0.8</span>, <span class=\"hljs-number\">0.8</span>, <span class=\"hljs-number\">0.8</span>) * <span class=\"hljs-number\">1.1</span> + <span class=\"hljs-number\">0.8</span> * <span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">0.9333</span>, <span class=\"hljs-number\">0.702</span>, <span class=\"hljs-number\">0.5255</span>)*diffuse;<br>            <span class=\"hljs-type\">vec4</span> color = <span class=\"hljs-type\">vec4</span>(<span class=\"hljs-built_in\">mix</span>(<span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">1.0</span>), <span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">0.0</span>), density), density);<br>            color.rgb *= color.a;<br><br>            res += color * (<span class=\"hljs-number\">1.0</span> - res.a);<br>        &#125;<br><br>        depth+=dt;<br>    &#125;<br><br>    <span class=\"hljs-keyword\">return</span> res;<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<p><img src=\"/2024/11/04/ProceduralTerrainGeneration2/terrain_with_all_cloud.png\" alt=\"增加体积云\"></p>\n<h1 id=\"最终效果\"><a href=\"#最终效果\" class=\"headerlink\" title=\"最终效果\"></a>最终效果</h1><iframe width=\"640\" height=\"360\" frameborder=\"0\" src=\"https://www.shadertoy.com/embed/4XByRV?gui=true&t=10&paused=true&muted=false\" allowfullscreen></iframe>\n\n<p>以上便是我如何实现这个场景的简单介绍了。这是我初次尝试在ShaderToy上渲染大场景，也是第一次将整个步骤拆解讲解，可能还是会有不少地方讲的不过仔细和通透，也有些遗漏。</p>\n<p>后面我还是会继续尝试创作和讲解，希望能给人带来帮助，也有助于巩固我所学的知识。</p>\n"},{"title":"浅谈数字人仿真的渲染技术（一）","date":"2024-12-28T07:24:16.000Z","index_img":"/2024/12/28/digital-human-render-1/metahuman_mugshot.jpg","banner_img":"/2024/12/28/digital-human-render-1/metahuman_mugshot.jpg","_content":"\n# 前言\n在2022年12月，我受邀在OGEEK上做过一次关于数字人渲染技术的分享，名为《浅谈数字人仿真的渲染技术》。为了这次分享我查阅了大量资料做了很多准备，但是很不幸的是在分享的前两天我便感染了新冠，身体开始发烧外加喉咙开始隐隐作痛。为了不影响OGEEK的流程我便在病情还未恶化的时候将分享提前录了下来，以播片的形式参加。\n\n这段经历确实还挺难忘，近期又翻到了这篇准备的PPT，和演讲的录屏，于是想将这部分内容做个记录，整理成文章分享出来。\n\n由于PPT内容还是比较多的，哪怕是精简后的版本也还有50多页，于是乎会准备做成一个系列，当初精简掉的部分可能也会想办法补充回来，让内容尽量的充分。\n\n# 数字人简介\n## 数字人的定义\n目前数字人缺乏一个统一的标准定义，我们从它的发展起源，从技术角度上选择一个最宽泛最简洁的标准：**由计算机生成的人类。**\n\n中国人工智能产业发展联盟发布的《2020年虚拟数字人发展白皮书》中给了一个更加详细的定义：虚拟数字人意指具有数字化外形的虚拟人物，除了拥有人的**外观**、人的**行为**之外、还有拥有人的**思想**，具有**识别外界环境**、并能与**人交流互动**的能力。\n\n那其实这个定义里面，也描述了数字人相关的几个关键技术方向，包括：*渲染-外观，行为-驱动算法，思想-AI，识别外接环境-感知，与人交流互动-表达*。\n\n## 数字人的发展历史\n\n在上世纪80年代，其实就有虚拟形象引入到现实世界的想法。\n1982 年，动画片《太空堡垒》中的女角色林明美作为虚拟歌姬出道，其专辑也成功打入当时的知名音乐排行榜。日本媒体率先提出了“虚拟偶像”的称号。1984 年，世界首位虚拟电影演员“Max Headroom”诞生，出演电影，并拍摄数支广告，在英国家喻户晓。\n\n![林明美](林明美.png)\n\n此时，虚拟人概念先行，给予虚拟形象以立体化人设，并带入大众视野。但受制于技术发展，“数字化”在这个阶段并不明显。打造虚拟人的技术以手工绘制为主，人物形象以 2 D 卡通的形式展现，展现方式以事先完成的音频和视频为主，并不具备实时交互功能。\n\n进入 21 世纪，虚拟人的 数字化特征逐渐明显。形象创建上，虚拟数字人开始从手绘转向 CG和动捕等计算机技术。\n\n2007 年，日本虚拟歌手“初音未来”的诞生与流行。初音未来的虚拟形象采用 CG 和动作捕捉技术。在动作捕捉技术的助力下，初音未来可以直接采用人类的表情和动作，并借助 CG 技术真实的360度渲染出来。作为虚拟歌姬，初音未来的歌喉基于 VOCALOID（电子音乐制作 的 语音合成软件）。采样于日本声优藤田咲，创作者只需要输入歌词和旋律，就能够自动形成歌曲。\n\n![初音未来](初音未来.png)\n\n近年来，由于各项技术的不断发展，出现了越来越多高真实度的数字人形象。\n\n比如说2016年出现的miquela，她在ins上的出现引发了一场“真假辩论”。许多粉丝相信她是真实存在的人物，只是修图“狠”了点。直到黑客们入侵了她的账号，才最终确定了她是由 3 D 电脑动画公司制作的虚拟人。她甚至在2018年一起被美国《时代》周刊列为“25 位最有影响力的互联网人物”\n\n同样是在2018年，由腾讯、Epic Games推出了Siren项目。Siren 的所有动作表情都由实时捕捉以及实时渲染形成，并且整个过程只有15毫秒，60帧。Siren在渲染的真实性和交互性之间找到平衡，打造了具备实时交互能力的数字虚拟人。\n\n![siren](siren.png)\n\n## 数字人的分类\n数字人可以按照不同维度进行分类。\n\n按照美术风格：\n - 2D、3D\n - 写实、卡通、风格化\n\n按照驱动方式：\n - 真人驱动\n - AI驱动\n\n按照商业和功能维度：\n - 内容/IP型\n - 功能服务型\n - 虚拟分身\n\n## 数字人的发展\n\n近几年，虚拟数字人在电商、金融、影视、游戏和金融等行业都拥有不同大小的市场规模。\n我们拿虚拟偶像的市场作为例子。虚拟偶像行业2020年中国的市场规模为34.6亿元，预计2023年将达到205.2亿元。带动的市场从2020的645.6亿元，预计2023年增长到3334.7亿元，是一个指数级的增长。\n\n当然除了虚拟偶像数字人还有很多其他方面的应用，所以市场前景是非常可观的。下面是一个虚拟偶像市场规模及预测的分析。\n\n![虚拟偶像市场规模](虚拟偶像市场规模.png)\n\n\n# 数字人的制作流程简介\n\n数字人制作大致分4个阶段：\n 1. 第一阶段（形象设计）：明确形象设计方向。\n\n 2. 第二阶段（模型制作）：根据平面形象，进行模型搭建。\n这里我们以可能是最为复杂的超写实数字人的制作流程进行举例，首先在lightstage里面扫描模型（扫描仪，360度单反相机阵列，300多个相机组成）。扫描出来的模型是一个点云，需要模型师去调整，抚平一些瑕疵。去除扫描的毛刺。有些部位可能拍照的时候出现遮挡（比如耳后），需要在模型软件工具中处理好。\n![老黄扫描](老黄扫描.png)\n![老黄建模](老黄建模.png)\n\n 3. 第三阶段（驱动绑定）：\n面部动画face rig绑定驱动，通过动画，人脸识别，或者AI去驱动。或者使用blendshape等技术。\n身体躯干使用骨骼绑定，辅以动作捕捉等等。\n![metahuman(UE)面部驱动](metahuman面部驱动.png)\n![老黄驱动](老黄驱动.png)\n\n 4. 第四阶段（渲染）：将场景、人物放入渲染工具进行渲染输出，常用的工具包括nVidia omniverse、unreal engine等等。\n![老黄模型渲染](老黄模型渲染.png)\n\n目前虚幻引擎5的metahuman creator是一个很流程化且易于使用的数字人制作工具。\n\n# 总结\n第一部分先总结到这里，在后面的部分我会更加详细的介绍一些数字人渲染技术，包括皮肤、头发的渲染以及卡通渲染等等。","source":"_posts/digital-human-render-1.md","raw":"---\ntitle: 浅谈数字人仿真的渲染技术（一）\ndate: 2024-12-28 15:24:16\ncategories: \n\t- 技术漫谈\ntags: [3D, 渲染, 数字孪生]\n\nindex_img: /2024/12/28/digital-human-render-1/metahuman_mugshot.jpg\nbanner_img: /2024/12/28/digital-human-render-1/metahuman_mugshot.jpg\n---\n\n# 前言\n在2022年12月，我受邀在OGEEK上做过一次关于数字人渲染技术的分享，名为《浅谈数字人仿真的渲染技术》。为了这次分享我查阅了大量资料做了很多准备，但是很不幸的是在分享的前两天我便感染了新冠，身体开始发烧外加喉咙开始隐隐作痛。为了不影响OGEEK的流程我便在病情还未恶化的时候将分享提前录了下来，以播片的形式参加。\n\n这段经历确实还挺难忘，近期又翻到了这篇准备的PPT，和演讲的录屏，于是想将这部分内容做个记录，整理成文章分享出来。\n\n由于PPT内容还是比较多的，哪怕是精简后的版本也还有50多页，于是乎会准备做成一个系列，当初精简掉的部分可能也会想办法补充回来，让内容尽量的充分。\n\n# 数字人简介\n## 数字人的定义\n目前数字人缺乏一个统一的标准定义，我们从它的发展起源，从技术角度上选择一个最宽泛最简洁的标准：**由计算机生成的人类。**\n\n中国人工智能产业发展联盟发布的《2020年虚拟数字人发展白皮书》中给了一个更加详细的定义：虚拟数字人意指具有数字化外形的虚拟人物，除了拥有人的**外观**、人的**行为**之外、还有拥有人的**思想**，具有**识别外界环境**、并能与**人交流互动**的能力。\n\n那其实这个定义里面，也描述了数字人相关的几个关键技术方向，包括：*渲染-外观，行为-驱动算法，思想-AI，识别外接环境-感知，与人交流互动-表达*。\n\n## 数字人的发展历史\n\n在上世纪80年代，其实就有虚拟形象引入到现实世界的想法。\n1982 年，动画片《太空堡垒》中的女角色林明美作为虚拟歌姬出道，其专辑也成功打入当时的知名音乐排行榜。日本媒体率先提出了“虚拟偶像”的称号。1984 年，世界首位虚拟电影演员“Max Headroom”诞生，出演电影，并拍摄数支广告，在英国家喻户晓。\n\n![林明美](林明美.png)\n\n此时，虚拟人概念先行，给予虚拟形象以立体化人设，并带入大众视野。但受制于技术发展，“数字化”在这个阶段并不明显。打造虚拟人的技术以手工绘制为主，人物形象以 2 D 卡通的形式展现，展现方式以事先完成的音频和视频为主，并不具备实时交互功能。\n\n进入 21 世纪，虚拟人的 数字化特征逐渐明显。形象创建上，虚拟数字人开始从手绘转向 CG和动捕等计算机技术。\n\n2007 年，日本虚拟歌手“初音未来”的诞生与流行。初音未来的虚拟形象采用 CG 和动作捕捉技术。在动作捕捉技术的助力下，初音未来可以直接采用人类的表情和动作，并借助 CG 技术真实的360度渲染出来。作为虚拟歌姬，初音未来的歌喉基于 VOCALOID（电子音乐制作 的 语音合成软件）。采样于日本声优藤田咲，创作者只需要输入歌词和旋律，就能够自动形成歌曲。\n\n![初音未来](初音未来.png)\n\n近年来，由于各项技术的不断发展，出现了越来越多高真实度的数字人形象。\n\n比如说2016年出现的miquela，她在ins上的出现引发了一场“真假辩论”。许多粉丝相信她是真实存在的人物，只是修图“狠”了点。直到黑客们入侵了她的账号，才最终确定了她是由 3 D 电脑动画公司制作的虚拟人。她甚至在2018年一起被美国《时代》周刊列为“25 位最有影响力的互联网人物”\n\n同样是在2018年，由腾讯、Epic Games推出了Siren项目。Siren 的所有动作表情都由实时捕捉以及实时渲染形成，并且整个过程只有15毫秒，60帧。Siren在渲染的真实性和交互性之间找到平衡，打造了具备实时交互能力的数字虚拟人。\n\n![siren](siren.png)\n\n## 数字人的分类\n数字人可以按照不同维度进行分类。\n\n按照美术风格：\n - 2D、3D\n - 写实、卡通、风格化\n\n按照驱动方式：\n - 真人驱动\n - AI驱动\n\n按照商业和功能维度：\n - 内容/IP型\n - 功能服务型\n - 虚拟分身\n\n## 数字人的发展\n\n近几年，虚拟数字人在电商、金融、影视、游戏和金融等行业都拥有不同大小的市场规模。\n我们拿虚拟偶像的市场作为例子。虚拟偶像行业2020年中国的市场规模为34.6亿元，预计2023年将达到205.2亿元。带动的市场从2020的645.6亿元，预计2023年增长到3334.7亿元，是一个指数级的增长。\n\n当然除了虚拟偶像数字人还有很多其他方面的应用，所以市场前景是非常可观的。下面是一个虚拟偶像市场规模及预测的分析。\n\n![虚拟偶像市场规模](虚拟偶像市场规模.png)\n\n\n# 数字人的制作流程简介\n\n数字人制作大致分4个阶段：\n 1. 第一阶段（形象设计）：明确形象设计方向。\n\n 2. 第二阶段（模型制作）：根据平面形象，进行模型搭建。\n这里我们以可能是最为复杂的超写实数字人的制作流程进行举例，首先在lightstage里面扫描模型（扫描仪，360度单反相机阵列，300多个相机组成）。扫描出来的模型是一个点云，需要模型师去调整，抚平一些瑕疵。去除扫描的毛刺。有些部位可能拍照的时候出现遮挡（比如耳后），需要在模型软件工具中处理好。\n![老黄扫描](老黄扫描.png)\n![老黄建模](老黄建模.png)\n\n 3. 第三阶段（驱动绑定）：\n面部动画face rig绑定驱动，通过动画，人脸识别，或者AI去驱动。或者使用blendshape等技术。\n身体躯干使用骨骼绑定，辅以动作捕捉等等。\n![metahuman(UE)面部驱动](metahuman面部驱动.png)\n![老黄驱动](老黄驱动.png)\n\n 4. 第四阶段（渲染）：将场景、人物放入渲染工具进行渲染输出，常用的工具包括nVidia omniverse、unreal engine等等。\n![老黄模型渲染](老黄模型渲染.png)\n\n目前虚幻引擎5的metahuman creator是一个很流程化且易于使用的数字人制作工具。\n\n# 总结\n第一部分先总结到这里，在后面的部分我会更加详细的介绍一些数字人渲染技术，包括皮肤、头发的渲染以及卡通渲染等等。","slug":"digital-human-render-1","published":1,"updated":"2024-12-28T11:40:17.307Z","comments":1,"layout":"post","photos":[],"_id":"cm5ht44vg000834578p411g6s","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><p>在2022年12月，我受邀在OGEEK上做过一次关于数字人渲染技术的分享，名为《浅谈数字人仿真的渲染技术》。为了这次分享我查阅了大量资料做了很多准备，但是很不幸的是在分享的前两天我便感染了新冠，身体开始发烧外加喉咙开始隐隐作痛。为了不影响OGEEK的流程我便在病情还未恶化的时候将分享提前录了下来，以播片的形式参加。</p>\n<p>这段经历确实还挺难忘，近期又翻到了这篇准备的PPT，和演讲的录屏，于是想将这部分内容做个记录，整理成文章分享出来。</p>\n<p>由于PPT内容还是比较多的，哪怕是精简后的版本也还有50多页，于是乎会准备做成一个系列，当初精简掉的部分可能也会想办法补充回来，让内容尽量的充分。</p>\n<h1 id=\"数字人简介\"><a href=\"#数字人简介\" class=\"headerlink\" title=\"数字人简介\"></a>数字人简介</h1><h2 id=\"数字人的定义\"><a href=\"#数字人的定义\" class=\"headerlink\" title=\"数字人的定义\"></a>数字人的定义</h2><p>目前数字人缺乏一个统一的标准定义，我们从它的发展起源，从技术角度上选择一个最宽泛最简洁的标准：<strong>由计算机生成的人类。</strong></p>\n<p>中国人工智能产业发展联盟发布的《2020年虚拟数字人发展白皮书》中给了一个更加详细的定义：虚拟数字人意指具有数字化外形的虚拟人物，除了拥有人的<strong>外观</strong>、人的<strong>行为</strong>之外、还有拥有人的<strong>思想</strong>，具有<strong>识别外界环境</strong>、并能与<strong>人交流互动</strong>的能力。</p>\n<p>那其实这个定义里面，也描述了数字人相关的几个关键技术方向，包括：<em>渲染-外观，行为-驱动算法，思想-AI，识别外接环境-感知，与人交流互动-表达</em>。</p>\n<h2 id=\"数字人的发展历史\"><a href=\"#数字人的发展历史\" class=\"headerlink\" title=\"数字人的发展历史\"></a>数字人的发展历史</h2><p>在上世纪80年代，其实就有虚拟形象引入到现实世界的想法。<br>1982 年，动画片《太空堡垒》中的女角色林明美作为虚拟歌姬出道，其专辑也成功打入当时的知名音乐排行榜。日本媒体率先提出了“虚拟偶像”的称号。1984 年，世界首位虚拟电影演员“Max Headroom”诞生，出演电影，并拍摄数支广告，在英国家喻户晓。</p>\n<p><img src=\"/2024/12/28/digital-human-render-1/%E6%9E%97%E6%98%8E%E7%BE%8E.png\" alt=\"林明美\"></p>\n<p>此时，虚拟人概念先行，给予虚拟形象以立体化人设，并带入大众视野。但受制于技术发展，“数字化”在这个阶段并不明显。打造虚拟人的技术以手工绘制为主，人物形象以 2 D 卡通的形式展现，展现方式以事先完成的音频和视频为主，并不具备实时交互功能。</p>\n<p>进入 21 世纪，虚拟人的 数字化特征逐渐明显。形象创建上，虚拟数字人开始从手绘转向 CG和动捕等计算机技术。</p>\n<p>2007 年，日本虚拟歌手“初音未来”的诞生与流行。初音未来的虚拟形象采用 CG 和动作捕捉技术。在动作捕捉技术的助力下，初音未来可以直接采用人类的表情和动作，并借助 CG 技术真实的360度渲染出来。作为虚拟歌姬，初音未来的歌喉基于 VOCALOID（电子音乐制作 的 语音合成软件）。采样于日本声优藤田咲，创作者只需要输入歌词和旋律，就能够自动形成歌曲。</p>\n<p><img src=\"/2024/12/28/digital-human-render-1/%E5%88%9D%E9%9F%B3%E6%9C%AA%E6%9D%A5.png\" alt=\"初音未来\"></p>\n<p>近年来，由于各项技术的不断发展，出现了越来越多高真实度的数字人形象。</p>\n<p>比如说2016年出现的miquela，她在ins上的出现引发了一场“真假辩论”。许多粉丝相信她是真实存在的人物，只是修图“狠”了点。直到黑客们入侵了她的账号，才最终确定了她是由 3 D 电脑动画公司制作的虚拟人。她甚至在2018年一起被美国《时代》周刊列为“25 位最有影响力的互联网人物”</p>\n<p>同样是在2018年，由腾讯、Epic Games推出了Siren项目。Siren 的所有动作表情都由实时捕捉以及实时渲染形成，并且整个过程只有15毫秒，60帧。Siren在渲染的真实性和交互性之间找到平衡，打造了具备实时交互能力的数字虚拟人。</p>\n<p><img src=\"/2024/12/28/digital-human-render-1/siren.png\" alt=\"siren\"></p>\n<h2 id=\"数字人的分类\"><a href=\"#数字人的分类\" class=\"headerlink\" title=\"数字人的分类\"></a>数字人的分类</h2><p>数字人可以按照不同维度进行分类。</p>\n<p>按照美术风格：</p>\n<ul>\n<li>2D、3D</li>\n<li>写实、卡通、风格化</li>\n</ul>\n<p>按照驱动方式：</p>\n<ul>\n<li>真人驱动</li>\n<li>AI驱动</li>\n</ul>\n<p>按照商业和功能维度：</p>\n<ul>\n<li>内容&#x2F;IP型</li>\n<li>功能服务型</li>\n<li>虚拟分身</li>\n</ul>\n<h2 id=\"数字人的发展\"><a href=\"#数字人的发展\" class=\"headerlink\" title=\"数字人的发展\"></a>数字人的发展</h2><p>近几年，虚拟数字人在电商、金融、影视、游戏和金融等行业都拥有不同大小的市场规模。<br>我们拿虚拟偶像的市场作为例子。虚拟偶像行业2020年中国的市场规模为34.6亿元，预计2023年将达到205.2亿元。带动的市场从2020的645.6亿元，预计2023年增长到3334.7亿元，是一个指数级的增长。</p>\n<p>当然除了虚拟偶像数字人还有很多其他方面的应用，所以市场前景是非常可观的。下面是一个虚拟偶像市场规模及预测的分析。</p>\n<p><img src=\"/2024/12/28/digital-human-render-1/%E8%99%9A%E6%8B%9F%E5%81%B6%E5%83%8F%E5%B8%82%E5%9C%BA%E8%A7%84%E6%A8%A1.png\" alt=\"虚拟偶像市场规模\"></p>\n<h1 id=\"数字人的制作流程简介\"><a href=\"#数字人的制作流程简介\" class=\"headerlink\" title=\"数字人的制作流程简介\"></a>数字人的制作流程简介</h1><p>数字人制作大致分4个阶段：</p>\n<ol>\n<li><p>第一阶段（形象设计）：明确形象设计方向。</p>\n</li>\n<li><p>第二阶段（模型制作）：根据平面形象，进行模型搭建。<br>这里我们以可能是最为复杂的超写实数字人的制作流程进行举例，首先在lightstage里面扫描模型（扫描仪，360度单反相机阵列，300多个相机组成）。扫描出来的模型是一个点云，需要模型师去调整，抚平一些瑕疵。去除扫描的毛刺。有些部位可能拍照的时候出现遮挡（比如耳后），需要在模型软件工具中处理好。<br><img src=\"/2024/12/28/digital-human-render-1/%E8%80%81%E9%BB%84%E6%89%AB%E6%8F%8F.png\" alt=\"老黄扫描\"><br><img src=\"/2024/12/28/digital-human-render-1/%E8%80%81%E9%BB%84%E5%BB%BA%E6%A8%A1.png\" alt=\"老黄建模\"></p>\n</li>\n<li><p>第三阶段（驱动绑定）：<br>面部动画face rig绑定驱动，通过动画，人脸识别，或者AI去驱动。或者使用blendshape等技术。<br>身体躯干使用骨骼绑定，辅以动作捕捉等等。<br><img src=\"/2024/12/28/digital-human-render-1/metahuman%E9%9D%A2%E9%83%A8%E9%A9%B1%E5%8A%A8.png\" alt=\"metahuman(UE)面部驱动\"><br><img src=\"/2024/12/28/digital-human-render-1/%E8%80%81%E9%BB%84%E9%A9%B1%E5%8A%A8.png\" alt=\"老黄驱动\"></p>\n</li>\n<li><p>第四阶段（渲染）：将场景、人物放入渲染工具进行渲染输出，常用的工具包括nVidia omniverse、unreal engine等等。<br><img src=\"/2024/12/28/digital-human-render-1/%E8%80%81%E9%BB%84%E6%A8%A1%E5%9E%8B%E6%B8%B2%E6%9F%93.png\" alt=\"老黄模型渲染\"></p>\n</li>\n</ol>\n<p>目前虚幻引擎5的metahuman creator是一个很流程化且易于使用的数字人制作工具。</p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>第一部分先总结到这里，在后面的部分我会更加详细的介绍一些数字人渲染技术，包括皮肤、头发的渲染以及卡通渲染等等。</p>\n","excerpt":"","more":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><p>在2022年12月，我受邀在OGEEK上做过一次关于数字人渲染技术的分享，名为《浅谈数字人仿真的渲染技术》。为了这次分享我查阅了大量资料做了很多准备，但是很不幸的是在分享的前两天我便感染了新冠，身体开始发烧外加喉咙开始隐隐作痛。为了不影响OGEEK的流程我便在病情还未恶化的时候将分享提前录了下来，以播片的形式参加。</p>\n<p>这段经历确实还挺难忘，近期又翻到了这篇准备的PPT，和演讲的录屏，于是想将这部分内容做个记录，整理成文章分享出来。</p>\n<p>由于PPT内容还是比较多的，哪怕是精简后的版本也还有50多页，于是乎会准备做成一个系列，当初精简掉的部分可能也会想办法补充回来，让内容尽量的充分。</p>\n<h1 id=\"数字人简介\"><a href=\"#数字人简介\" class=\"headerlink\" title=\"数字人简介\"></a>数字人简介</h1><h2 id=\"数字人的定义\"><a href=\"#数字人的定义\" class=\"headerlink\" title=\"数字人的定义\"></a>数字人的定义</h2><p>目前数字人缺乏一个统一的标准定义，我们从它的发展起源，从技术角度上选择一个最宽泛最简洁的标准：<strong>由计算机生成的人类。</strong></p>\n<p>中国人工智能产业发展联盟发布的《2020年虚拟数字人发展白皮书》中给了一个更加详细的定义：虚拟数字人意指具有数字化外形的虚拟人物，除了拥有人的<strong>外观</strong>、人的<strong>行为</strong>之外、还有拥有人的<strong>思想</strong>，具有<strong>识别外界环境</strong>、并能与<strong>人交流互动</strong>的能力。</p>\n<p>那其实这个定义里面，也描述了数字人相关的几个关键技术方向，包括：<em>渲染-外观，行为-驱动算法，思想-AI，识别外接环境-感知，与人交流互动-表达</em>。</p>\n<h2 id=\"数字人的发展历史\"><a href=\"#数字人的发展历史\" class=\"headerlink\" title=\"数字人的发展历史\"></a>数字人的发展历史</h2><p>在上世纪80年代，其实就有虚拟形象引入到现实世界的想法。<br>1982 年，动画片《太空堡垒》中的女角色林明美作为虚拟歌姬出道，其专辑也成功打入当时的知名音乐排行榜。日本媒体率先提出了“虚拟偶像”的称号。1984 年，世界首位虚拟电影演员“Max Headroom”诞生，出演电影，并拍摄数支广告，在英国家喻户晓。</p>\n<p><img src=\"/2024/12/28/digital-human-render-1/%E6%9E%97%E6%98%8E%E7%BE%8E.png\" alt=\"林明美\"></p>\n<p>此时，虚拟人概念先行，给予虚拟形象以立体化人设，并带入大众视野。但受制于技术发展，“数字化”在这个阶段并不明显。打造虚拟人的技术以手工绘制为主，人物形象以 2 D 卡通的形式展现，展现方式以事先完成的音频和视频为主，并不具备实时交互功能。</p>\n<p>进入 21 世纪，虚拟人的 数字化特征逐渐明显。形象创建上，虚拟数字人开始从手绘转向 CG和动捕等计算机技术。</p>\n<p>2007 年，日本虚拟歌手“初音未来”的诞生与流行。初音未来的虚拟形象采用 CG 和动作捕捉技术。在动作捕捉技术的助力下，初音未来可以直接采用人类的表情和动作，并借助 CG 技术真实的360度渲染出来。作为虚拟歌姬，初音未来的歌喉基于 VOCALOID（电子音乐制作 的 语音合成软件）。采样于日本声优藤田咲，创作者只需要输入歌词和旋律，就能够自动形成歌曲。</p>\n<p><img src=\"/2024/12/28/digital-human-render-1/%E5%88%9D%E9%9F%B3%E6%9C%AA%E6%9D%A5.png\" alt=\"初音未来\"></p>\n<p>近年来，由于各项技术的不断发展，出现了越来越多高真实度的数字人形象。</p>\n<p>比如说2016年出现的miquela，她在ins上的出现引发了一场“真假辩论”。许多粉丝相信她是真实存在的人物，只是修图“狠”了点。直到黑客们入侵了她的账号，才最终确定了她是由 3 D 电脑动画公司制作的虚拟人。她甚至在2018年一起被美国《时代》周刊列为“25 位最有影响力的互联网人物”</p>\n<p>同样是在2018年，由腾讯、Epic Games推出了Siren项目。Siren 的所有动作表情都由实时捕捉以及实时渲染形成，并且整个过程只有15毫秒，60帧。Siren在渲染的真实性和交互性之间找到平衡，打造了具备实时交互能力的数字虚拟人。</p>\n<p><img src=\"/2024/12/28/digital-human-render-1/siren.png\" alt=\"siren\"></p>\n<h2 id=\"数字人的分类\"><a href=\"#数字人的分类\" class=\"headerlink\" title=\"数字人的分类\"></a>数字人的分类</h2><p>数字人可以按照不同维度进行分类。</p>\n<p>按照美术风格：</p>\n<ul>\n<li>2D、3D</li>\n<li>写实、卡通、风格化</li>\n</ul>\n<p>按照驱动方式：</p>\n<ul>\n<li>真人驱动</li>\n<li>AI驱动</li>\n</ul>\n<p>按照商业和功能维度：</p>\n<ul>\n<li>内容&#x2F;IP型</li>\n<li>功能服务型</li>\n<li>虚拟分身</li>\n</ul>\n<h2 id=\"数字人的发展\"><a href=\"#数字人的发展\" class=\"headerlink\" title=\"数字人的发展\"></a>数字人的发展</h2><p>近几年，虚拟数字人在电商、金融、影视、游戏和金融等行业都拥有不同大小的市场规模。<br>我们拿虚拟偶像的市场作为例子。虚拟偶像行业2020年中国的市场规模为34.6亿元，预计2023年将达到205.2亿元。带动的市场从2020的645.6亿元，预计2023年增长到3334.7亿元，是一个指数级的增长。</p>\n<p>当然除了虚拟偶像数字人还有很多其他方面的应用，所以市场前景是非常可观的。下面是一个虚拟偶像市场规模及预测的分析。</p>\n<p><img src=\"/2024/12/28/digital-human-render-1/%E8%99%9A%E6%8B%9F%E5%81%B6%E5%83%8F%E5%B8%82%E5%9C%BA%E8%A7%84%E6%A8%A1.png\" alt=\"虚拟偶像市场规模\"></p>\n<h1 id=\"数字人的制作流程简介\"><a href=\"#数字人的制作流程简介\" class=\"headerlink\" title=\"数字人的制作流程简介\"></a>数字人的制作流程简介</h1><p>数字人制作大致分4个阶段：</p>\n<ol>\n<li><p>第一阶段（形象设计）：明确形象设计方向。</p>\n</li>\n<li><p>第二阶段（模型制作）：根据平面形象，进行模型搭建。<br>这里我们以可能是最为复杂的超写实数字人的制作流程进行举例，首先在lightstage里面扫描模型（扫描仪，360度单反相机阵列，300多个相机组成）。扫描出来的模型是一个点云，需要模型师去调整，抚平一些瑕疵。去除扫描的毛刺。有些部位可能拍照的时候出现遮挡（比如耳后），需要在模型软件工具中处理好。<br><img src=\"/2024/12/28/digital-human-render-1/%E8%80%81%E9%BB%84%E6%89%AB%E6%8F%8F.png\" alt=\"老黄扫描\"><br><img src=\"/2024/12/28/digital-human-render-1/%E8%80%81%E9%BB%84%E5%BB%BA%E6%A8%A1.png\" alt=\"老黄建模\"></p>\n</li>\n<li><p>第三阶段（驱动绑定）：<br>面部动画face rig绑定驱动，通过动画，人脸识别，或者AI去驱动。或者使用blendshape等技术。<br>身体躯干使用骨骼绑定，辅以动作捕捉等等。<br><img src=\"/2024/12/28/digital-human-render-1/metahuman%E9%9D%A2%E9%83%A8%E9%A9%B1%E5%8A%A8.png\" alt=\"metahuman(UE)面部驱动\"><br><img src=\"/2024/12/28/digital-human-render-1/%E8%80%81%E9%BB%84%E9%A9%B1%E5%8A%A8.png\" alt=\"老黄驱动\"></p>\n</li>\n<li><p>第四阶段（渲染）：将场景、人物放入渲染工具进行渲染输出，常用的工具包括nVidia omniverse、unreal engine等等。<br><img src=\"/2024/12/28/digital-human-render-1/%E8%80%81%E9%BB%84%E6%A8%A1%E5%9E%8B%E6%B8%B2%E6%9F%93.png\" alt=\"老黄模型渲染\"></p>\n</li>\n</ol>\n<p>目前虚幻引擎5的metahuman creator是一个很流程化且易于使用的数字人制作工具。</p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>第一部分先总结到这里，在后面的部分我会更加详细的介绍一些数字人渲染技术，包括皮肤、头发的渲染以及卡通渲染等等。</p>\n"},{"title":"浅谈数字人仿真的渲染技术（二）","date":"2025-01-04T04:19:35.000Z","index_img":"/2024/12/28/digital-human-render-1/metahuman_mugshot.jpg","banner_img":"/2024/12/28/digital-human-render-1/metahuman_mugshot.jpg","_content":"\n# 简介\n新年好。\n这是接着上次关于数字人渲染技术的第二部分，今天的这部分的分享，我会开始介绍一些关于数字人渲染的实际技术。\n\n# 数字人渲染技术介绍\n\n接下来我们来聊一下数字人渲染技术方面的课题，我本身其实在这方面也不是什么大牛，在这里只是把一些我所学到的东西分享给大家。本次也不涉及到过深的技术讨论，如果想要对某个算法的细节想做更深的探讨，我们可以做后续的讨论。\n\n在这里我提前预告一下，接下来的分享会包括哪些方面。\n\n首先是会介绍一些数字人常用的渲染技术，比如皮肤，头发的渲染技术。在介绍这些渲染技术的时候，我主要会解释一下这个算法的构成，他是基于哪些理论而得出的算法，他的流程大致是怎样的，以及效果的一些展示。\n\n本次分享不会包括的方面有：\n\n - 数学公式推导，当今很多渲染的算法都会遵循实际的物理意义，大多包含较为复杂的数学公式的推导，以辅助实现最后的算法。我们今天的分享是浅谈，所以不会讲的那么深，要是专注于数学公式推导的话会花上非常多的时间，门槛也会提升很多，这块并不是今天的目的。\n\n - 另外一个是不会Review相关的Shader代码或者材质蓝图连线，今天主要是希望大家理解好概念就好，代码这些在理解了概念后自己动手去写可以进一步帮助理解。\n\n\n\n## 基于物理的渲染方式（Physically Based Rendering - PBR）\n首先我们来看一下基于物理的渲染方式，也就是我们所说的PBR，这种方式一般是用于渲染写实、高保真类型风格的数字人。\n\n这里我大概介绍一下PBR的概念。PBR的是基于物理的渲染，他的定义是利用真实世界的原理和理论，通过各种数学方法推导或者简化或者模拟出一系列的渲染方程，来输出拟真的画面。\n\n![PBR对比传统渲染方法](pbr_sample.png)\n\n上面两张图是PBR和传统的shader的比较。在PBR出现之前，若想渲染出一张高质量的图，需要机械化的死记各种参数，然后基于烘焙贴图来实现的，并且通常环境光、物体位置必须保持不变。这些缺点在高质量的实时渲染里面显然是不能接受的。\n\n而使用PBR这种渲染方式的话，我们需要分析物体自身物理属性然后给材质设定正确的光照参数，无论物体位置、光照如何改变，都有很好的效果。\n\n但是PBR并不是纯粹的物理渲染，目前PBR还没办法用和现实一模一样的物理规律来实现渲染效果，这其中有硬件条件的限制（GPU，人眼5亿到10亿个像素的信息量），也有知识水平的限制，光照建模没办法达到和现实一模一样，所以在效果和性能上会需要做取舍。\n\n### BRDF\n这里稍微做个补充，这段在原先的PPT中是没有包括的。\n\nPBR的一个很经典的方法就是BRDF模型。\n\n**BRDF**的是双向反射分布函数（Bidirectional Reflectance Distribution Function）的英文缩写。*它从本质上描述了光线如何在物体表面反射，是一个用于量化给定入射方向的光在某个出射方向上的反射比例的函数。*\n\n具体来说，它定义为出射方向的反射辐射率**r0**（radiance）与入射方向的辐照度**r1**（irradiance）的比值，基于入射光方向，和观察（出射）方向。\n\n例如，假设有一束光从某个方向照射到一个物体表面（这是入射方向），我们从另一个方向观察这个物体表面反射出来的光（这是出射方向），BRDF 就可以告诉我们从这个观察方向看到的反射光的强度和特性与入射光的关系。\n\n有很多具体实现BRDF的方法，如*Cook-Torrance模型*、*Disney模型*等等。BRDF很适用于渲染非透明的物体，如墙壁、木头等等，对于人的皮肤，玉石等带有透明的材质则不太合适。这些材质需要用到**次表面散射（BSSRDF）模型**。\n\n### PBR-皮肤\n好了，简单理解一下PBR的一些概念后，我们现在来介绍一下写实风格的数字人的皮肤渲染。\n\n皮肤的渲染一直是渲染领域的难点之一：皮肤具有许多微妙的视觉特征，而观察者对皮肤的外观，特别是脸部的外观会非常敏感（恐怖谷）。皮肤的真实感渲染模型须包括皱纹，毛孔，雀斑等细节，而真实还原人体皮肤上的这些细节则是一个较大的挑战。\n\n![皮肤多层结构](layers_of_skin.png)\n\n皮肤作为一种属性复杂的材质，不同于简单的材质表面比如说水泥墙这些，其物理结构由**多层结构**组成，其表面油脂层主要贡献了皮肤光照的*反射*部分，而油脂层下面的表皮层和真皮层则贡献了的*次表面散射*部分，而且还有一部分光会*透射*过皮肤的边缘或者很薄的地方。\n\n这三个方面组成了皮肤渲染的主要因素，我们今天也着重介绍这三部分的一些计算方法。\n\n#### 镜面反射\n在皮肤渲染中，高光这部分主要是皮肤的油脂层贡献的。高光的算法可以使用基本的**[cook Torrance brdf模型](https://zhuanlan.zhihu.com/p/715918965)**的高光计算部分，因为时间比较紧张，我们就不花时间介绍了。这是最经典PBR算法之一了，如果大家网上搜BRDF就很容易能能够找到。大致思路是高光表现会依据平面的粗糙度，观测角度等不同而不同。如下方图所示。\n\n![Cook-Torrance BRDF](Cook-Torrance-BRDF.png)\n\n但是直接用BRDF计算皮肤高光一般并不能获得最好的效果，因为皮肤是一个复合的表面，他的突出部分和凹陷部分的粗糙度是不一样的。导致有两个粗糙参数，也就有两个高光需要表示。\n\n所以虚幻引擎等某些渲染器会使用一个叫做*双镜叶高光*的技术。**镜叶 lobe**，也是如下图所示，其实就是光在某一个粗糙度平面下的一个分布状态。\n\n![镜叶]](specular-lobe.png)\n\n下面这张图是UE里面默认的高光混合的参数，通过混合两个粗糙度的高光表现，可以达到更贴近人脸皮肤的效果:\n\n![皮肤高光](skin-specular.png)\n\n#### 次表面散射(BSSRDF)\n计算完高光后，我们之前提到，光线接触到皮肤时，有大约94%被皮肤各层散射，只有大约6%被反射。\n我们可以看下对比图，前面我们提到的BRDF，其实主要就是假设光线的反射基于图a的现象，入射点和出射点是同一个，光在这个地方发生漫反射:\n\n![反射对比](diffuse-comparison.png)\n\n但其实光线在进入皮肤后的真实情况是更接近图b的，光线会进入我们的皮肤，通过油脂层到下面的表皮层和真皮层，会进行一阵游走，然后最终有一部分光线会被反射出来。\n\n实际上几乎所有材质都存在次表面散射现象，区别只在于材质散射密度分布的集中程度，如果绝大部分能量都集中在入射点附近，就表示附近像素对当前像素的光照贡献不明显，可以忽略，则在渲染时我们就用漫反射代替，如果该函数分布比较均匀，附近像素对当前像素的光照贡献明显，则需要单独计算次表面散射。\n\n为了模拟这种光线表现，提出了**BSSRDF**。\n\n![BSSRDF](BSSRDF.png)\n\nBSSRDF描述的是，对于当前出射点和出射方向，某个入射点和入射方向的光线能量对其结果的贡献。\n我们观察点是固定的po，已知需要的出射方向，根据这些条件获取周围点对他的光照贡献（w：omega）。\n\n如果我们想要按照真实世界，实时的模拟出每一束光在皮肤材质中的路线，从而获取到每一束光的正确出射点和角度的话，是难道很大的。BSSRDF的意义在于快速的近似真实世界的效果，为了平衡性能和效果，我们假设了4个前提：\n - 物体是一个曲率为0的平面。\n - 平面的厚度和大小都是无限。\n - 内部的介质参数是均匀的。\n - 光线永远是从垂直方向入射表面。\n\n基于这些前提，我们就可以单纯以像素的距离作为权重，距离当前像素近的入射光照，贡献就大，反之距离远的，贡献小。对应的也就是公式上的R(||pi-po||)这部分。\n\n![BSSRDF-1](BSSRDF-1.png)\n\n当然真正人体表面的皮肤是不满足与上面四点的，但是考虑到实时渲染的性能，单纯按照两个点的距离的近似可以达到能接受的效果。\n\n用来描述光在物体内部的散射或者扩散的行为，就是公式中R那个部分，这个分布函数我们叫做*散射剖面（diffusion profile）*，也有叫*扩散剖面*的。\n\n计算散射剖面的算法有很多种，常见的有**偶极子，多极子，高斯和拟合**等等。这里内容比较深，由于时间的关系我们暂时不做详细介绍了。\n\n同时，由于是单纯的根据距离来获得光照的权重，我们可以预处理散射剖面，做成一张lookup table，在实时渲染的时候直接查找对应的值，以加速渲染。\n可以看一下这张图，reflectance（反射率）根据距离的变化，而且rgb三原色是分开计算的.\n\n![反射率参照表](reflectance-lookup-table.png)\n\n\n#### 基于模糊的算法\n\n计算次表面散射的光照的时候，当前像素的光照会受到周边像素的影响，而这个影响的程度我们是以距离来决定的。那其实换个角度想想，这是不是就是我们把原来当前像素的漫反射，抹匀到了周边，因为光的能量经过次表面散射分散到了周边的像素。这其实就是一个模糊操作，从数学角度上，都是做卷积处理。所以就有了基于模糊的皮肤渲染算法。\n\n那么根据施加模糊的空间，分为了*纹理空间模糊*和*屏幕空间模糊*：\n\n##### 纹理空间模糊\n纹理空间模糊他的一般步骤大概是：\n\n 1. 首先需要获得一张拉伸校正贴图，一般是会预计算这张帖图，主要是为了表示每个Texel（纹素）需要进行多大范围的模糊。\n 2. 然后渲染出模型的光照，漫反射，然后将模型的光照展开到纹理空间。\n 3. 将这张图根据拉伸校正贴图所标定的范围，进行模糊处理，保存成一张或者多张纹理贴图。\n 4. 最终渲染的时候，我们会获取依据这些贴图，然后按照某些特定的权重将它们混合，得到最后的漫反射结果\n\n![纹理空间模糊](texture-space-blur.png)\n\n在纹理空间模糊的好处很明显：比较正确，不会穿帮，可以进行低精度的绘制再利用硬件插值来辅助Blur，模糊的方法也很多。\n\n但缺点也很明显：主要是背面也同样要绘制，而且美术需要处理好纹理不然会有接缝问题。\n\n##### 屏幕空间模糊\n\n那么屏幕空间模糊就比较好理解了，就是在屏幕空间对皮肤的光照结果进行模糊。\n![屏幕空间模糊](screen-space-blur.png)\n\n需要注意一下边界问题，不能模糊出界了，处理的时候可以根据深度等作为模板处理。\n下面的图是皮肤在纹理空间和屏幕空间的模糊的效果的不同。\n\n![模糊比较](blur-camparison.png)\n\n\n#### 透射\n\n最后我们简单讲一下透射，透射和次表面散射的区别是透射的光一般是从另外一面照射过来的，而次表面散射我们一般是按照光源和我们的观察点在模型的同一侧。像是我们手指边缘，或者耳垂部分，是比较容易出现这种现象的。\n\n我们一般计算透射的方式的步骤包括三步：\n1. 计算光照在进入半透明介质时的强度\n2. 计算光线在介质中经过的路径\n3. 根据路径长度和BTDF来计算出射光线的强度\n\n那这里我们又要提出一个BXXXDF了，也就是BTDF，其中的T就代表透射。他和BRDF较为类似，只不过光源是从另外一面穿出来的。\n\n![BTDF](BTDF.png)\n\n当然由于光在另外一面穿到前面来，光的强度会有损失，光也会在皮肤出现次表面散射，不过BTDF作为一种近似算法在实时渲染中一般只简化为一个和光线路径长度的函数。\n\n不过一般来说皮肤渲染上透射出现的区域也是比较小的。\n\n### UE里面的皮肤着色模型\n皮肤的渲染算法我们目前就介绍到这里了，当然作为浅谈我们只是稍微触及了一些皮毛，想要做出更好更加真实的皮肤效果还有很多地方可以深入。\n\n![UE皮肤着色模型](ue-skin.png)\n\n这里我大概介绍一下UE，可能很多同学自己做项目也是用的UE。UE的皮肤材质有很多种着色类型，在材质的着色类型可以选择，一般来说皮肤的材质会默认是**次表面轮廓类型**，这是效果最好的着色模型，像是metahuman的皮肤材质着色也是这种类型。\n\n**次表面**就是我们前面讲的基于次表面散射的着色模型，因为次表面散射也可以用作冰川等材质，所以如果选择次表面着色模型且针对皮肤想要有更好的效果的话，需要自己进行调整。\n\n最后**预整合皮肤**也是我们之前提到的优化方法之一，他精度比次表面略低但是性能开销也低。\n\n# 总结\n\n写实风格的皮肤渲染技术就分享到这了，不知不觉文章的长度已经超长了，剩下的部分只能放到后续的文章了。下次的分享的主要内容将会是头发的渲染。\n","source":"_posts/digital-human-render-2.md","raw":"---\ntitle: 浅谈数字人仿真的渲染技术（二）\ndate: 2025-01-04 12:19:35\ncategories: \n\t- 技术漫谈\ntags: [3D, 渲染, 数字孪生]\n\nindex_img: /2024/12/28/digital-human-render-1/metahuman_mugshot.jpg\nbanner_img: /2024/12/28/digital-human-render-1/metahuman_mugshot.jpg\n---\n\n# 简介\n新年好。\n这是接着上次关于数字人渲染技术的第二部分，今天的这部分的分享，我会开始介绍一些关于数字人渲染的实际技术。\n\n# 数字人渲染技术介绍\n\n接下来我们来聊一下数字人渲染技术方面的课题，我本身其实在这方面也不是什么大牛，在这里只是把一些我所学到的东西分享给大家。本次也不涉及到过深的技术讨论，如果想要对某个算法的细节想做更深的探讨，我们可以做后续的讨论。\n\n在这里我提前预告一下，接下来的分享会包括哪些方面。\n\n首先是会介绍一些数字人常用的渲染技术，比如皮肤，头发的渲染技术。在介绍这些渲染技术的时候，我主要会解释一下这个算法的构成，他是基于哪些理论而得出的算法，他的流程大致是怎样的，以及效果的一些展示。\n\n本次分享不会包括的方面有：\n\n - 数学公式推导，当今很多渲染的算法都会遵循实际的物理意义，大多包含较为复杂的数学公式的推导，以辅助实现最后的算法。我们今天的分享是浅谈，所以不会讲的那么深，要是专注于数学公式推导的话会花上非常多的时间，门槛也会提升很多，这块并不是今天的目的。\n\n - 另外一个是不会Review相关的Shader代码或者材质蓝图连线，今天主要是希望大家理解好概念就好，代码这些在理解了概念后自己动手去写可以进一步帮助理解。\n\n\n\n## 基于物理的渲染方式（Physically Based Rendering - PBR）\n首先我们来看一下基于物理的渲染方式，也就是我们所说的PBR，这种方式一般是用于渲染写实、高保真类型风格的数字人。\n\n这里我大概介绍一下PBR的概念。PBR的是基于物理的渲染，他的定义是利用真实世界的原理和理论，通过各种数学方法推导或者简化或者模拟出一系列的渲染方程，来输出拟真的画面。\n\n![PBR对比传统渲染方法](pbr_sample.png)\n\n上面两张图是PBR和传统的shader的比较。在PBR出现之前，若想渲染出一张高质量的图，需要机械化的死记各种参数，然后基于烘焙贴图来实现的，并且通常环境光、物体位置必须保持不变。这些缺点在高质量的实时渲染里面显然是不能接受的。\n\n而使用PBR这种渲染方式的话，我们需要分析物体自身物理属性然后给材质设定正确的光照参数，无论物体位置、光照如何改变，都有很好的效果。\n\n但是PBR并不是纯粹的物理渲染，目前PBR还没办法用和现实一模一样的物理规律来实现渲染效果，这其中有硬件条件的限制（GPU，人眼5亿到10亿个像素的信息量），也有知识水平的限制，光照建模没办法达到和现实一模一样，所以在效果和性能上会需要做取舍。\n\n### BRDF\n这里稍微做个补充，这段在原先的PPT中是没有包括的。\n\nPBR的一个很经典的方法就是BRDF模型。\n\n**BRDF**的是双向反射分布函数（Bidirectional Reflectance Distribution Function）的英文缩写。*它从本质上描述了光线如何在物体表面反射，是一个用于量化给定入射方向的光在某个出射方向上的反射比例的函数。*\n\n具体来说，它定义为出射方向的反射辐射率**r0**（radiance）与入射方向的辐照度**r1**（irradiance）的比值，基于入射光方向，和观察（出射）方向。\n\n例如，假设有一束光从某个方向照射到一个物体表面（这是入射方向），我们从另一个方向观察这个物体表面反射出来的光（这是出射方向），BRDF 就可以告诉我们从这个观察方向看到的反射光的强度和特性与入射光的关系。\n\n有很多具体实现BRDF的方法，如*Cook-Torrance模型*、*Disney模型*等等。BRDF很适用于渲染非透明的物体，如墙壁、木头等等，对于人的皮肤，玉石等带有透明的材质则不太合适。这些材质需要用到**次表面散射（BSSRDF）模型**。\n\n### PBR-皮肤\n好了，简单理解一下PBR的一些概念后，我们现在来介绍一下写实风格的数字人的皮肤渲染。\n\n皮肤的渲染一直是渲染领域的难点之一：皮肤具有许多微妙的视觉特征，而观察者对皮肤的外观，特别是脸部的外观会非常敏感（恐怖谷）。皮肤的真实感渲染模型须包括皱纹，毛孔，雀斑等细节，而真实还原人体皮肤上的这些细节则是一个较大的挑战。\n\n![皮肤多层结构](layers_of_skin.png)\n\n皮肤作为一种属性复杂的材质，不同于简单的材质表面比如说水泥墙这些，其物理结构由**多层结构**组成，其表面油脂层主要贡献了皮肤光照的*反射*部分，而油脂层下面的表皮层和真皮层则贡献了的*次表面散射*部分，而且还有一部分光会*透射*过皮肤的边缘或者很薄的地方。\n\n这三个方面组成了皮肤渲染的主要因素，我们今天也着重介绍这三部分的一些计算方法。\n\n#### 镜面反射\n在皮肤渲染中，高光这部分主要是皮肤的油脂层贡献的。高光的算法可以使用基本的**[cook Torrance brdf模型](https://zhuanlan.zhihu.com/p/715918965)**的高光计算部分，因为时间比较紧张，我们就不花时间介绍了。这是最经典PBR算法之一了，如果大家网上搜BRDF就很容易能能够找到。大致思路是高光表现会依据平面的粗糙度，观测角度等不同而不同。如下方图所示。\n\n![Cook-Torrance BRDF](Cook-Torrance-BRDF.png)\n\n但是直接用BRDF计算皮肤高光一般并不能获得最好的效果，因为皮肤是一个复合的表面，他的突出部分和凹陷部分的粗糙度是不一样的。导致有两个粗糙参数，也就有两个高光需要表示。\n\n所以虚幻引擎等某些渲染器会使用一个叫做*双镜叶高光*的技术。**镜叶 lobe**，也是如下图所示，其实就是光在某一个粗糙度平面下的一个分布状态。\n\n![镜叶]](specular-lobe.png)\n\n下面这张图是UE里面默认的高光混合的参数，通过混合两个粗糙度的高光表现，可以达到更贴近人脸皮肤的效果:\n\n![皮肤高光](skin-specular.png)\n\n#### 次表面散射(BSSRDF)\n计算完高光后，我们之前提到，光线接触到皮肤时，有大约94%被皮肤各层散射，只有大约6%被反射。\n我们可以看下对比图，前面我们提到的BRDF，其实主要就是假设光线的反射基于图a的现象，入射点和出射点是同一个，光在这个地方发生漫反射:\n\n![反射对比](diffuse-comparison.png)\n\n但其实光线在进入皮肤后的真实情况是更接近图b的，光线会进入我们的皮肤，通过油脂层到下面的表皮层和真皮层，会进行一阵游走，然后最终有一部分光线会被反射出来。\n\n实际上几乎所有材质都存在次表面散射现象，区别只在于材质散射密度分布的集中程度，如果绝大部分能量都集中在入射点附近，就表示附近像素对当前像素的光照贡献不明显，可以忽略，则在渲染时我们就用漫反射代替，如果该函数分布比较均匀，附近像素对当前像素的光照贡献明显，则需要单独计算次表面散射。\n\n为了模拟这种光线表现，提出了**BSSRDF**。\n\n![BSSRDF](BSSRDF.png)\n\nBSSRDF描述的是，对于当前出射点和出射方向，某个入射点和入射方向的光线能量对其结果的贡献。\n我们观察点是固定的po，已知需要的出射方向，根据这些条件获取周围点对他的光照贡献（w：omega）。\n\n如果我们想要按照真实世界，实时的模拟出每一束光在皮肤材质中的路线，从而获取到每一束光的正确出射点和角度的话，是难道很大的。BSSRDF的意义在于快速的近似真实世界的效果，为了平衡性能和效果，我们假设了4个前提：\n - 物体是一个曲率为0的平面。\n - 平面的厚度和大小都是无限。\n - 内部的介质参数是均匀的。\n - 光线永远是从垂直方向入射表面。\n\n基于这些前提，我们就可以单纯以像素的距离作为权重，距离当前像素近的入射光照，贡献就大，反之距离远的，贡献小。对应的也就是公式上的R(||pi-po||)这部分。\n\n![BSSRDF-1](BSSRDF-1.png)\n\n当然真正人体表面的皮肤是不满足与上面四点的，但是考虑到实时渲染的性能，单纯按照两个点的距离的近似可以达到能接受的效果。\n\n用来描述光在物体内部的散射或者扩散的行为，就是公式中R那个部分，这个分布函数我们叫做*散射剖面（diffusion profile）*，也有叫*扩散剖面*的。\n\n计算散射剖面的算法有很多种，常见的有**偶极子，多极子，高斯和拟合**等等。这里内容比较深，由于时间的关系我们暂时不做详细介绍了。\n\n同时，由于是单纯的根据距离来获得光照的权重，我们可以预处理散射剖面，做成一张lookup table，在实时渲染的时候直接查找对应的值，以加速渲染。\n可以看一下这张图，reflectance（反射率）根据距离的变化，而且rgb三原色是分开计算的.\n\n![反射率参照表](reflectance-lookup-table.png)\n\n\n#### 基于模糊的算法\n\n计算次表面散射的光照的时候，当前像素的光照会受到周边像素的影响，而这个影响的程度我们是以距离来决定的。那其实换个角度想想，这是不是就是我们把原来当前像素的漫反射，抹匀到了周边，因为光的能量经过次表面散射分散到了周边的像素。这其实就是一个模糊操作，从数学角度上，都是做卷积处理。所以就有了基于模糊的皮肤渲染算法。\n\n那么根据施加模糊的空间，分为了*纹理空间模糊*和*屏幕空间模糊*：\n\n##### 纹理空间模糊\n纹理空间模糊他的一般步骤大概是：\n\n 1. 首先需要获得一张拉伸校正贴图，一般是会预计算这张帖图，主要是为了表示每个Texel（纹素）需要进行多大范围的模糊。\n 2. 然后渲染出模型的光照，漫反射，然后将模型的光照展开到纹理空间。\n 3. 将这张图根据拉伸校正贴图所标定的范围，进行模糊处理，保存成一张或者多张纹理贴图。\n 4. 最终渲染的时候，我们会获取依据这些贴图，然后按照某些特定的权重将它们混合，得到最后的漫反射结果\n\n![纹理空间模糊](texture-space-blur.png)\n\n在纹理空间模糊的好处很明显：比较正确，不会穿帮，可以进行低精度的绘制再利用硬件插值来辅助Blur，模糊的方法也很多。\n\n但缺点也很明显：主要是背面也同样要绘制，而且美术需要处理好纹理不然会有接缝问题。\n\n##### 屏幕空间模糊\n\n那么屏幕空间模糊就比较好理解了，就是在屏幕空间对皮肤的光照结果进行模糊。\n![屏幕空间模糊](screen-space-blur.png)\n\n需要注意一下边界问题，不能模糊出界了，处理的时候可以根据深度等作为模板处理。\n下面的图是皮肤在纹理空间和屏幕空间的模糊的效果的不同。\n\n![模糊比较](blur-camparison.png)\n\n\n#### 透射\n\n最后我们简单讲一下透射，透射和次表面散射的区别是透射的光一般是从另外一面照射过来的，而次表面散射我们一般是按照光源和我们的观察点在模型的同一侧。像是我们手指边缘，或者耳垂部分，是比较容易出现这种现象的。\n\n我们一般计算透射的方式的步骤包括三步：\n1. 计算光照在进入半透明介质时的强度\n2. 计算光线在介质中经过的路径\n3. 根据路径长度和BTDF来计算出射光线的强度\n\n那这里我们又要提出一个BXXXDF了，也就是BTDF，其中的T就代表透射。他和BRDF较为类似，只不过光源是从另外一面穿出来的。\n\n![BTDF](BTDF.png)\n\n当然由于光在另外一面穿到前面来，光的强度会有损失，光也会在皮肤出现次表面散射，不过BTDF作为一种近似算法在实时渲染中一般只简化为一个和光线路径长度的函数。\n\n不过一般来说皮肤渲染上透射出现的区域也是比较小的。\n\n### UE里面的皮肤着色模型\n皮肤的渲染算法我们目前就介绍到这里了，当然作为浅谈我们只是稍微触及了一些皮毛，想要做出更好更加真实的皮肤效果还有很多地方可以深入。\n\n![UE皮肤着色模型](ue-skin.png)\n\n这里我大概介绍一下UE，可能很多同学自己做项目也是用的UE。UE的皮肤材质有很多种着色类型，在材质的着色类型可以选择，一般来说皮肤的材质会默认是**次表面轮廓类型**，这是效果最好的着色模型，像是metahuman的皮肤材质着色也是这种类型。\n\n**次表面**就是我们前面讲的基于次表面散射的着色模型，因为次表面散射也可以用作冰川等材质，所以如果选择次表面着色模型且针对皮肤想要有更好的效果的话，需要自己进行调整。\n\n最后**预整合皮肤**也是我们之前提到的优化方法之一，他精度比次表面略低但是性能开销也低。\n\n# 总结\n\n写实风格的皮肤渲染技术就分享到这了，不知不觉文章的长度已经超长了，剩下的部分只能放到后续的文章了。下次的分享的主要内容将会是头发的渲染。\n","slug":"digital-human-render-2","published":1,"updated":"2025-01-04T06:33:06.557Z","_id":"cm5ht44vi000b34574gl60tn2","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>新年好。<br>这是接着上次关于数字人渲染技术的第二部分，今天的这部分的分享，我会开始介绍一些关于数字人渲染的实际技术。</p>\n<h1 id=\"数字人渲染技术介绍\"><a href=\"#数字人渲染技术介绍\" class=\"headerlink\" title=\"数字人渲染技术介绍\"></a>数字人渲染技术介绍</h1><p>接下来我们来聊一下数字人渲染技术方面的课题，我本身其实在这方面也不是什么大牛，在这里只是把一些我所学到的东西分享给大家。本次也不涉及到过深的技术讨论，如果想要对某个算法的细节想做更深的探讨，我们可以做后续的讨论。</p>\n<p>在这里我提前预告一下，接下来的分享会包括哪些方面。</p>\n<p>首先是会介绍一些数字人常用的渲染技术，比如皮肤，头发的渲染技术。在介绍这些渲染技术的时候，我主要会解释一下这个算法的构成，他是基于哪些理论而得出的算法，他的流程大致是怎样的，以及效果的一些展示。</p>\n<p>本次分享不会包括的方面有：</p>\n<ul>\n<li><p>数学公式推导，当今很多渲染的算法都会遵循实际的物理意义，大多包含较为复杂的数学公式的推导，以辅助实现最后的算法。我们今天的分享是浅谈，所以不会讲的那么深，要是专注于数学公式推导的话会花上非常多的时间，门槛也会提升很多，这块并不是今天的目的。</p>\n</li>\n<li><p>另外一个是不会Review相关的Shader代码或者材质蓝图连线，今天主要是希望大家理解好概念就好，代码这些在理解了概念后自己动手去写可以进一步帮助理解。</p>\n</li>\n</ul>\n<h2 id=\"基于物理的渲染方式（Physically-Based-Rendering-PBR）\"><a href=\"#基于物理的渲染方式（Physically-Based-Rendering-PBR）\" class=\"headerlink\" title=\"基于物理的渲染方式（Physically Based Rendering - PBR）\"></a>基于物理的渲染方式（Physically Based Rendering - PBR）</h2><p>首先我们来看一下基于物理的渲染方式，也就是我们所说的PBR，这种方式一般是用于渲染写实、高保真类型风格的数字人。</p>\n<p>这里我大概介绍一下PBR的概念。PBR的是基于物理的渲染，他的定义是利用真实世界的原理和理论，通过各种数学方法推导或者简化或者模拟出一系列的渲染方程，来输出拟真的画面。</p>\n<p><img src=\"/2025/01/04/digital-human-render-2/pbr_sample.png\" alt=\"PBR对比传统渲染方法\"></p>\n<p>上面两张图是PBR和传统的shader的比较。在PBR出现之前，若想渲染出一张高质量的图，需要机械化的死记各种参数，然后基于烘焙贴图来实现的，并且通常环境光、物体位置必须保持不变。这些缺点在高质量的实时渲染里面显然是不能接受的。</p>\n<p>而使用PBR这种渲染方式的话，我们需要分析物体自身物理属性然后给材质设定正确的光照参数，无论物体位置、光照如何改变，都有很好的效果。</p>\n<p>但是PBR并不是纯粹的物理渲染，目前PBR还没办法用和现实一模一样的物理规律来实现渲染效果，这其中有硬件条件的限制（GPU，人眼5亿到10亿个像素的信息量），也有知识水平的限制，光照建模没办法达到和现实一模一样，所以在效果和性能上会需要做取舍。</p>\n<h3 id=\"BRDF\"><a href=\"#BRDF\" class=\"headerlink\" title=\"BRDF\"></a>BRDF</h3><p>这里稍微做个补充，这段在原先的PPT中是没有包括的。</p>\n<p>PBR的一个很经典的方法就是BRDF模型。</p>\n<p><strong>BRDF</strong>的是双向反射分布函数（Bidirectional Reflectance Distribution Function）的英文缩写。<em>它从本质上描述了光线如何在物体表面反射，是一个用于量化给定入射方向的光在某个出射方向上的反射比例的函数。</em></p>\n<p>具体来说，它定义为出射方向的反射辐射率<strong>r0</strong>（radiance）与入射方向的辐照度<strong>r1</strong>（irradiance）的比值，基于入射光方向，和观察（出射）方向。</p>\n<p>例如，假设有一束光从某个方向照射到一个物体表面（这是入射方向），我们从另一个方向观察这个物体表面反射出来的光（这是出射方向），BRDF 就可以告诉我们从这个观察方向看到的反射光的强度和特性与入射光的关系。</p>\n<p>有很多具体实现BRDF的方法，如<em>Cook-Torrance模型</em>、<em>Disney模型</em>等等。BRDF很适用于渲染非透明的物体，如墙壁、木头等等，对于人的皮肤，玉石等带有透明的材质则不太合适。这些材质需要用到<strong>次表面散射（BSSRDF）模型</strong>。</p>\n<h3 id=\"PBR-皮肤\"><a href=\"#PBR-皮肤\" class=\"headerlink\" title=\"PBR-皮肤\"></a>PBR-皮肤</h3><p>好了，简单理解一下PBR的一些概念后，我们现在来介绍一下写实风格的数字人的皮肤渲染。</p>\n<p>皮肤的渲染一直是渲染领域的难点之一：皮肤具有许多微妙的视觉特征，而观察者对皮肤的外观，特别是脸部的外观会非常敏感（恐怖谷）。皮肤的真实感渲染模型须包括皱纹，毛孔，雀斑等细节，而真实还原人体皮肤上的这些细节则是一个较大的挑战。</p>\n<p><img src=\"/2025/01/04/digital-human-render-2/layers_of_skin.png\" alt=\"皮肤多层结构\"></p>\n<p>皮肤作为一种属性复杂的材质，不同于简单的材质表面比如说水泥墙这些，其物理结构由<strong>多层结构</strong>组成，其表面油脂层主要贡献了皮肤光照的<em>反射</em>部分，而油脂层下面的表皮层和真皮层则贡献了的<em>次表面散射</em>部分，而且还有一部分光会<em>透射</em>过皮肤的边缘或者很薄的地方。</p>\n<p>这三个方面组成了皮肤渲染的主要因素，我们今天也着重介绍这三部分的一些计算方法。</p>\n<h4 id=\"镜面反射\"><a href=\"#镜面反射\" class=\"headerlink\" title=\"镜面反射\"></a>镜面反射</h4><p>在皮肤渲染中，高光这部分主要是皮肤的油脂层贡献的。高光的算法可以使用基本的**<a href=\"https://zhuanlan.zhihu.com/p/715918965\">cook Torrance brdf模型</a>**的高光计算部分，因为时间比较紧张，我们就不花时间介绍了。这是最经典PBR算法之一了，如果大家网上搜BRDF就很容易能能够找到。大致思路是高光表现会依据平面的粗糙度，观测角度等不同而不同。如下方图所示。</p>\n<p><img src=\"/2025/01/04/digital-human-render-2/Cook-Torrance-BRDF.png\" alt=\"Cook-Torrance BRDF\"></p>\n<p>但是直接用BRDF计算皮肤高光一般并不能获得最好的效果，因为皮肤是一个复合的表面，他的突出部分和凹陷部分的粗糙度是不一样的。导致有两个粗糙参数，也就有两个高光需要表示。</p>\n<p>所以虚幻引擎等某些渲染器会使用一个叫做<em>双镜叶高光</em>的技术。<strong>镜叶 lobe</strong>，也是如下图所示，其实就是光在某一个粗糙度平面下的一个分布状态。</p>\n<p>![镜叶]](specular-lobe.png)</p>\n<p>下面这张图是UE里面默认的高光混合的参数，通过混合两个粗糙度的高光表现，可以达到更贴近人脸皮肤的效果:</p>\n<p><img src=\"/2025/01/04/digital-human-render-2/skin-specular.png\" alt=\"皮肤高光\"></p>\n<h4 id=\"次表面散射-BSSRDF\"><a href=\"#次表面散射-BSSRDF\" class=\"headerlink\" title=\"次表面散射(BSSRDF)\"></a>次表面散射(BSSRDF)</h4><p>计算完高光后，我们之前提到，光线接触到皮肤时，有大约94%被皮肤各层散射，只有大约6%被反射。<br>我们可以看下对比图，前面我们提到的BRDF，其实主要就是假设光线的反射基于图a的现象，入射点和出射点是同一个，光在这个地方发生漫反射:</p>\n<p><img src=\"/2025/01/04/digital-human-render-2/diffuse-comparison.png\" alt=\"反射对比\"></p>\n<p>但其实光线在进入皮肤后的真实情况是更接近图b的，光线会进入我们的皮肤，通过油脂层到下面的表皮层和真皮层，会进行一阵游走，然后最终有一部分光线会被反射出来。</p>\n<p>实际上几乎所有材质都存在次表面散射现象，区别只在于材质散射密度分布的集中程度，如果绝大部分能量都集中在入射点附近，就表示附近像素对当前像素的光照贡献不明显，可以忽略，则在渲染时我们就用漫反射代替，如果该函数分布比较均匀，附近像素对当前像素的光照贡献明显，则需要单独计算次表面散射。</p>\n<p>为了模拟这种光线表现，提出了<strong>BSSRDF</strong>。</p>\n<p><img src=\"/2025/01/04/digital-human-render-2/BSSRDF.png\" alt=\"BSSRDF\"></p>\n<p>BSSRDF描述的是，对于当前出射点和出射方向，某个入射点和入射方向的光线能量对其结果的贡献。<br>我们观察点是固定的po，已知需要的出射方向，根据这些条件获取周围点对他的光照贡献（w：omega）。</p>\n<p>如果我们想要按照真实世界，实时的模拟出每一束光在皮肤材质中的路线，从而获取到每一束光的正确出射点和角度的话，是难道很大的。BSSRDF的意义在于快速的近似真实世界的效果，为了平衡性能和效果，我们假设了4个前提：</p>\n<ul>\n<li>物体是一个曲率为0的平面。</li>\n<li>平面的厚度和大小都是无限。</li>\n<li>内部的介质参数是均匀的。</li>\n<li>光线永远是从垂直方向入射表面。</li>\n</ul>\n<p>基于这些前提，我们就可以单纯以像素的距离作为权重，距离当前像素近的入射光照，贡献就大，反之距离远的，贡献小。对应的也就是公式上的R(||pi-po||)这部分。</p>\n<p><img src=\"/2025/01/04/digital-human-render-2/BSSRDF-1.png\" alt=\"BSSRDF-1\"></p>\n<p>当然真正人体表面的皮肤是不满足与上面四点的，但是考虑到实时渲染的性能，单纯按照两个点的距离的近似可以达到能接受的效果。</p>\n<p>用来描述光在物体内部的散射或者扩散的行为，就是公式中R那个部分，这个分布函数我们叫做<em>散射剖面（diffusion profile）</em>，也有叫<em>扩散剖面</em>的。</p>\n<p>计算散射剖面的算法有很多种，常见的有<strong>偶极子，多极子，高斯和拟合</strong>等等。这里内容比较深，由于时间的关系我们暂时不做详细介绍了。</p>\n<p>同时，由于是单纯的根据距离来获得光照的权重，我们可以预处理散射剖面，做成一张lookup table，在实时渲染的时候直接查找对应的值，以加速渲染。<br>可以看一下这张图，reflectance（反射率）根据距离的变化，而且rgb三原色是分开计算的.</p>\n<p><img src=\"/2025/01/04/digital-human-render-2/reflectance-lookup-table.png\" alt=\"反射率参照表\"></p>\n<h4 id=\"基于模糊的算法\"><a href=\"#基于模糊的算法\" class=\"headerlink\" title=\"基于模糊的算法\"></a>基于模糊的算法</h4><p>计算次表面散射的光照的时候，当前像素的光照会受到周边像素的影响，而这个影响的程度我们是以距离来决定的。那其实换个角度想想，这是不是就是我们把原来当前像素的漫反射，抹匀到了周边，因为光的能量经过次表面散射分散到了周边的像素。这其实就是一个模糊操作，从数学角度上，都是做卷积处理。所以就有了基于模糊的皮肤渲染算法。</p>\n<p>那么根据施加模糊的空间，分为了<em>纹理空间模糊</em>和<em>屏幕空间模糊</em>：</p>\n<h5 id=\"纹理空间模糊\"><a href=\"#纹理空间模糊\" class=\"headerlink\" title=\"纹理空间模糊\"></a>纹理空间模糊</h5><p>纹理空间模糊他的一般步骤大概是：</p>\n<ol>\n<li>首先需要获得一张拉伸校正贴图，一般是会预计算这张帖图，主要是为了表示每个Texel（纹素）需要进行多大范围的模糊。</li>\n<li>然后渲染出模型的光照，漫反射，然后将模型的光照展开到纹理空间。</li>\n<li>将这张图根据拉伸校正贴图所标定的范围，进行模糊处理，保存成一张或者多张纹理贴图。</li>\n<li>最终渲染的时候，我们会获取依据这些贴图，然后按照某些特定的权重将它们混合，得到最后的漫反射结果</li>\n</ol>\n<p><img src=\"/2025/01/04/digital-human-render-2/texture-space-blur.png\" alt=\"纹理空间模糊\"></p>\n<p>在纹理空间模糊的好处很明显：比较正确，不会穿帮，可以进行低精度的绘制再利用硬件插值来辅助Blur，模糊的方法也很多。</p>\n<p>但缺点也很明显：主要是背面也同样要绘制，而且美术需要处理好纹理不然会有接缝问题。</p>\n<h5 id=\"屏幕空间模糊\"><a href=\"#屏幕空间模糊\" class=\"headerlink\" title=\"屏幕空间模糊\"></a>屏幕空间模糊</h5><p>那么屏幕空间模糊就比较好理解了，就是在屏幕空间对皮肤的光照结果进行模糊。<br><img src=\"/2025/01/04/digital-human-render-2/screen-space-blur.png\" alt=\"屏幕空间模糊\"></p>\n<p>需要注意一下边界问题，不能模糊出界了，处理的时候可以根据深度等作为模板处理。<br>下面的图是皮肤在纹理空间和屏幕空间的模糊的效果的不同。</p>\n<p><img src=\"/2025/01/04/digital-human-render-2/blur-camparison.png\" alt=\"模糊比较\"></p>\n<h4 id=\"透射\"><a href=\"#透射\" class=\"headerlink\" title=\"透射\"></a>透射</h4><p>最后我们简单讲一下透射，透射和次表面散射的区别是透射的光一般是从另外一面照射过来的，而次表面散射我们一般是按照光源和我们的观察点在模型的同一侧。像是我们手指边缘，或者耳垂部分，是比较容易出现这种现象的。</p>\n<p>我们一般计算透射的方式的步骤包括三步：</p>\n<ol>\n<li>计算光照在进入半透明介质时的强度</li>\n<li>计算光线在介质中经过的路径</li>\n<li>根据路径长度和BTDF来计算出射光线的强度</li>\n</ol>\n<p>那这里我们又要提出一个BXXXDF了，也就是BTDF，其中的T就代表透射。他和BRDF较为类似，只不过光源是从另外一面穿出来的。</p>\n<p><img src=\"/2025/01/04/digital-human-render-2/BTDF.png\" alt=\"BTDF\"></p>\n<p>当然由于光在另外一面穿到前面来，光的强度会有损失，光也会在皮肤出现次表面散射，不过BTDF作为一种近似算法在实时渲染中一般只简化为一个和光线路径长度的函数。</p>\n<p>不过一般来说皮肤渲染上透射出现的区域也是比较小的。</p>\n<h3 id=\"UE里面的皮肤着色模型\"><a href=\"#UE里面的皮肤着色模型\" class=\"headerlink\" title=\"UE里面的皮肤着色模型\"></a>UE里面的皮肤着色模型</h3><p>皮肤的渲染算法我们目前就介绍到这里了，当然作为浅谈我们只是稍微触及了一些皮毛，想要做出更好更加真实的皮肤效果还有很多地方可以深入。</p>\n<p><img src=\"/2025/01/04/digital-human-render-2/ue-skin.png\" alt=\"UE皮肤着色模型\"></p>\n<p>这里我大概介绍一下UE，可能很多同学自己做项目也是用的UE。UE的皮肤材质有很多种着色类型，在材质的着色类型可以选择，一般来说皮肤的材质会默认是<strong>次表面轮廓类型</strong>，这是效果最好的着色模型，像是metahuman的皮肤材质着色也是这种类型。</p>\n<p><strong>次表面</strong>就是我们前面讲的基于次表面散射的着色模型，因为次表面散射也可以用作冰川等材质，所以如果选择次表面着色模型且针对皮肤想要有更好的效果的话，需要自己进行调整。</p>\n<p>最后<strong>预整合皮肤</strong>也是我们之前提到的优化方法之一，他精度比次表面略低但是性能开销也低。</p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>写实风格的皮肤渲染技术就分享到这了，不知不觉文章的长度已经超长了，剩下的部分只能放到后续的文章了。下次的分享的主要内容将会是头发的渲染。</p>\n","excerpt":"","more":"<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>新年好。<br>这是接着上次关于数字人渲染技术的第二部分，今天的这部分的分享，我会开始介绍一些关于数字人渲染的实际技术。</p>\n<h1 id=\"数字人渲染技术介绍\"><a href=\"#数字人渲染技术介绍\" class=\"headerlink\" title=\"数字人渲染技术介绍\"></a>数字人渲染技术介绍</h1><p>接下来我们来聊一下数字人渲染技术方面的课题，我本身其实在这方面也不是什么大牛，在这里只是把一些我所学到的东西分享给大家。本次也不涉及到过深的技术讨论，如果想要对某个算法的细节想做更深的探讨，我们可以做后续的讨论。</p>\n<p>在这里我提前预告一下，接下来的分享会包括哪些方面。</p>\n<p>首先是会介绍一些数字人常用的渲染技术，比如皮肤，头发的渲染技术。在介绍这些渲染技术的时候，我主要会解释一下这个算法的构成，他是基于哪些理论而得出的算法，他的流程大致是怎样的，以及效果的一些展示。</p>\n<p>本次分享不会包括的方面有：</p>\n<ul>\n<li><p>数学公式推导，当今很多渲染的算法都会遵循实际的物理意义，大多包含较为复杂的数学公式的推导，以辅助实现最后的算法。我们今天的分享是浅谈，所以不会讲的那么深，要是专注于数学公式推导的话会花上非常多的时间，门槛也会提升很多，这块并不是今天的目的。</p>\n</li>\n<li><p>另外一个是不会Review相关的Shader代码或者材质蓝图连线，今天主要是希望大家理解好概念就好，代码这些在理解了概念后自己动手去写可以进一步帮助理解。</p>\n</li>\n</ul>\n<h2 id=\"基于物理的渲染方式（Physically-Based-Rendering-PBR）\"><a href=\"#基于物理的渲染方式（Physically-Based-Rendering-PBR）\" class=\"headerlink\" title=\"基于物理的渲染方式（Physically Based Rendering - PBR）\"></a>基于物理的渲染方式（Physically Based Rendering - PBR）</h2><p>首先我们来看一下基于物理的渲染方式，也就是我们所说的PBR，这种方式一般是用于渲染写实、高保真类型风格的数字人。</p>\n<p>这里我大概介绍一下PBR的概念。PBR的是基于物理的渲染，他的定义是利用真实世界的原理和理论，通过各种数学方法推导或者简化或者模拟出一系列的渲染方程，来输出拟真的画面。</p>\n<p><img src=\"/2025/01/04/digital-human-render-2/pbr_sample.png\" alt=\"PBR对比传统渲染方法\"></p>\n<p>上面两张图是PBR和传统的shader的比较。在PBR出现之前，若想渲染出一张高质量的图，需要机械化的死记各种参数，然后基于烘焙贴图来实现的，并且通常环境光、物体位置必须保持不变。这些缺点在高质量的实时渲染里面显然是不能接受的。</p>\n<p>而使用PBR这种渲染方式的话，我们需要分析物体自身物理属性然后给材质设定正确的光照参数，无论物体位置、光照如何改变，都有很好的效果。</p>\n<p>但是PBR并不是纯粹的物理渲染，目前PBR还没办法用和现实一模一样的物理规律来实现渲染效果，这其中有硬件条件的限制（GPU，人眼5亿到10亿个像素的信息量），也有知识水平的限制，光照建模没办法达到和现实一模一样，所以在效果和性能上会需要做取舍。</p>\n<h3 id=\"BRDF\"><a href=\"#BRDF\" class=\"headerlink\" title=\"BRDF\"></a>BRDF</h3><p>这里稍微做个补充，这段在原先的PPT中是没有包括的。</p>\n<p>PBR的一个很经典的方法就是BRDF模型。</p>\n<p><strong>BRDF</strong>的是双向反射分布函数（Bidirectional Reflectance Distribution Function）的英文缩写。<em>它从本质上描述了光线如何在物体表面反射，是一个用于量化给定入射方向的光在某个出射方向上的反射比例的函数。</em></p>\n<p>具体来说，它定义为出射方向的反射辐射率<strong>r0</strong>（radiance）与入射方向的辐照度<strong>r1</strong>（irradiance）的比值，基于入射光方向，和观察（出射）方向。</p>\n<p>例如，假设有一束光从某个方向照射到一个物体表面（这是入射方向），我们从另一个方向观察这个物体表面反射出来的光（这是出射方向），BRDF 就可以告诉我们从这个观察方向看到的反射光的强度和特性与入射光的关系。</p>\n<p>有很多具体实现BRDF的方法，如<em>Cook-Torrance模型</em>、<em>Disney模型</em>等等。BRDF很适用于渲染非透明的物体，如墙壁、木头等等，对于人的皮肤，玉石等带有透明的材质则不太合适。这些材质需要用到<strong>次表面散射（BSSRDF）模型</strong>。</p>\n<h3 id=\"PBR-皮肤\"><a href=\"#PBR-皮肤\" class=\"headerlink\" title=\"PBR-皮肤\"></a>PBR-皮肤</h3><p>好了，简单理解一下PBR的一些概念后，我们现在来介绍一下写实风格的数字人的皮肤渲染。</p>\n<p>皮肤的渲染一直是渲染领域的难点之一：皮肤具有许多微妙的视觉特征，而观察者对皮肤的外观，特别是脸部的外观会非常敏感（恐怖谷）。皮肤的真实感渲染模型须包括皱纹，毛孔，雀斑等细节，而真实还原人体皮肤上的这些细节则是一个较大的挑战。</p>\n<p><img src=\"/2025/01/04/digital-human-render-2/layers_of_skin.png\" alt=\"皮肤多层结构\"></p>\n<p>皮肤作为一种属性复杂的材质，不同于简单的材质表面比如说水泥墙这些，其物理结构由<strong>多层结构</strong>组成，其表面油脂层主要贡献了皮肤光照的<em>反射</em>部分，而油脂层下面的表皮层和真皮层则贡献了的<em>次表面散射</em>部分，而且还有一部分光会<em>透射</em>过皮肤的边缘或者很薄的地方。</p>\n<p>这三个方面组成了皮肤渲染的主要因素，我们今天也着重介绍这三部分的一些计算方法。</p>\n<h4 id=\"镜面反射\"><a href=\"#镜面反射\" class=\"headerlink\" title=\"镜面反射\"></a>镜面反射</h4><p>在皮肤渲染中，高光这部分主要是皮肤的油脂层贡献的。高光的算法可以使用基本的**<a href=\"https://zhuanlan.zhihu.com/p/715918965\">cook Torrance brdf模型</a>**的高光计算部分，因为时间比较紧张，我们就不花时间介绍了。这是最经典PBR算法之一了，如果大家网上搜BRDF就很容易能能够找到。大致思路是高光表现会依据平面的粗糙度，观测角度等不同而不同。如下方图所示。</p>\n<p><img src=\"/2025/01/04/digital-human-render-2/Cook-Torrance-BRDF.png\" alt=\"Cook-Torrance BRDF\"></p>\n<p>但是直接用BRDF计算皮肤高光一般并不能获得最好的效果，因为皮肤是一个复合的表面，他的突出部分和凹陷部分的粗糙度是不一样的。导致有两个粗糙参数，也就有两个高光需要表示。</p>\n<p>所以虚幻引擎等某些渲染器会使用一个叫做<em>双镜叶高光</em>的技术。<strong>镜叶 lobe</strong>，也是如下图所示，其实就是光在某一个粗糙度平面下的一个分布状态。</p>\n<p>![镜叶]](specular-lobe.png)</p>\n<p>下面这张图是UE里面默认的高光混合的参数，通过混合两个粗糙度的高光表现，可以达到更贴近人脸皮肤的效果:</p>\n<p><img src=\"/2025/01/04/digital-human-render-2/skin-specular.png\" alt=\"皮肤高光\"></p>\n<h4 id=\"次表面散射-BSSRDF\"><a href=\"#次表面散射-BSSRDF\" class=\"headerlink\" title=\"次表面散射(BSSRDF)\"></a>次表面散射(BSSRDF)</h4><p>计算完高光后，我们之前提到，光线接触到皮肤时，有大约94%被皮肤各层散射，只有大约6%被反射。<br>我们可以看下对比图，前面我们提到的BRDF，其实主要就是假设光线的反射基于图a的现象，入射点和出射点是同一个，光在这个地方发生漫反射:</p>\n<p><img src=\"/2025/01/04/digital-human-render-2/diffuse-comparison.png\" alt=\"反射对比\"></p>\n<p>但其实光线在进入皮肤后的真实情况是更接近图b的，光线会进入我们的皮肤，通过油脂层到下面的表皮层和真皮层，会进行一阵游走，然后最终有一部分光线会被反射出来。</p>\n<p>实际上几乎所有材质都存在次表面散射现象，区别只在于材质散射密度分布的集中程度，如果绝大部分能量都集中在入射点附近，就表示附近像素对当前像素的光照贡献不明显，可以忽略，则在渲染时我们就用漫反射代替，如果该函数分布比较均匀，附近像素对当前像素的光照贡献明显，则需要单独计算次表面散射。</p>\n<p>为了模拟这种光线表现，提出了<strong>BSSRDF</strong>。</p>\n<p><img src=\"/2025/01/04/digital-human-render-2/BSSRDF.png\" alt=\"BSSRDF\"></p>\n<p>BSSRDF描述的是，对于当前出射点和出射方向，某个入射点和入射方向的光线能量对其结果的贡献。<br>我们观察点是固定的po，已知需要的出射方向，根据这些条件获取周围点对他的光照贡献（w：omega）。</p>\n<p>如果我们想要按照真实世界，实时的模拟出每一束光在皮肤材质中的路线，从而获取到每一束光的正确出射点和角度的话，是难道很大的。BSSRDF的意义在于快速的近似真实世界的效果，为了平衡性能和效果，我们假设了4个前提：</p>\n<ul>\n<li>物体是一个曲率为0的平面。</li>\n<li>平面的厚度和大小都是无限。</li>\n<li>内部的介质参数是均匀的。</li>\n<li>光线永远是从垂直方向入射表面。</li>\n</ul>\n<p>基于这些前提，我们就可以单纯以像素的距离作为权重，距离当前像素近的入射光照，贡献就大，反之距离远的，贡献小。对应的也就是公式上的R(||pi-po||)这部分。</p>\n<p><img src=\"/2025/01/04/digital-human-render-2/BSSRDF-1.png\" alt=\"BSSRDF-1\"></p>\n<p>当然真正人体表面的皮肤是不满足与上面四点的，但是考虑到实时渲染的性能，单纯按照两个点的距离的近似可以达到能接受的效果。</p>\n<p>用来描述光在物体内部的散射或者扩散的行为，就是公式中R那个部分，这个分布函数我们叫做<em>散射剖面（diffusion profile）</em>，也有叫<em>扩散剖面</em>的。</p>\n<p>计算散射剖面的算法有很多种，常见的有<strong>偶极子，多极子，高斯和拟合</strong>等等。这里内容比较深，由于时间的关系我们暂时不做详细介绍了。</p>\n<p>同时，由于是单纯的根据距离来获得光照的权重，我们可以预处理散射剖面，做成一张lookup table，在实时渲染的时候直接查找对应的值，以加速渲染。<br>可以看一下这张图，reflectance（反射率）根据距离的变化，而且rgb三原色是分开计算的.</p>\n<p><img src=\"/2025/01/04/digital-human-render-2/reflectance-lookup-table.png\" alt=\"反射率参照表\"></p>\n<h4 id=\"基于模糊的算法\"><a href=\"#基于模糊的算法\" class=\"headerlink\" title=\"基于模糊的算法\"></a>基于模糊的算法</h4><p>计算次表面散射的光照的时候，当前像素的光照会受到周边像素的影响，而这个影响的程度我们是以距离来决定的。那其实换个角度想想，这是不是就是我们把原来当前像素的漫反射，抹匀到了周边，因为光的能量经过次表面散射分散到了周边的像素。这其实就是一个模糊操作，从数学角度上，都是做卷积处理。所以就有了基于模糊的皮肤渲染算法。</p>\n<p>那么根据施加模糊的空间，分为了<em>纹理空间模糊</em>和<em>屏幕空间模糊</em>：</p>\n<h5 id=\"纹理空间模糊\"><a href=\"#纹理空间模糊\" class=\"headerlink\" title=\"纹理空间模糊\"></a>纹理空间模糊</h5><p>纹理空间模糊他的一般步骤大概是：</p>\n<ol>\n<li>首先需要获得一张拉伸校正贴图，一般是会预计算这张帖图，主要是为了表示每个Texel（纹素）需要进行多大范围的模糊。</li>\n<li>然后渲染出模型的光照，漫反射，然后将模型的光照展开到纹理空间。</li>\n<li>将这张图根据拉伸校正贴图所标定的范围，进行模糊处理，保存成一张或者多张纹理贴图。</li>\n<li>最终渲染的时候，我们会获取依据这些贴图，然后按照某些特定的权重将它们混合，得到最后的漫反射结果</li>\n</ol>\n<p><img src=\"/2025/01/04/digital-human-render-2/texture-space-blur.png\" alt=\"纹理空间模糊\"></p>\n<p>在纹理空间模糊的好处很明显：比较正确，不会穿帮，可以进行低精度的绘制再利用硬件插值来辅助Blur，模糊的方法也很多。</p>\n<p>但缺点也很明显：主要是背面也同样要绘制，而且美术需要处理好纹理不然会有接缝问题。</p>\n<h5 id=\"屏幕空间模糊\"><a href=\"#屏幕空间模糊\" class=\"headerlink\" title=\"屏幕空间模糊\"></a>屏幕空间模糊</h5><p>那么屏幕空间模糊就比较好理解了，就是在屏幕空间对皮肤的光照结果进行模糊。<br><img src=\"/2025/01/04/digital-human-render-2/screen-space-blur.png\" alt=\"屏幕空间模糊\"></p>\n<p>需要注意一下边界问题，不能模糊出界了，处理的时候可以根据深度等作为模板处理。<br>下面的图是皮肤在纹理空间和屏幕空间的模糊的效果的不同。</p>\n<p><img src=\"/2025/01/04/digital-human-render-2/blur-camparison.png\" alt=\"模糊比较\"></p>\n<h4 id=\"透射\"><a href=\"#透射\" class=\"headerlink\" title=\"透射\"></a>透射</h4><p>最后我们简单讲一下透射，透射和次表面散射的区别是透射的光一般是从另外一面照射过来的，而次表面散射我们一般是按照光源和我们的观察点在模型的同一侧。像是我们手指边缘，或者耳垂部分，是比较容易出现这种现象的。</p>\n<p>我们一般计算透射的方式的步骤包括三步：</p>\n<ol>\n<li>计算光照在进入半透明介质时的强度</li>\n<li>计算光线在介质中经过的路径</li>\n<li>根据路径长度和BTDF来计算出射光线的强度</li>\n</ol>\n<p>那这里我们又要提出一个BXXXDF了，也就是BTDF，其中的T就代表透射。他和BRDF较为类似，只不过光源是从另外一面穿出来的。</p>\n<p><img src=\"/2025/01/04/digital-human-render-2/BTDF.png\" alt=\"BTDF\"></p>\n<p>当然由于光在另外一面穿到前面来，光的强度会有损失，光也会在皮肤出现次表面散射，不过BTDF作为一种近似算法在实时渲染中一般只简化为一个和光线路径长度的函数。</p>\n<p>不过一般来说皮肤渲染上透射出现的区域也是比较小的。</p>\n<h3 id=\"UE里面的皮肤着色模型\"><a href=\"#UE里面的皮肤着色模型\" class=\"headerlink\" title=\"UE里面的皮肤着色模型\"></a>UE里面的皮肤着色模型</h3><p>皮肤的渲染算法我们目前就介绍到这里了，当然作为浅谈我们只是稍微触及了一些皮毛，想要做出更好更加真实的皮肤效果还有很多地方可以深入。</p>\n<p><img src=\"/2025/01/04/digital-human-render-2/ue-skin.png\" alt=\"UE皮肤着色模型\"></p>\n<p>这里我大概介绍一下UE，可能很多同学自己做项目也是用的UE。UE的皮肤材质有很多种着色类型，在材质的着色类型可以选择，一般来说皮肤的材质会默认是<strong>次表面轮廓类型</strong>，这是效果最好的着色模型，像是metahuman的皮肤材质着色也是这种类型。</p>\n<p><strong>次表面</strong>就是我们前面讲的基于次表面散射的着色模型，因为次表面散射也可以用作冰川等材质，所以如果选择次表面着色模型且针对皮肤想要有更好的效果的话，需要自己进行调整。</p>\n<p>最后<strong>预整合皮肤</strong>也是我们之前提到的优化方法之一，他精度比次表面略低但是性能开销也低。</p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>写实风格的皮肤渲染技术就分享到这了，不知不觉文章的长度已经超长了，剩下的部分只能放到后续的文章了。下次的分享的主要内容将会是头发的渲染。</p>\n"},{"title":"级联阴影贴图实现","date":"2024-10-13T09:06:53.000Z","index_img":"/2024/10/13/cascade-shadow-map/sm_far.png","banner_img":"/2024/10/13/cascade-shadow-map/sm_far.png","_content":"\n# 阴影贴图的局限\n阴影贴图（shadow map）是3D场景中实现阴影效果的基础手段，它通过预先将光线方向的场景深度存储到贴图中，在渲染的时候取每个场景中的点到光源的距离和深度贴图作比较，来判定该点是否在阴影当中。\n\n但是在较大的场景中，使用阴影贴图会有几个明显的不足：\n\n1. 阴影贴图只能覆盖部分场景，在渲染较大的场景的时候（如大世界），远处的场景基本上无法被阴影贴图所覆盖。\n2. 贴图的分辨率是有限的，太大的分辨率会对性能造成非常大的影响。但是在覆盖较大场景的时候，贴图分辨率不足会导致阴影模糊，效果不佳。\n3. 阴影贴图的实现一开始其实并没有考虑玩家相机的视椎体，也就是说在玩家没有看的地方也会渲染阴影贴图，这对渲染资源来说显然是个浪费。\n\nKongEngine计划在后面接入大地形的渲染，借此机会接入了级联阴影贴图的能力。\n实现方法参考了[LearnOpenGL的教程](https://learnopengl.com/Guest-Articles/2021/CSM)。\n\n\n# 级联阴影贴图的实现\n级联阴影贴图的基本概念包括如下几点：\n\n1. 将玩家的视椎体划分为几段，每一段视椎体构建一张阴影贴图覆盖，这个阴影贴图完美贴合从光源方向投射到这段视椎体中心点的正交投影。\n2. 和模型LOD的理念类似，离相机近的阴影贴图需要采用较高精度，而离相机远的阴影贴图可以使用低精度。\n3. 将多级阴影贴图传入最后的光照计算着色器，根据每个点所处视椎体的分段不同采用对应不同的阴影贴图计算光照。\n\n听起来挺简单的对吧，那我们一步一步来。\n\n## 视椎体分段\n上面说到我们需要将视椎体分为几段，在每一段视椎体覆盖一张阴影贴图，并计算出这张贴图的从光源方向看向视椎体中心点的正交投影的矩阵，也就是Light projection matrix和Light view matrix。这个矩阵需要紧密贴合这段视椎体，为此我们需要得到视椎体的顶点的世界坐标，得到顶点的min、max和视椎体的中心点。\n\n我们从相机的视椎体出发，当处于视椎体范围上的顶点的世界坐标经过projection矩阵和view矩阵转换后，xyz都会被映射到[-1,1]范围的屏幕空间坐标。矩阵转换是可逆的，也就是说取屏幕空间坐标为[-1, 1]边界的八个顶点，经过视椎体的projection矩阵和view矩阵的逆矩阵转换后，就能得到边界顶点的世界空间坐标。在代码里面的实现如下：\n\n```c++\nstd::vector<glm::vec4> CDirectionalLightComponent::GetFrustumCornersWorldSpace(const glm::mat4& proj_view)\n{\n    const auto inv = glm::inverse(proj_view);\n\n    // 顶点的世界坐标在projection和view matrix的转换下的坐标范围是[-1,1]\n    // 那么将在[-1,1]这个边界的八个顶点坐标乘以projection和view matrix的逆矩阵则可以得到视锥体边界的顶点的世界坐标\n    vector<vec4> frustum_corners;\n    for(unsigned int i = 0; i < 2; i++)\n    {\n        for(unsigned int j = 0; j < 2; j++)\n        {\n            for(unsigned int k = 0; k < 2; k++)\n            {\n                const vec4 pt = inv * vec4(2.0f*i-1.0f,2.0f*j-1.0f,2.0f*k-1.0f, 1.0f);\n                frustum_corners.push_back(pt / pt.w);\n            }\n        }   \n    }\n    \n    return frustum_corners;\n}\n```\n\n我们得到了视椎体角落的顶点世界坐标，我们希望阴影贴图能够如下图一般贴合每一段视椎体。那么我们需要计算视椎体的中心顶点坐标，中心顶点在计算view矩阵的时候需要用到；我们需要计算在xyz轴上顶点坐标的最大和最小值，这些数值在计算projection矩阵的时候会被用到。\n\n![级联阴影贴图由远及近](https://learnopengl.com/img/guest/2021/CSM/frustum_fitting.png)\n\n计算中心点的代码十分简单，将视椎体角落的坐标相加后再除以数量即可，代码如下：\n\n``` c++\nvec3 center = vec3(0.0f);\nfor(const auto& v : corners)\n{\n    center += vec3(v);\n}\ncenter /= corners.size();   // 获取视锥体的中心点\n\nconst auto light_view = lookAt(center-light_dir, center, vec3(0.0f, 1.0f, 0.0f));\n```\n\n计算贴合视椎体的范围则是比较并记录各个顶点在xyz轴的最大值和最小值，方法如下。这里提一下在z轴方向和一个参数z_mult进行了处理，其意义是阴影的投射源是有可能在视椎体范围之外的，如果不考虑这一部分的影响的话可能在阴影过度的时候会非常生硬，并且丢掉一些本来该显示的阴影导致渲染错误。\n\n``` c++\nfloat min_x = std::numeric_limits<float>::max();\nfloat min_y = std::numeric_limits<float>::max();\nfloat min_z = std::numeric_limits<float>::max();\nfloat max_x = std::numeric_limits<float>::lowest();\nfloat max_y = std::numeric_limits<float>::lowest();\nfloat max_z = std::numeric_limits<float>::lowest();\nfor (const auto& v : corners)\n{\n    const auto trf = light_view * v;\n    min_x = std::min(min_x, trf.x);\n    max_x = std::max(max_x, trf.x);\n    min_y = std::min(min_y, trf.y);\n    max_y = std::max(max_y, trf.y);\n    min_z = std::min(min_z, trf.z);\n    max_z = std::max(max_z, trf.z);\n}\nconstexpr float z_mult = 10.0f;\nif (min_z < 0)\n{\n    min_z *= z_mult;\n}\nelse\n{\n    min_z /= z_mult;\n}\nif (max_z < 0)\n{\n    max_z /= z_mult;\n}\nelse\n{\n    max_z *= z_mult;\n}\n\nconst mat4 light_projection = ortho(min_x, max_x, min_y, max_y, min_z, max_z);\n```\n\n## 计算级联阴影贴图\n一般的阴影贴图我们采用的是GL_TEXTURE_2D，而级联阴影贴图我们需要传入多张贴图，因此对应的贴图类型会变为GL_TEXTURE_2D_ARRAY。\n\n``` c++\nglGenTextures(1, &csm_texture);\nglBindTexture(GL_TEXTURE_2D_ARRAY, csm_texture);\nglTexImage3D(GL_TEXTURE_2D_ARRAY, 0, GL_DEPTH_COMPONENT32F, SHADOW_RESOLUTION, SHADOW_RESOLUTION, (int)csm_distances.size()+1, 0, GL_DEPTH_COMPONENT, GL_FLOAT, nullptr);\nglTexParameteri(GL_TEXTURE_2D_ARRAY, GL_TEXTURE_MAG_FILTER, GL_NEAREST);\nglTexParameteri(GL_TEXTURE_2D_ARRAY, GL_TEXTURE_MIN_FILTER, GL_NEAREST);\nglTexParameteri(GL_TEXTURE_2D_ARRAY, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_BORDER);\nglTexParameteri(GL_TEXTURE_2D_ARRAY, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_BORDER);\nglTexParameterfv(GL_TEXTURE_2D_ARRAY, GL_TEXTURE_BORDER_COLOR, border_color);\n\nglBindFramebuffer(GL_FRAMEBUFFER, shadowmap_fbo);\nglFramebufferTexture(GL_FRAMEBUFFER, GL_DEPTH_ATTACHMENT, csm_texture, 0);\nglDrawBuffer(GL_NONE);\nglReadBuffer(GL_NONE);\nglBindFramebuffer(GL_FRAMEBUFFER, 0);\n```\n\n除此之外，我们需要一次性渲染多张贴图，我们参考点光源阴影贴图使用geometry shader的做法，将顶点映射到不同的视椎体分段的光源的投影。代码如下：\n\n``` glsl\n#version 450 compatibility\nlayout(triangles, invocations = 6) in;\nlayout(triangle_strip, max_vertices = 3) out;\n\nuniform mat4 light_space_matrix[16];\n\n\nvoid main()\n{\n\tfor (int i = 0; i < 3; ++i)\n\t{\n\t\tgl_Position = light_space_matrix[gl_InvocationID] * gl_in[i].gl_Position;\n\t\tgl_Layer = gl_InvocationID;\n\t\tEmitVertex();\n\t}\n\tEndPrimitive();\n} \n```\n\n这里新增的**invocations = 6**代表了这个Shader可以被实例化，每个实例同时平行进行运算，实例的个数为6。内置的**gl_InvocationID**代表了当前处理的是哪一个实例，我们将其赋值到**gl_Layer**。其余的阴影贴图渲染步骤和普通的阴影贴图类似。\n\n下面几张图所示展示的，就是从近到远的几个级联阴影贴图的表现：\n![](csm_near.png)\n![](csm_mid.png)\n![](csm_far.png)\n\n## 使用级联阴影贴图\n级联阴影贴图的使用和阴影贴图是类似的，由于传入给光照Shader的是GL_TEXTURE_2D_ARRAY，需要使用vec3来索引贴图数据的，vec3的z值代表的是Layer索引。\n\nLayer代表的是使用哪一个视椎体分段的阴影贴图，取决于当前像素和相机的距离。取得对应的Layer参数后带入texcoord的z值读取对应的阴影贴图的值。示例代码如下：\n\n``` glsl\n// 计算阴影\nfloat ShadowCalculation_DirLight(vec4 frag_world_pos, vec3 to_light_dir, vec3 in_normal)\n{\n    // 获取像素和相机的距离，也就是view转换后的z值\n    vec4 frag_pos_view_space = matrix_ubo.view * frag_world_pos;\n    float depthValue = abs(frag_pos_view_space.z);\n\n    // 根据距离和每段视椎体分段的距离区间，获取Layer值\n    int layer = -1;\n    for (int i = 0; i < csm_level_count; ++i)\n    {\n        if (depthValue < csm_distances[i])\n        {\n            layer = i;\n            break;\n        }\n    }\n    if (layer == -1)\n    {\n        layer = csm_level_count;\n    }\n    // 下面的和应用普通阴影贴图的一致\n    // 转换到-1,1的范围，再转到0,1的范围\n    vec4 frag_pos_light_space = light_space_matrices[layer] * frag_world_pos;\n    // perform perspective divide\n    vec3 proj_coord = frag_pos_light_space.xyz / frag_pos_light_space.w;\n    // transform to [0,1] range\n    proj_coord = proj_coord * 0.5 + 0.5;\n\n    // get depth of current fragment from light's perspective\n    float current_depth = proj_coord.z;\n\n    // keep the shadow at 0.0 when outside the far_plane region of the light's frustum.\n    if (current_depth > 1.0)\n    {\n        return 0.0;\n    }\n\n    // PCF\n    float shadow = 0.0;\n    vec2 texel_size = 1.0 / vec2(textureSize(shadow_map, 0));\n    for(int x = -1; x <= 1; ++x)\n    {\n        for(int y = -1; y <= 1; ++y)\n        {\n            float pcf_depth = texture(shadow_map, vec3(proj_coord.xy + vec2(x, y) * texel_size, layer)).r;\n            shadow += current_depth > pcf_depth ? 1.0 : 0.0;\n        }\n    }\n    shadow /= 9.0;\n        \n    return shadow;\n}\n```\n\n# 效果对比\n## 原先的阴影贴图\n原先的阴影贴图只能覆盖有限的场景：\n![](sm_near.png)\n\n提升覆盖范围后，阴影的质量则会出现下降：\n![](sm_far.png)\n\n## 级联阴影贴图\n采用级联阴影贴图可以覆盖很大的场景，并且在可控的性能消耗下仍然有不错的显示智联。\n![](csm_result.png)\n","source":"_posts/cascade-shadow-map.md","raw":"---\ntitle: 级联阴影贴图实现\ndate: 2024-10-13 17:06:53\ncategories: \n\t- 技术漫谈\ntags: [3D, render, 渲染, 编程]\nindex_img: /2024/10/13/cascade-shadow-map/sm_far.png\nbanner_img: /2024/10/13/cascade-shadow-map/sm_far.png\n---\n\n# 阴影贴图的局限\n阴影贴图（shadow map）是3D场景中实现阴影效果的基础手段，它通过预先将光线方向的场景深度存储到贴图中，在渲染的时候取每个场景中的点到光源的距离和深度贴图作比较，来判定该点是否在阴影当中。\n\n但是在较大的场景中，使用阴影贴图会有几个明显的不足：\n\n1. 阴影贴图只能覆盖部分场景，在渲染较大的场景的时候（如大世界），远处的场景基本上无法被阴影贴图所覆盖。\n2. 贴图的分辨率是有限的，太大的分辨率会对性能造成非常大的影响。但是在覆盖较大场景的时候，贴图分辨率不足会导致阴影模糊，效果不佳。\n3. 阴影贴图的实现一开始其实并没有考虑玩家相机的视椎体，也就是说在玩家没有看的地方也会渲染阴影贴图，这对渲染资源来说显然是个浪费。\n\nKongEngine计划在后面接入大地形的渲染，借此机会接入了级联阴影贴图的能力。\n实现方法参考了[LearnOpenGL的教程](https://learnopengl.com/Guest-Articles/2021/CSM)。\n\n\n# 级联阴影贴图的实现\n级联阴影贴图的基本概念包括如下几点：\n\n1. 将玩家的视椎体划分为几段，每一段视椎体构建一张阴影贴图覆盖，这个阴影贴图完美贴合从光源方向投射到这段视椎体中心点的正交投影。\n2. 和模型LOD的理念类似，离相机近的阴影贴图需要采用较高精度，而离相机远的阴影贴图可以使用低精度。\n3. 将多级阴影贴图传入最后的光照计算着色器，根据每个点所处视椎体的分段不同采用对应不同的阴影贴图计算光照。\n\n听起来挺简单的对吧，那我们一步一步来。\n\n## 视椎体分段\n上面说到我们需要将视椎体分为几段，在每一段视椎体覆盖一张阴影贴图，并计算出这张贴图的从光源方向看向视椎体中心点的正交投影的矩阵，也就是Light projection matrix和Light view matrix。这个矩阵需要紧密贴合这段视椎体，为此我们需要得到视椎体的顶点的世界坐标，得到顶点的min、max和视椎体的中心点。\n\n我们从相机的视椎体出发，当处于视椎体范围上的顶点的世界坐标经过projection矩阵和view矩阵转换后，xyz都会被映射到[-1,1]范围的屏幕空间坐标。矩阵转换是可逆的，也就是说取屏幕空间坐标为[-1, 1]边界的八个顶点，经过视椎体的projection矩阵和view矩阵的逆矩阵转换后，就能得到边界顶点的世界空间坐标。在代码里面的实现如下：\n\n```c++\nstd::vector<glm::vec4> CDirectionalLightComponent::GetFrustumCornersWorldSpace(const glm::mat4& proj_view)\n{\n    const auto inv = glm::inverse(proj_view);\n\n    // 顶点的世界坐标在projection和view matrix的转换下的坐标范围是[-1,1]\n    // 那么将在[-1,1]这个边界的八个顶点坐标乘以projection和view matrix的逆矩阵则可以得到视锥体边界的顶点的世界坐标\n    vector<vec4> frustum_corners;\n    for(unsigned int i = 0; i < 2; i++)\n    {\n        for(unsigned int j = 0; j < 2; j++)\n        {\n            for(unsigned int k = 0; k < 2; k++)\n            {\n                const vec4 pt = inv * vec4(2.0f*i-1.0f,2.0f*j-1.0f,2.0f*k-1.0f, 1.0f);\n                frustum_corners.push_back(pt / pt.w);\n            }\n        }   \n    }\n    \n    return frustum_corners;\n}\n```\n\n我们得到了视椎体角落的顶点世界坐标，我们希望阴影贴图能够如下图一般贴合每一段视椎体。那么我们需要计算视椎体的中心顶点坐标，中心顶点在计算view矩阵的时候需要用到；我们需要计算在xyz轴上顶点坐标的最大和最小值，这些数值在计算projection矩阵的时候会被用到。\n\n![级联阴影贴图由远及近](https://learnopengl.com/img/guest/2021/CSM/frustum_fitting.png)\n\n计算中心点的代码十分简单，将视椎体角落的坐标相加后再除以数量即可，代码如下：\n\n``` c++\nvec3 center = vec3(0.0f);\nfor(const auto& v : corners)\n{\n    center += vec3(v);\n}\ncenter /= corners.size();   // 获取视锥体的中心点\n\nconst auto light_view = lookAt(center-light_dir, center, vec3(0.0f, 1.0f, 0.0f));\n```\n\n计算贴合视椎体的范围则是比较并记录各个顶点在xyz轴的最大值和最小值，方法如下。这里提一下在z轴方向和一个参数z_mult进行了处理，其意义是阴影的投射源是有可能在视椎体范围之外的，如果不考虑这一部分的影响的话可能在阴影过度的时候会非常生硬，并且丢掉一些本来该显示的阴影导致渲染错误。\n\n``` c++\nfloat min_x = std::numeric_limits<float>::max();\nfloat min_y = std::numeric_limits<float>::max();\nfloat min_z = std::numeric_limits<float>::max();\nfloat max_x = std::numeric_limits<float>::lowest();\nfloat max_y = std::numeric_limits<float>::lowest();\nfloat max_z = std::numeric_limits<float>::lowest();\nfor (const auto& v : corners)\n{\n    const auto trf = light_view * v;\n    min_x = std::min(min_x, trf.x);\n    max_x = std::max(max_x, trf.x);\n    min_y = std::min(min_y, trf.y);\n    max_y = std::max(max_y, trf.y);\n    min_z = std::min(min_z, trf.z);\n    max_z = std::max(max_z, trf.z);\n}\nconstexpr float z_mult = 10.0f;\nif (min_z < 0)\n{\n    min_z *= z_mult;\n}\nelse\n{\n    min_z /= z_mult;\n}\nif (max_z < 0)\n{\n    max_z /= z_mult;\n}\nelse\n{\n    max_z *= z_mult;\n}\n\nconst mat4 light_projection = ortho(min_x, max_x, min_y, max_y, min_z, max_z);\n```\n\n## 计算级联阴影贴图\n一般的阴影贴图我们采用的是GL_TEXTURE_2D，而级联阴影贴图我们需要传入多张贴图，因此对应的贴图类型会变为GL_TEXTURE_2D_ARRAY。\n\n``` c++\nglGenTextures(1, &csm_texture);\nglBindTexture(GL_TEXTURE_2D_ARRAY, csm_texture);\nglTexImage3D(GL_TEXTURE_2D_ARRAY, 0, GL_DEPTH_COMPONENT32F, SHADOW_RESOLUTION, SHADOW_RESOLUTION, (int)csm_distances.size()+1, 0, GL_DEPTH_COMPONENT, GL_FLOAT, nullptr);\nglTexParameteri(GL_TEXTURE_2D_ARRAY, GL_TEXTURE_MAG_FILTER, GL_NEAREST);\nglTexParameteri(GL_TEXTURE_2D_ARRAY, GL_TEXTURE_MIN_FILTER, GL_NEAREST);\nglTexParameteri(GL_TEXTURE_2D_ARRAY, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_BORDER);\nglTexParameteri(GL_TEXTURE_2D_ARRAY, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_BORDER);\nglTexParameterfv(GL_TEXTURE_2D_ARRAY, GL_TEXTURE_BORDER_COLOR, border_color);\n\nglBindFramebuffer(GL_FRAMEBUFFER, shadowmap_fbo);\nglFramebufferTexture(GL_FRAMEBUFFER, GL_DEPTH_ATTACHMENT, csm_texture, 0);\nglDrawBuffer(GL_NONE);\nglReadBuffer(GL_NONE);\nglBindFramebuffer(GL_FRAMEBUFFER, 0);\n```\n\n除此之外，我们需要一次性渲染多张贴图，我们参考点光源阴影贴图使用geometry shader的做法，将顶点映射到不同的视椎体分段的光源的投影。代码如下：\n\n``` glsl\n#version 450 compatibility\nlayout(triangles, invocations = 6) in;\nlayout(triangle_strip, max_vertices = 3) out;\n\nuniform mat4 light_space_matrix[16];\n\n\nvoid main()\n{\n\tfor (int i = 0; i < 3; ++i)\n\t{\n\t\tgl_Position = light_space_matrix[gl_InvocationID] * gl_in[i].gl_Position;\n\t\tgl_Layer = gl_InvocationID;\n\t\tEmitVertex();\n\t}\n\tEndPrimitive();\n} \n```\n\n这里新增的**invocations = 6**代表了这个Shader可以被实例化，每个实例同时平行进行运算，实例的个数为6。内置的**gl_InvocationID**代表了当前处理的是哪一个实例，我们将其赋值到**gl_Layer**。其余的阴影贴图渲染步骤和普通的阴影贴图类似。\n\n下面几张图所示展示的，就是从近到远的几个级联阴影贴图的表现：\n![](csm_near.png)\n![](csm_mid.png)\n![](csm_far.png)\n\n## 使用级联阴影贴图\n级联阴影贴图的使用和阴影贴图是类似的，由于传入给光照Shader的是GL_TEXTURE_2D_ARRAY，需要使用vec3来索引贴图数据的，vec3的z值代表的是Layer索引。\n\nLayer代表的是使用哪一个视椎体分段的阴影贴图，取决于当前像素和相机的距离。取得对应的Layer参数后带入texcoord的z值读取对应的阴影贴图的值。示例代码如下：\n\n``` glsl\n// 计算阴影\nfloat ShadowCalculation_DirLight(vec4 frag_world_pos, vec3 to_light_dir, vec3 in_normal)\n{\n    // 获取像素和相机的距离，也就是view转换后的z值\n    vec4 frag_pos_view_space = matrix_ubo.view * frag_world_pos;\n    float depthValue = abs(frag_pos_view_space.z);\n\n    // 根据距离和每段视椎体分段的距离区间，获取Layer值\n    int layer = -1;\n    for (int i = 0; i < csm_level_count; ++i)\n    {\n        if (depthValue < csm_distances[i])\n        {\n            layer = i;\n            break;\n        }\n    }\n    if (layer == -1)\n    {\n        layer = csm_level_count;\n    }\n    // 下面的和应用普通阴影贴图的一致\n    // 转换到-1,1的范围，再转到0,1的范围\n    vec4 frag_pos_light_space = light_space_matrices[layer] * frag_world_pos;\n    // perform perspective divide\n    vec3 proj_coord = frag_pos_light_space.xyz / frag_pos_light_space.w;\n    // transform to [0,1] range\n    proj_coord = proj_coord * 0.5 + 0.5;\n\n    // get depth of current fragment from light's perspective\n    float current_depth = proj_coord.z;\n\n    // keep the shadow at 0.0 when outside the far_plane region of the light's frustum.\n    if (current_depth > 1.0)\n    {\n        return 0.0;\n    }\n\n    // PCF\n    float shadow = 0.0;\n    vec2 texel_size = 1.0 / vec2(textureSize(shadow_map, 0));\n    for(int x = -1; x <= 1; ++x)\n    {\n        for(int y = -1; y <= 1; ++y)\n        {\n            float pcf_depth = texture(shadow_map, vec3(proj_coord.xy + vec2(x, y) * texel_size, layer)).r;\n            shadow += current_depth > pcf_depth ? 1.0 : 0.0;\n        }\n    }\n    shadow /= 9.0;\n        \n    return shadow;\n}\n```\n\n# 效果对比\n## 原先的阴影贴图\n原先的阴影贴图只能覆盖有限的场景：\n![](sm_near.png)\n\n提升覆盖范围后，阴影的质量则会出现下降：\n![](sm_far.png)\n\n## 级联阴影贴图\n采用级联阴影贴图可以覆盖很大的场景，并且在可控的性能消耗下仍然有不错的显示智联。\n![](csm_result.png)\n","slug":"cascade-shadow-map","published":1,"updated":"2024-10-15T14:31:53.649Z","comments":1,"layout":"post","photos":[],"_id":"cm5ht44vi000c3457fj093ouj","content":"<h1 id=\"阴影贴图的局限\"><a href=\"#阴影贴图的局限\" class=\"headerlink\" title=\"阴影贴图的局限\"></a>阴影贴图的局限</h1><p>阴影贴图（shadow map）是3D场景中实现阴影效果的基础手段，它通过预先将光线方向的场景深度存储到贴图中，在渲染的时候取每个场景中的点到光源的距离和深度贴图作比较，来判定该点是否在阴影当中。</p>\n<p>但是在较大的场景中，使用阴影贴图会有几个明显的不足：</p>\n<ol>\n<li>阴影贴图只能覆盖部分场景，在渲染较大的场景的时候（如大世界），远处的场景基本上无法被阴影贴图所覆盖。</li>\n<li>贴图的分辨率是有限的，太大的分辨率会对性能造成非常大的影响。但是在覆盖较大场景的时候，贴图分辨率不足会导致阴影模糊，效果不佳。</li>\n<li>阴影贴图的实现一开始其实并没有考虑玩家相机的视椎体，也就是说在玩家没有看的地方也会渲染阴影贴图，这对渲染资源来说显然是个浪费。</li>\n</ol>\n<p>KongEngine计划在后面接入大地形的渲染，借此机会接入了级联阴影贴图的能力。<br>实现方法参考了<a href=\"https://learnopengl.com/Guest-Articles/2021/CSM\">LearnOpenGL的教程</a>。</p>\n<h1 id=\"级联阴影贴图的实现\"><a href=\"#级联阴影贴图的实现\" class=\"headerlink\" title=\"级联阴影贴图的实现\"></a>级联阴影贴图的实现</h1><p>级联阴影贴图的基本概念包括如下几点：</p>\n<ol>\n<li>将玩家的视椎体划分为几段，每一段视椎体构建一张阴影贴图覆盖，这个阴影贴图完美贴合从光源方向投射到这段视椎体中心点的正交投影。</li>\n<li>和模型LOD的理念类似，离相机近的阴影贴图需要采用较高精度，而离相机远的阴影贴图可以使用低精度。</li>\n<li>将多级阴影贴图传入最后的光照计算着色器，根据每个点所处视椎体的分段不同采用对应不同的阴影贴图计算光照。</li>\n</ol>\n<p>听起来挺简单的对吧，那我们一步一步来。</p>\n<h2 id=\"视椎体分段\"><a href=\"#视椎体分段\" class=\"headerlink\" title=\"视椎体分段\"></a>视椎体分段</h2><p>上面说到我们需要将视椎体分为几段，在每一段视椎体覆盖一张阴影贴图，并计算出这张贴图的从光源方向看向视椎体中心点的正交投影的矩阵，也就是Light projection matrix和Light view matrix。这个矩阵需要紧密贴合这段视椎体，为此我们需要得到视椎体的顶点的世界坐标，得到顶点的min、max和视椎体的中心点。</p>\n<p>我们从相机的视椎体出发，当处于视椎体范围上的顶点的世界坐标经过projection矩阵和view矩阵转换后，xyz都会被映射到[-1,1]范围的屏幕空间坐标。矩阵转换是可逆的，也就是说取屏幕空间坐标为[-1, 1]边界的八个顶点，经过视椎体的projection矩阵和view矩阵的逆矩阵转换后，就能得到边界顶点的世界空间坐标。在代码里面的实现如下：</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c++\"><span class=\"hljs-function\">std::vector&lt;glm::vec4&gt; <span class=\"hljs-title\">CDirectionalLightComponent::GetFrustumCornersWorldSpace</span><span class=\"hljs-params\">(<span class=\"hljs-type\">const</span> glm::mat4&amp; proj_view)</span></span><br><span class=\"hljs-function\"></span>&#123;<br>    <span class=\"hljs-type\">const</span> <span class=\"hljs-keyword\">auto</span> inv = glm::<span class=\"hljs-built_in\">inverse</span>(proj_view);<br><br>    <span class=\"hljs-comment\">// 顶点的世界坐标在projection和view matrix的转换下的坐标范围是[-1,1]</span><br>    <span class=\"hljs-comment\">// 那么将在[-1,1]这个边界的八个顶点坐标乘以projection和view matrix的逆矩阵则可以得到视锥体边界的顶点的世界坐标</span><br>    vector&lt;vec4&gt; frustum_corners;<br>    <span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">unsigned</span> <span class=\"hljs-type\">int</span> i = <span class=\"hljs-number\">0</span>; i &lt; <span class=\"hljs-number\">2</span>; i++)<br>    &#123;<br>        <span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">unsigned</span> <span class=\"hljs-type\">int</span> j = <span class=\"hljs-number\">0</span>; j &lt; <span class=\"hljs-number\">2</span>; j++)<br>        &#123;<br>            <span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">unsigned</span> <span class=\"hljs-type\">int</span> k = <span class=\"hljs-number\">0</span>; k &lt; <span class=\"hljs-number\">2</span>; k++)<br>            &#123;<br>                <span class=\"hljs-type\">const</span> vec4 pt = inv * <span class=\"hljs-built_in\">vec4</span>(<span class=\"hljs-number\">2.0f</span>*i<span class=\"hljs-number\">-1.0f</span>,<span class=\"hljs-number\">2.0f</span>*j<span class=\"hljs-number\">-1.0f</span>,<span class=\"hljs-number\">2.0f</span>*k<span class=\"hljs-number\">-1.0f</span>, <span class=\"hljs-number\">1.0f</span>);<br>                frustum_corners.<span class=\"hljs-built_in\">push_back</span>(pt / pt.w);<br>            &#125;<br>        &#125;   <br>    &#125;<br>    <br>    <span class=\"hljs-keyword\">return</span> frustum_corners;<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<p>我们得到了视椎体角落的顶点世界坐标，我们希望阴影贴图能够如下图一般贴合每一段视椎体。那么我们需要计算视椎体的中心顶点坐标，中心顶点在计算view矩阵的时候需要用到；我们需要计算在xyz轴上顶点坐标的最大和最小值，这些数值在计算projection矩阵的时候会被用到。</p>\n<p><img src=\"https://learnopengl.com/img/guest/2021/CSM/frustum_fitting.png\" alt=\"级联阴影贴图由远及近\"></p>\n<p>计算中心点的代码十分简单，将视椎体角落的坐标相加后再除以数量即可，代码如下：</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c++\">vec3 center = <span class=\"hljs-built_in\">vec3</span>(<span class=\"hljs-number\">0.0f</span>);<br><span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">const</span> <span class=\"hljs-keyword\">auto</span>&amp; v : corners)<br>&#123;<br>    center += <span class=\"hljs-built_in\">vec3</span>(v);<br>&#125;<br>center /= corners.<span class=\"hljs-built_in\">size</span>();   <span class=\"hljs-comment\">// 获取视锥体的中心点</span><br><br><span class=\"hljs-type\">const</span> <span class=\"hljs-keyword\">auto</span> light_view = <span class=\"hljs-built_in\">lookAt</span>(center-light_dir, center, <span class=\"hljs-built_in\">vec3</span>(<span class=\"hljs-number\">0.0f</span>, <span class=\"hljs-number\">1.0f</span>, <span class=\"hljs-number\">0.0f</span>));<br></code></pre></td></tr></table></figure>\n\n<p>计算贴合视椎体的范围则是比较并记录各个顶点在xyz轴的最大值和最小值，方法如下。这里提一下在z轴方向和一个参数z_mult进行了处理，其意义是阴影的投射源是有可能在视椎体范围之外的，如果不考虑这一部分的影响的话可能在阴影过度的时候会非常生硬，并且丢掉一些本来该显示的阴影导致渲染错误。</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c++\"><span class=\"hljs-type\">float</span> min_x = std::numeric_limits&lt;<span class=\"hljs-type\">float</span>&gt;::<span class=\"hljs-built_in\">max</span>();<br><span class=\"hljs-type\">float</span> min_y = std::numeric_limits&lt;<span class=\"hljs-type\">float</span>&gt;::<span class=\"hljs-built_in\">max</span>();<br><span class=\"hljs-type\">float</span> min_z = std::numeric_limits&lt;<span class=\"hljs-type\">float</span>&gt;::<span class=\"hljs-built_in\">max</span>();<br><span class=\"hljs-type\">float</span> max_x = std::numeric_limits&lt;<span class=\"hljs-type\">float</span>&gt;::<span class=\"hljs-built_in\">lowest</span>();<br><span class=\"hljs-type\">float</span> max_y = std::numeric_limits&lt;<span class=\"hljs-type\">float</span>&gt;::<span class=\"hljs-built_in\">lowest</span>();<br><span class=\"hljs-type\">float</span> max_z = std::numeric_limits&lt;<span class=\"hljs-type\">float</span>&gt;::<span class=\"hljs-built_in\">lowest</span>();<br><span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">const</span> <span class=\"hljs-keyword\">auto</span>&amp; v : corners)<br>&#123;<br>    <span class=\"hljs-type\">const</span> <span class=\"hljs-keyword\">auto</span> trf = light_view * v;<br>    min_x = std::<span class=\"hljs-built_in\">min</span>(min_x, trf.x);<br>    max_x = std::<span class=\"hljs-built_in\">max</span>(max_x, trf.x);<br>    min_y = std::<span class=\"hljs-built_in\">min</span>(min_y, trf.y);<br>    max_y = std::<span class=\"hljs-built_in\">max</span>(max_y, trf.y);<br>    min_z = std::<span class=\"hljs-built_in\">min</span>(min_z, trf.z);<br>    max_z = std::<span class=\"hljs-built_in\">max</span>(max_z, trf.z);<br>&#125;<br><span class=\"hljs-keyword\">constexpr</span> <span class=\"hljs-type\">float</span> z_mult = <span class=\"hljs-number\">10.0f</span>;<br><span class=\"hljs-keyword\">if</span> (min_z &lt; <span class=\"hljs-number\">0</span>)<br>&#123;<br>    min_z *= z_mult;<br>&#125;<br><span class=\"hljs-keyword\">else</span><br>&#123;<br>    min_z /= z_mult;<br>&#125;<br><span class=\"hljs-keyword\">if</span> (max_z &lt; <span class=\"hljs-number\">0</span>)<br>&#123;<br>    max_z /= z_mult;<br>&#125;<br><span class=\"hljs-keyword\">else</span><br>&#123;<br>    max_z *= z_mult;<br>&#125;<br><br><span class=\"hljs-type\">const</span> mat4 light_projection = <span class=\"hljs-built_in\">ortho</span>(min_x, max_x, min_y, max_y, min_z, max_z);<br></code></pre></td></tr></table></figure>\n\n<h2 id=\"计算级联阴影贴图\"><a href=\"#计算级联阴影贴图\" class=\"headerlink\" title=\"计算级联阴影贴图\"></a>计算级联阴影贴图</h2><p>一般的阴影贴图我们采用的是GL_TEXTURE_2D，而级联阴影贴图我们需要传入多张贴图，因此对应的贴图类型会变为GL_TEXTURE_2D_ARRAY。</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c++\"><span class=\"hljs-built_in\">glGenTextures</span>(<span class=\"hljs-number\">1</span>, &amp;csm_texture);<br><span class=\"hljs-built_in\">glBindTexture</span>(GL_TEXTURE_2D_ARRAY, csm_texture);<br><span class=\"hljs-built_in\">glTexImage3D</span>(GL_TEXTURE_2D_ARRAY, <span class=\"hljs-number\">0</span>, GL_DEPTH_COMPONENT32F, SHADOW_RESOLUTION, SHADOW_RESOLUTION, (<span class=\"hljs-type\">int</span>)csm_distances.<span class=\"hljs-built_in\">size</span>()<span class=\"hljs-number\">+1</span>, <span class=\"hljs-number\">0</span>, GL_DEPTH_COMPONENT, GL_FLOAT, <span class=\"hljs-literal\">nullptr</span>);<br><span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D_ARRAY, GL_TEXTURE_MAG_FILTER, GL_NEAREST);<br><span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D_ARRAY, GL_TEXTURE_MIN_FILTER, GL_NEAREST);<br><span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D_ARRAY, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_BORDER);<br><span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D_ARRAY, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_BORDER);<br><span class=\"hljs-built_in\">glTexParameterfv</span>(GL_TEXTURE_2D_ARRAY, GL_TEXTURE_BORDER_COLOR, border_color);<br><br><span class=\"hljs-built_in\">glBindFramebuffer</span>(GL_FRAMEBUFFER, shadowmap_fbo);<br><span class=\"hljs-built_in\">glFramebufferTexture</span>(GL_FRAMEBUFFER, GL_DEPTH_ATTACHMENT, csm_texture, <span class=\"hljs-number\">0</span>);<br><span class=\"hljs-built_in\">glDrawBuffer</span>(GL_NONE);<br><span class=\"hljs-built_in\">glReadBuffer</span>(GL_NONE);<br><span class=\"hljs-built_in\">glBindFramebuffer</span>(GL_FRAMEBUFFER, <span class=\"hljs-number\">0</span>);<br></code></pre></td></tr></table></figure>\n\n<p>除此之外，我们需要一次性渲染多张贴图，我们参考点光源阴影贴图使用geometry shader的做法，将顶点映射到不同的视椎体分段的光源的投影。代码如下：</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-meta\">#version 450 compatibility</span><br><span class=\"hljs-keyword\">layout</span>(<span class=\"hljs-keyword\">triangles</span>, <span class=\"hljs-keyword\">invocations</span> = <span class=\"hljs-number\">6</span>) <span class=\"hljs-keyword\">in</span>;<br><span class=\"hljs-keyword\">layout</span>(<span class=\"hljs-keyword\">triangle_strip</span>, <span class=\"hljs-keyword\">max_vertices</span> = <span class=\"hljs-number\">3</span>) <span class=\"hljs-keyword\">out</span>;<br><br><span class=\"hljs-keyword\">uniform</span> <span class=\"hljs-type\">mat4</span> light_space_matrix[<span class=\"hljs-number\">16</span>];<br><br><br><span class=\"hljs-type\">void</span> main()<br>&#123;<br>\t<span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> i = <span class=\"hljs-number\">0</span>; i &lt; <span class=\"hljs-number\">3</span>; ++i)<br>\t&#123;<br>\t\t<span class=\"hljs-built_in\">gl_Position</span> = light_space_matrix[<span class=\"hljs-built_in\">gl_InvocationID</span>] * <span class=\"hljs-built_in\">gl_in</span>[i].<span class=\"hljs-built_in\">gl_Position</span>;<br>\t\t<span class=\"hljs-built_in\">gl_Layer</span> = <span class=\"hljs-built_in\">gl_InvocationID</span>;<br>\t\t<span class=\"hljs-built_in\">EmitVertex</span>();<br>\t&#125;<br>\t<span class=\"hljs-built_in\">EndPrimitive</span>();<br>&#125; <br></code></pre></td></tr></table></figure>\n\n<p>这里新增的<strong>invocations &#x3D; 6</strong>代表了这个Shader可以被实例化，每个实例同时平行进行运算，实例的个数为6。内置的<strong>gl_InvocationID</strong>代表了当前处理的是哪一个实例，我们将其赋值到<strong>gl_Layer</strong>。其余的阴影贴图渲染步骤和普通的阴影贴图类似。</p>\n<p>下面几张图所示展示的，就是从近到远的几个级联阴影贴图的表现：<br><img src=\"/2024/10/13/cascade-shadow-map/csm_near.png\"><br><img src=\"/2024/10/13/cascade-shadow-map/csm_mid.png\"><br><img src=\"/2024/10/13/cascade-shadow-map/csm_far.png\"></p>\n<h2 id=\"使用级联阴影贴图\"><a href=\"#使用级联阴影贴图\" class=\"headerlink\" title=\"使用级联阴影贴图\"></a>使用级联阴影贴图</h2><p>级联阴影贴图的使用和阴影贴图是类似的，由于传入给光照Shader的是GL_TEXTURE_2D_ARRAY，需要使用vec3来索引贴图数据的，vec3的z值代表的是Layer索引。</p>\n<p>Layer代表的是使用哪一个视椎体分段的阴影贴图，取决于当前像素和相机的距离。取得对应的Layer参数后带入texcoord的z值读取对应的阴影贴图的值。示例代码如下：</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-comment\">// 计算阴影</span><br><span class=\"hljs-type\">float</span> ShadowCalculation_DirLight(<span class=\"hljs-type\">vec4</span> frag_world_pos, <span class=\"hljs-type\">vec3</span> to_light_dir, <span class=\"hljs-type\">vec3</span> in_normal)<br>&#123;<br>    <span class=\"hljs-comment\">// 获取像素和相机的距离，也就是view转换后的z值</span><br>    <span class=\"hljs-type\">vec4</span> frag_pos_view_space = matrix_ubo.view * frag_world_pos;<br>    <span class=\"hljs-type\">float</span> depthValue = <span class=\"hljs-built_in\">abs</span>(frag_pos_view_space.z);<br><br>    <span class=\"hljs-comment\">// 根据距离和每段视椎体分段的距离区间，获取Layer值</span><br>    <span class=\"hljs-type\">int</span> layer = <span class=\"hljs-number\">-1</span>;<br>    <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> i = <span class=\"hljs-number\">0</span>; i &lt; csm_level_count; ++i)<br>    &#123;<br>        <span class=\"hljs-keyword\">if</span> (depthValue &lt; csm_distances[i])<br>        &#123;<br>            layer = i;<br>            <span class=\"hljs-keyword\">break</span>;<br>        &#125;<br>    &#125;<br>    <span class=\"hljs-keyword\">if</span> (layer == <span class=\"hljs-number\">-1</span>)<br>    &#123;<br>        layer = csm_level_count;<br>    &#125;<br>    <span class=\"hljs-comment\">// 下面的和应用普通阴影贴图的一致</span><br>    <span class=\"hljs-comment\">// 转换到-1,1的范围，再转到0,1的范围</span><br>    <span class=\"hljs-type\">vec4</span> frag_pos_light_space = light_space_matrices[layer] * frag_world_pos;<br>    <span class=\"hljs-comment\">// perform perspective divide</span><br>    <span class=\"hljs-type\">vec3</span> proj_coord = frag_pos_light_space.xyz / frag_pos_light_space.w;<br>    <span class=\"hljs-comment\">// transform to [0,1] range</span><br>    proj_coord = proj_coord * <span class=\"hljs-number\">0.5</span> + <span class=\"hljs-number\">0.5</span>;<br><br>    <span class=\"hljs-comment\">// get depth of current fragment from light&#x27;s perspective</span><br>    <span class=\"hljs-type\">float</span> current_depth = proj_coord.z;<br><br>    <span class=\"hljs-comment\">// keep the shadow at 0.0 when outside the far_plane region of the light&#x27;s frustum.</span><br>    <span class=\"hljs-keyword\">if</span> (current_depth &gt; <span class=\"hljs-number\">1.0</span>)<br>    &#123;<br>        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-number\">0.0</span>;<br>    &#125;<br><br>    <span class=\"hljs-comment\">// PCF</span><br>    <span class=\"hljs-type\">float</span> shadow = <span class=\"hljs-number\">0.0</span>;<br>    <span class=\"hljs-type\">vec2</span> texel_size = <span class=\"hljs-number\">1.0</span> / <span class=\"hljs-type\">vec2</span>(<span class=\"hljs-built_in\">textureSize</span>(shadow_map, <span class=\"hljs-number\">0</span>));<br>    <span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">int</span> x = <span class=\"hljs-number\">-1</span>; x &lt;= <span class=\"hljs-number\">1</span>; ++x)<br>    &#123;<br>        <span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">int</span> y = <span class=\"hljs-number\">-1</span>; y &lt;= <span class=\"hljs-number\">1</span>; ++y)<br>        &#123;<br>            <span class=\"hljs-type\">float</span> pcf_depth = <span class=\"hljs-built_in\">texture</span>(shadow_map, <span class=\"hljs-type\">vec3</span>(proj_coord.xy + <span class=\"hljs-type\">vec2</span>(x, y) * texel_size, layer)).r;<br>            shadow += current_depth &gt; pcf_depth ? <span class=\"hljs-number\">1.0</span> : <span class=\"hljs-number\">0.0</span>;<br>        &#125;<br>    &#125;<br>    shadow /= <span class=\"hljs-number\">9.0</span>;<br>        <br>    <span class=\"hljs-keyword\">return</span> shadow;<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<h1 id=\"效果对比\"><a href=\"#效果对比\" class=\"headerlink\" title=\"效果对比\"></a>效果对比</h1><h2 id=\"原先的阴影贴图\"><a href=\"#原先的阴影贴图\" class=\"headerlink\" title=\"原先的阴影贴图\"></a>原先的阴影贴图</h2><p>原先的阴影贴图只能覆盖有限的场景：<br><img src=\"/2024/10/13/cascade-shadow-map/sm_near.png\"></p>\n<p>提升覆盖范围后，阴影的质量则会出现下降：<br><img src=\"/2024/10/13/cascade-shadow-map/sm_far.png\"></p>\n<h2 id=\"级联阴影贴图\"><a href=\"#级联阴影贴图\" class=\"headerlink\" title=\"级联阴影贴图\"></a>级联阴影贴图</h2><p>采用级联阴影贴图可以覆盖很大的场景，并且在可控的性能消耗下仍然有不错的显示智联。<br><img src=\"/2024/10/13/cascade-shadow-map/csm_result.png\"></p>\n","excerpt":"","more":"<h1 id=\"阴影贴图的局限\"><a href=\"#阴影贴图的局限\" class=\"headerlink\" title=\"阴影贴图的局限\"></a>阴影贴图的局限</h1><p>阴影贴图（shadow map）是3D场景中实现阴影效果的基础手段，它通过预先将光线方向的场景深度存储到贴图中，在渲染的时候取每个场景中的点到光源的距离和深度贴图作比较，来判定该点是否在阴影当中。</p>\n<p>但是在较大的场景中，使用阴影贴图会有几个明显的不足：</p>\n<ol>\n<li>阴影贴图只能覆盖部分场景，在渲染较大的场景的时候（如大世界），远处的场景基本上无法被阴影贴图所覆盖。</li>\n<li>贴图的分辨率是有限的，太大的分辨率会对性能造成非常大的影响。但是在覆盖较大场景的时候，贴图分辨率不足会导致阴影模糊，效果不佳。</li>\n<li>阴影贴图的实现一开始其实并没有考虑玩家相机的视椎体，也就是说在玩家没有看的地方也会渲染阴影贴图，这对渲染资源来说显然是个浪费。</li>\n</ol>\n<p>KongEngine计划在后面接入大地形的渲染，借此机会接入了级联阴影贴图的能力。<br>实现方法参考了<a href=\"https://learnopengl.com/Guest-Articles/2021/CSM\">LearnOpenGL的教程</a>。</p>\n<h1 id=\"级联阴影贴图的实现\"><a href=\"#级联阴影贴图的实现\" class=\"headerlink\" title=\"级联阴影贴图的实现\"></a>级联阴影贴图的实现</h1><p>级联阴影贴图的基本概念包括如下几点：</p>\n<ol>\n<li>将玩家的视椎体划分为几段，每一段视椎体构建一张阴影贴图覆盖，这个阴影贴图完美贴合从光源方向投射到这段视椎体中心点的正交投影。</li>\n<li>和模型LOD的理念类似，离相机近的阴影贴图需要采用较高精度，而离相机远的阴影贴图可以使用低精度。</li>\n<li>将多级阴影贴图传入最后的光照计算着色器，根据每个点所处视椎体的分段不同采用对应不同的阴影贴图计算光照。</li>\n</ol>\n<p>听起来挺简单的对吧，那我们一步一步来。</p>\n<h2 id=\"视椎体分段\"><a href=\"#视椎体分段\" class=\"headerlink\" title=\"视椎体分段\"></a>视椎体分段</h2><p>上面说到我们需要将视椎体分为几段，在每一段视椎体覆盖一张阴影贴图，并计算出这张贴图的从光源方向看向视椎体中心点的正交投影的矩阵，也就是Light projection matrix和Light view matrix。这个矩阵需要紧密贴合这段视椎体，为此我们需要得到视椎体的顶点的世界坐标，得到顶点的min、max和视椎体的中心点。</p>\n<p>我们从相机的视椎体出发，当处于视椎体范围上的顶点的世界坐标经过projection矩阵和view矩阵转换后，xyz都会被映射到[-1,1]范围的屏幕空间坐标。矩阵转换是可逆的，也就是说取屏幕空间坐标为[-1, 1]边界的八个顶点，经过视椎体的projection矩阵和view矩阵的逆矩阵转换后，就能得到边界顶点的世界空间坐标。在代码里面的实现如下：</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c++\"><span class=\"hljs-function\">std::vector&lt;glm::vec4&gt; <span class=\"hljs-title\">CDirectionalLightComponent::GetFrustumCornersWorldSpace</span><span class=\"hljs-params\">(<span class=\"hljs-type\">const</span> glm::mat4&amp; proj_view)</span></span><br><span class=\"hljs-function\"></span>&#123;<br>    <span class=\"hljs-type\">const</span> <span class=\"hljs-keyword\">auto</span> inv = glm::<span class=\"hljs-built_in\">inverse</span>(proj_view);<br><br>    <span class=\"hljs-comment\">// 顶点的世界坐标在projection和view matrix的转换下的坐标范围是[-1,1]</span><br>    <span class=\"hljs-comment\">// 那么将在[-1,1]这个边界的八个顶点坐标乘以projection和view matrix的逆矩阵则可以得到视锥体边界的顶点的世界坐标</span><br>    vector&lt;vec4&gt; frustum_corners;<br>    <span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">unsigned</span> <span class=\"hljs-type\">int</span> i = <span class=\"hljs-number\">0</span>; i &lt; <span class=\"hljs-number\">2</span>; i++)<br>    &#123;<br>        <span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">unsigned</span> <span class=\"hljs-type\">int</span> j = <span class=\"hljs-number\">0</span>; j &lt; <span class=\"hljs-number\">2</span>; j++)<br>        &#123;<br>            <span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">unsigned</span> <span class=\"hljs-type\">int</span> k = <span class=\"hljs-number\">0</span>; k &lt; <span class=\"hljs-number\">2</span>; k++)<br>            &#123;<br>                <span class=\"hljs-type\">const</span> vec4 pt = inv * <span class=\"hljs-built_in\">vec4</span>(<span class=\"hljs-number\">2.0f</span>*i<span class=\"hljs-number\">-1.0f</span>,<span class=\"hljs-number\">2.0f</span>*j<span class=\"hljs-number\">-1.0f</span>,<span class=\"hljs-number\">2.0f</span>*k<span class=\"hljs-number\">-1.0f</span>, <span class=\"hljs-number\">1.0f</span>);<br>                frustum_corners.<span class=\"hljs-built_in\">push_back</span>(pt / pt.w);<br>            &#125;<br>        &#125;   <br>    &#125;<br>    <br>    <span class=\"hljs-keyword\">return</span> frustum_corners;<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<p>我们得到了视椎体角落的顶点世界坐标，我们希望阴影贴图能够如下图一般贴合每一段视椎体。那么我们需要计算视椎体的中心顶点坐标，中心顶点在计算view矩阵的时候需要用到；我们需要计算在xyz轴上顶点坐标的最大和最小值，这些数值在计算projection矩阵的时候会被用到。</p>\n<p><img src=\"https://learnopengl.com/img/guest/2021/CSM/frustum_fitting.png\" alt=\"级联阴影贴图由远及近\"></p>\n<p>计算中心点的代码十分简单，将视椎体角落的坐标相加后再除以数量即可，代码如下：</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c++\">vec3 center = <span class=\"hljs-built_in\">vec3</span>(<span class=\"hljs-number\">0.0f</span>);<br><span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">const</span> <span class=\"hljs-keyword\">auto</span>&amp; v : corners)<br>&#123;<br>    center += <span class=\"hljs-built_in\">vec3</span>(v);<br>&#125;<br>center /= corners.<span class=\"hljs-built_in\">size</span>();   <span class=\"hljs-comment\">// 获取视锥体的中心点</span><br><br><span class=\"hljs-type\">const</span> <span class=\"hljs-keyword\">auto</span> light_view = <span class=\"hljs-built_in\">lookAt</span>(center-light_dir, center, <span class=\"hljs-built_in\">vec3</span>(<span class=\"hljs-number\">0.0f</span>, <span class=\"hljs-number\">1.0f</span>, <span class=\"hljs-number\">0.0f</span>));<br></code></pre></td></tr></table></figure>\n\n<p>计算贴合视椎体的范围则是比较并记录各个顶点在xyz轴的最大值和最小值，方法如下。这里提一下在z轴方向和一个参数z_mult进行了处理，其意义是阴影的投射源是有可能在视椎体范围之外的，如果不考虑这一部分的影响的话可能在阴影过度的时候会非常生硬，并且丢掉一些本来该显示的阴影导致渲染错误。</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c++\"><span class=\"hljs-type\">float</span> min_x = std::numeric_limits&lt;<span class=\"hljs-type\">float</span>&gt;::<span class=\"hljs-built_in\">max</span>();<br><span class=\"hljs-type\">float</span> min_y = std::numeric_limits&lt;<span class=\"hljs-type\">float</span>&gt;::<span class=\"hljs-built_in\">max</span>();<br><span class=\"hljs-type\">float</span> min_z = std::numeric_limits&lt;<span class=\"hljs-type\">float</span>&gt;::<span class=\"hljs-built_in\">max</span>();<br><span class=\"hljs-type\">float</span> max_x = std::numeric_limits&lt;<span class=\"hljs-type\">float</span>&gt;::<span class=\"hljs-built_in\">lowest</span>();<br><span class=\"hljs-type\">float</span> max_y = std::numeric_limits&lt;<span class=\"hljs-type\">float</span>&gt;::<span class=\"hljs-built_in\">lowest</span>();<br><span class=\"hljs-type\">float</span> max_z = std::numeric_limits&lt;<span class=\"hljs-type\">float</span>&gt;::<span class=\"hljs-built_in\">lowest</span>();<br><span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">const</span> <span class=\"hljs-keyword\">auto</span>&amp; v : corners)<br>&#123;<br>    <span class=\"hljs-type\">const</span> <span class=\"hljs-keyword\">auto</span> trf = light_view * v;<br>    min_x = std::<span class=\"hljs-built_in\">min</span>(min_x, trf.x);<br>    max_x = std::<span class=\"hljs-built_in\">max</span>(max_x, trf.x);<br>    min_y = std::<span class=\"hljs-built_in\">min</span>(min_y, trf.y);<br>    max_y = std::<span class=\"hljs-built_in\">max</span>(max_y, trf.y);<br>    min_z = std::<span class=\"hljs-built_in\">min</span>(min_z, trf.z);<br>    max_z = std::<span class=\"hljs-built_in\">max</span>(max_z, trf.z);<br>&#125;<br><span class=\"hljs-keyword\">constexpr</span> <span class=\"hljs-type\">float</span> z_mult = <span class=\"hljs-number\">10.0f</span>;<br><span class=\"hljs-keyword\">if</span> (min_z &lt; <span class=\"hljs-number\">0</span>)<br>&#123;<br>    min_z *= z_mult;<br>&#125;<br><span class=\"hljs-keyword\">else</span><br>&#123;<br>    min_z /= z_mult;<br>&#125;<br><span class=\"hljs-keyword\">if</span> (max_z &lt; <span class=\"hljs-number\">0</span>)<br>&#123;<br>    max_z /= z_mult;<br>&#125;<br><span class=\"hljs-keyword\">else</span><br>&#123;<br>    max_z *= z_mult;<br>&#125;<br><br><span class=\"hljs-type\">const</span> mat4 light_projection = <span class=\"hljs-built_in\">ortho</span>(min_x, max_x, min_y, max_y, min_z, max_z);<br></code></pre></td></tr></table></figure>\n\n<h2 id=\"计算级联阴影贴图\"><a href=\"#计算级联阴影贴图\" class=\"headerlink\" title=\"计算级联阴影贴图\"></a>计算级联阴影贴图</h2><p>一般的阴影贴图我们采用的是GL_TEXTURE_2D，而级联阴影贴图我们需要传入多张贴图，因此对应的贴图类型会变为GL_TEXTURE_2D_ARRAY。</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c++\"><span class=\"hljs-built_in\">glGenTextures</span>(<span class=\"hljs-number\">1</span>, &amp;csm_texture);<br><span class=\"hljs-built_in\">glBindTexture</span>(GL_TEXTURE_2D_ARRAY, csm_texture);<br><span class=\"hljs-built_in\">glTexImage3D</span>(GL_TEXTURE_2D_ARRAY, <span class=\"hljs-number\">0</span>, GL_DEPTH_COMPONENT32F, SHADOW_RESOLUTION, SHADOW_RESOLUTION, (<span class=\"hljs-type\">int</span>)csm_distances.<span class=\"hljs-built_in\">size</span>()<span class=\"hljs-number\">+1</span>, <span class=\"hljs-number\">0</span>, GL_DEPTH_COMPONENT, GL_FLOAT, <span class=\"hljs-literal\">nullptr</span>);<br><span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D_ARRAY, GL_TEXTURE_MAG_FILTER, GL_NEAREST);<br><span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D_ARRAY, GL_TEXTURE_MIN_FILTER, GL_NEAREST);<br><span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D_ARRAY, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_BORDER);<br><span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D_ARRAY, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_BORDER);<br><span class=\"hljs-built_in\">glTexParameterfv</span>(GL_TEXTURE_2D_ARRAY, GL_TEXTURE_BORDER_COLOR, border_color);<br><br><span class=\"hljs-built_in\">glBindFramebuffer</span>(GL_FRAMEBUFFER, shadowmap_fbo);<br><span class=\"hljs-built_in\">glFramebufferTexture</span>(GL_FRAMEBUFFER, GL_DEPTH_ATTACHMENT, csm_texture, <span class=\"hljs-number\">0</span>);<br><span class=\"hljs-built_in\">glDrawBuffer</span>(GL_NONE);<br><span class=\"hljs-built_in\">glReadBuffer</span>(GL_NONE);<br><span class=\"hljs-built_in\">glBindFramebuffer</span>(GL_FRAMEBUFFER, <span class=\"hljs-number\">0</span>);<br></code></pre></td></tr></table></figure>\n\n<p>除此之外，我们需要一次性渲染多张贴图，我们参考点光源阴影贴图使用geometry shader的做法，将顶点映射到不同的视椎体分段的光源的投影。代码如下：</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-meta\">#version 450 compatibility</span><br><span class=\"hljs-keyword\">layout</span>(<span class=\"hljs-keyword\">triangles</span>, <span class=\"hljs-keyword\">invocations</span> = <span class=\"hljs-number\">6</span>) <span class=\"hljs-keyword\">in</span>;<br><span class=\"hljs-keyword\">layout</span>(<span class=\"hljs-keyword\">triangle_strip</span>, <span class=\"hljs-keyword\">max_vertices</span> = <span class=\"hljs-number\">3</span>) <span class=\"hljs-keyword\">out</span>;<br><br><span class=\"hljs-keyword\">uniform</span> <span class=\"hljs-type\">mat4</span> light_space_matrix[<span class=\"hljs-number\">16</span>];<br><br><br><span class=\"hljs-type\">void</span> main()<br>&#123;<br>\t<span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> i = <span class=\"hljs-number\">0</span>; i &lt; <span class=\"hljs-number\">3</span>; ++i)<br>\t&#123;<br>\t\t<span class=\"hljs-built_in\">gl_Position</span> = light_space_matrix[<span class=\"hljs-built_in\">gl_InvocationID</span>] * <span class=\"hljs-built_in\">gl_in</span>[i].<span class=\"hljs-built_in\">gl_Position</span>;<br>\t\t<span class=\"hljs-built_in\">gl_Layer</span> = <span class=\"hljs-built_in\">gl_InvocationID</span>;<br>\t\t<span class=\"hljs-built_in\">EmitVertex</span>();<br>\t&#125;<br>\t<span class=\"hljs-built_in\">EndPrimitive</span>();<br>&#125; <br></code></pre></td></tr></table></figure>\n\n<p>这里新增的<strong>invocations &#x3D; 6</strong>代表了这个Shader可以被实例化，每个实例同时平行进行运算，实例的个数为6。内置的<strong>gl_InvocationID</strong>代表了当前处理的是哪一个实例，我们将其赋值到<strong>gl_Layer</strong>。其余的阴影贴图渲染步骤和普通的阴影贴图类似。</p>\n<p>下面几张图所示展示的，就是从近到远的几个级联阴影贴图的表现：<br><img src=\"/2024/10/13/cascade-shadow-map/csm_near.png\"><br><img src=\"/2024/10/13/cascade-shadow-map/csm_mid.png\"><br><img src=\"/2024/10/13/cascade-shadow-map/csm_far.png\"></p>\n<h2 id=\"使用级联阴影贴图\"><a href=\"#使用级联阴影贴图\" class=\"headerlink\" title=\"使用级联阴影贴图\"></a>使用级联阴影贴图</h2><p>级联阴影贴图的使用和阴影贴图是类似的，由于传入给光照Shader的是GL_TEXTURE_2D_ARRAY，需要使用vec3来索引贴图数据的，vec3的z值代表的是Layer索引。</p>\n<p>Layer代表的是使用哪一个视椎体分段的阴影贴图，取决于当前像素和相机的距离。取得对应的Layer参数后带入texcoord的z值读取对应的阴影贴图的值。示例代码如下：</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-comment\">// 计算阴影</span><br><span class=\"hljs-type\">float</span> ShadowCalculation_DirLight(<span class=\"hljs-type\">vec4</span> frag_world_pos, <span class=\"hljs-type\">vec3</span> to_light_dir, <span class=\"hljs-type\">vec3</span> in_normal)<br>&#123;<br>    <span class=\"hljs-comment\">// 获取像素和相机的距离，也就是view转换后的z值</span><br>    <span class=\"hljs-type\">vec4</span> frag_pos_view_space = matrix_ubo.view * frag_world_pos;<br>    <span class=\"hljs-type\">float</span> depthValue = <span class=\"hljs-built_in\">abs</span>(frag_pos_view_space.z);<br><br>    <span class=\"hljs-comment\">// 根据距离和每段视椎体分段的距离区间，获取Layer值</span><br>    <span class=\"hljs-type\">int</span> layer = <span class=\"hljs-number\">-1</span>;<br>    <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> i = <span class=\"hljs-number\">0</span>; i &lt; csm_level_count; ++i)<br>    &#123;<br>        <span class=\"hljs-keyword\">if</span> (depthValue &lt; csm_distances[i])<br>        &#123;<br>            layer = i;<br>            <span class=\"hljs-keyword\">break</span>;<br>        &#125;<br>    &#125;<br>    <span class=\"hljs-keyword\">if</span> (layer == <span class=\"hljs-number\">-1</span>)<br>    &#123;<br>        layer = csm_level_count;<br>    &#125;<br>    <span class=\"hljs-comment\">// 下面的和应用普通阴影贴图的一致</span><br>    <span class=\"hljs-comment\">// 转换到-1,1的范围，再转到0,1的范围</span><br>    <span class=\"hljs-type\">vec4</span> frag_pos_light_space = light_space_matrices[layer] * frag_world_pos;<br>    <span class=\"hljs-comment\">// perform perspective divide</span><br>    <span class=\"hljs-type\">vec3</span> proj_coord = frag_pos_light_space.xyz / frag_pos_light_space.w;<br>    <span class=\"hljs-comment\">// transform to [0,1] range</span><br>    proj_coord = proj_coord * <span class=\"hljs-number\">0.5</span> + <span class=\"hljs-number\">0.5</span>;<br><br>    <span class=\"hljs-comment\">// get depth of current fragment from light&#x27;s perspective</span><br>    <span class=\"hljs-type\">float</span> current_depth = proj_coord.z;<br><br>    <span class=\"hljs-comment\">// keep the shadow at 0.0 when outside the far_plane region of the light&#x27;s frustum.</span><br>    <span class=\"hljs-keyword\">if</span> (current_depth &gt; <span class=\"hljs-number\">1.0</span>)<br>    &#123;<br>        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-number\">0.0</span>;<br>    &#125;<br><br>    <span class=\"hljs-comment\">// PCF</span><br>    <span class=\"hljs-type\">float</span> shadow = <span class=\"hljs-number\">0.0</span>;<br>    <span class=\"hljs-type\">vec2</span> texel_size = <span class=\"hljs-number\">1.0</span> / <span class=\"hljs-type\">vec2</span>(<span class=\"hljs-built_in\">textureSize</span>(shadow_map, <span class=\"hljs-number\">0</span>));<br>    <span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">int</span> x = <span class=\"hljs-number\">-1</span>; x &lt;= <span class=\"hljs-number\">1</span>; ++x)<br>    &#123;<br>        <span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">int</span> y = <span class=\"hljs-number\">-1</span>; y &lt;= <span class=\"hljs-number\">1</span>; ++y)<br>        &#123;<br>            <span class=\"hljs-type\">float</span> pcf_depth = <span class=\"hljs-built_in\">texture</span>(shadow_map, <span class=\"hljs-type\">vec3</span>(proj_coord.xy + <span class=\"hljs-type\">vec2</span>(x, y) * texel_size, layer)).r;<br>            shadow += current_depth &gt; pcf_depth ? <span class=\"hljs-number\">1.0</span> : <span class=\"hljs-number\">0.0</span>;<br>        &#125;<br>    &#125;<br>    shadow /= <span class=\"hljs-number\">9.0</span>;<br>        <br>    <span class=\"hljs-keyword\">return</span> shadow;<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<h1 id=\"效果对比\"><a href=\"#效果对比\" class=\"headerlink\" title=\"效果对比\"></a>效果对比</h1><h2 id=\"原先的阴影贴图\"><a href=\"#原先的阴影贴图\" class=\"headerlink\" title=\"原先的阴影贴图\"></a>原先的阴影贴图</h2><p>原先的阴影贴图只能覆盖有限的场景：<br><img src=\"/2024/10/13/cascade-shadow-map/sm_near.png\"></p>\n<p>提升覆盖范围后，阴影的质量则会出现下降：<br><img src=\"/2024/10/13/cascade-shadow-map/sm_far.png\"></p>\n<h2 id=\"级联阴影贴图\"><a href=\"#级联阴影贴图\" class=\"headerlink\" title=\"级联阴影贴图\"></a>级联阴影贴图</h2><p>采用级联阴影贴图可以覆盖很大的场景，并且在可控的性能消耗下仍然有不错的显示智联。<br><img src=\"/2024/10/13/cascade-shadow-map/csm_result.png\"></p>\n"},{"title":"延迟渲染实现","date":"2024-10-19T09:18:41.000Z","index_img":"/2024/10/19/defer-render/defer_render_banner.png","banner_img":"/2024/10/19/defer-render/defer_render_banner.png","_content":"\n\n想要在Kong引擎里面实现的场景慢慢复杂了起来，光源和模型的数量从原先的十以内的数量增长到几十甚至几百的数量级，是时候接入延迟渲染的方法了。\n\n# 延迟渲染\n\n\n**延迟渲染**（Defer Rendering），或者**延迟着色法**（Defer Shading），是区别于**正向渲染**（Forward Shading）的一种计算场景光照的方式。\n\n正向渲染方法就是遍历场景中的每一个模型，计算一个模型的光照表现后再继续下一个模型的计算，根据深度测试的结果更新屏幕上最终像素显示的颜色。这种方法是很容易让人理解并实现的。但是当场景中的光照和模型数量变多的时候，模型重叠的区域会进行不必要的光照计算（被挡住的模型像素区域最终会被前面的模型遮挡，但是这篇被挡住的区域还是被计算了光照），而光照计算一般来说是渲染消耗的大头，这部分时间就被浪费了。\n\n而延迟渲染的想法则是将光照计算分成两部分。第一个部分叫做<strong>几何处理阶段</strong>（Geometry Pass），它先将光照计算所需要的模型信息（顶点位置、法线、颜色、材质属性等等）先渲染到多张贴图上（消耗低），经由深度检测保留最终在屏幕上显示的模型部分的这些信息。\n\n<!-- wp:image {\"sizeSlug\":\"large\",\"align\":\"center\"} -->\n<figure class=\"wp-block-image aligncenter size-large\"><img src=\"https://learnopengl-cn.github.io/img/05/08/deferred_g_buffer.png\" alt=\"\"/></figure>\n<!-- /wp:image -->\n\n\n第二部分叫做<strong>光照处理阶段</strong>（Lighting Pass），根据几何处理阶段保存的信息再去进行光照计算，这样就不会将算力浪费在计算被遮挡的模型部分的光照了，从而优化渲染的性能，也有赋予了能够更加方便的实现某些效果的能力（如SSAO）。\n\n<!-- wp:image {\"sizeSlug\":\"large\",\"align\":\"center\"} -->\n<figure class=\"wp-block-image aligncenter size-large\"><img src=\"https://learnopengl-cn.github.io/img/05/08/deferred_overview.png\" alt=\"\"/></figure>\n<!-- /wp:image -->\n\n# G缓冲\nG缓冲(G-buffer)是对所有用来储存光照相关的数据，并在最后的光照处理阶段中使用的所有纹理的总称。它是我们计算最终渲染输出中的缓存和中转站，为了实现延迟渲染，G-buffer中会包含如下几张纹理的数据：模型顶点位置数据；模型法线数据；模型漫反射颜色数据；材质数据（ao，roughness，metallic）等等。有了这些数据则能够实现Kong引擎的PBR光照计算，初始化G-buffer的代码如下：\n\n```c++\nvoid DeferBuffer::GenerateDeferRenderTextures(int width, int height)\n{\n\tglBindFramebuffer(GL_FRAMEBUFFER, g_buffer_);\n\n\t// 将当前视野的数据用贴图缓存\n\t// 位置数据\n\tglGenTextures(1, &g_position_);\n\tglBindTexture(GL_TEXTURE_2D, g_position_);\n\tglTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA32F, width, height, 0, GL_RGBA, GL_FLOAT, NULL);\n\tglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);\n\tglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);\n\tglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);\n\tglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);\n\tglFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, g_position_, 0);\n\n\t// 法线数据\n\tglGenTextures(1, &g_normal_);\n\tglBindTexture(GL_TEXTURE_2D, g_normal_);\n\tglTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA32F, width, height, 0, GL_RGBA, GL_FLOAT, NULL);\n\tglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);\n\tglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);\n\tglFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT1, GL_TEXTURE_2D, g_normal_, 0);\n\n\t// 顶点颜色数据\n\tglGenTextures(1, &g_albedo_);\n\tglBindTexture(GL_TEXTURE_2D, g_albedo_);\n\tglTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA, width, height, 0, GL_RGBA, GL_UNSIGNED_INT, NULL);\n\tglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);\n\tglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);\n\tglFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT2, GL_TEXTURE_2D, g_albedo_, 0);\n\n\t// orm数据（ao，roughness，metallic）\n\tglGenTextures(1, &g_orm_);\n\tglBindTexture(GL_TEXTURE_2D, g_orm_);\n\tglTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA, width, height, 0, GL_RGBA, GL_UNSIGNED_INT, NULL);\n\tglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);\n\tglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);\n\tglFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT3, GL_TEXTURE_2D, g_orm_, 0);\n\n\t// 生成renderbuffer\n\tglGenRenderbuffers(1, &g_rbo_);\n\tglBindRenderbuffer(GL_RENDERBUFFER, g_rbo_);\n\tglRenderbufferStorage(GL_RENDERBUFFER, GL_DEPTH_COMPONENT, width, height);\n\tglFramebufferRenderbuffer(GL_FRAMEBUFFER, GL_DEPTH_ATTACHMENT, GL_RENDERBUFFER, g_rbo_);\n\tglEnable(GL_DEPTH_TEST);\n\t\n\tunsigned int attachments[4] = {GL_COLOR_ATTACHMENT0, GL_COLOR_ATTACHMENT1, GL_COLOR_ATTACHMENT2, GL_COLOR_ATTACHMENT3};\n\tglDrawBuffers(4, attachments);\n\tglBindFramebuffer(GL_FRAMEBUFFER, 0);\n}\n```\n\n\n可以从上面的代码看到，我们利用了多渲染目标（multiple render targets）可以一次处理并输出到多个缓冲（GL_COLOR_ATTACHMENT0到3）。简化的几何处理着色器示例代码如下：\n\n```glsl\n// defer_geometry_pass.frag\nlayout(location = 0) out vec4 gPosition;\nlayout(location = 1) out vec4 gNormal;\nlayout(location = 2) out vec4 gAlbedo;\nlayout(location = 3) out vec4 gORM;\n\nin vec4 frag_pos;\nin vec3 frag_normal;\nin vec2 frag_uv;\n\nuniform vec4 albedo;    // color\nuniform float metallic;\nuniform float roughness;\nuniform float ao;\n\nvoid main()\n{\n    // 深度信息存储到position贴图的w值中\n    gPosition = frag_pos;\n    gNormal = vec4(frag_normal, 1.0);\n    gAlbedo = albedo;\n    gORM = vec4(ao, roughness, metallic, 1.0);\n}\n```\n\n上方的代码将我们所需要的世界坐标下的顶点坐标信息、法线信息、漫反射颜色和材质信息输出到了四张贴图。带着这四张贴图的信息，我们进入下一个阶段，光照处理阶段。下面是个简化的光照处理着色器代码：\n\n```glsl\nvoid main()\n{\n    vec3 frag_pos = texture(position_texture, TexCoords).xyz;\n    vec3 frag_normal = texture(normal_texture, TexCoords).rgb;\n    vec4 env_albedo = texture(albedo_texture, TexCoords);\n\n    vec3 orm = texture(orm_texture, TexCoords).rgb;\n    float ao = orm.x;\n    float env_roughness = orm.y;\n    float env_metallic = orm.z;\n\n    vec3 view = normalize(cam_pos - frag_pos);  //to_view\n\n    vec3 light_color = CalcLight(light_info, frag_normal, view,  frag_pos, material);\n\n    vec3 color = ambient + light_color;\n    FragColor = vec4(color, 1.0);\n}\n```\n\n# 结合延迟和正向渲染\n\n延迟渲染实现起来其实还是比较简单明了的，但是需要注意的是，有些材质并不能通过延迟渲染实现，比如说半透明这种需要进行alpha混合的材质，因此就会出现需要结合延迟渲染和正向渲染的情况。\n\n\n结合延迟渲染和正向渲染的时候，一般来说是先处理延迟渲染的部分。在处理完延迟渲染后，将延迟渲染的G-buffer的深度缓冲复制到最后输出屏幕的深度缓冲上（我这里最后会继续后处理，所以是会输出到后处理的FrameBuffer上）。如此一来，正向渲染的物体才可以和延迟渲染的场景有正确的深度遮挡结合，否则会出现正向渲染的物体永远在上的情况。实例代码如下所示：\n\n```c++\n// 需要将延迟渲染的深度缓冲复制到后面的后处理buffer上\nglBindFramebuffer(GL_READ_FRAMEBUFFER, defer_buffer_.g_buffer_);\nglBindFramebuffer(GL_DRAW_FRAMEBUFFER, post_process.GetScreenFrameBuffer());\nglBlitFramebuffer(0, 0, window_size.x, window_size.y, 0, 0, window_size.x, window_size.y, GL_DEPTH_BUFFER_BIT, \nGL_NEAREST);\n```\n\n# 延迟渲染的效能提升\n之前提过，延迟渲染最大的好处之一便是能够提升渲染的效率，这里大概做一个粗略的测试。下方是一个包含着1000个人物模型和200个点光源的场景，如果按照正常的正向渲染，这个场景在我的笔记本上的帧率大概在35左右：\n\n![非延迟渲染](no_defer_render.png)\n\n当使用延迟渲染的情况下，该场景的帧率可以提升到170左右：\n\n![延迟渲染](defer_render.png)\n\n当然上方这是个比较极端的场景，实际场景上可能不会有这么复杂的光源，以及模型可能不会像测试场景这样重叠，所以差距可能不会像测试场景那般明显。但是一般来说延迟渲染对渲染场景的性能提升会是比较客观的。\n\n## 基于延迟渲染的延伸\n延迟渲染的好处之一不仅仅体现在性能上，由于延迟渲染将很多有用的信息存储下来，基于延迟渲染我们可以实现非常多其他的效果。比如说屏幕空间环境光遮蔽SSAO（如下图）以及屏幕空间反射SSR等等，我计划在后面的文章详细介绍一下。\n\n![SSAO效果](ssao.png)\n","source":"_posts/defer-render.md","raw":"---\ntitle: 延迟渲染实现\ndate: 2024-10-19 17:18:41\ncategories: \n\t- 技术漫谈\ntags: [3D, render, 渲染, 编程]\nindex_img: /2024/10/19/defer-render/defer_render_banner.png\nbanner_img: /2024/10/19/defer-render/defer_render_banner.png\n---\n\n\n想要在Kong引擎里面实现的场景慢慢复杂了起来，光源和模型的数量从原先的十以内的数量增长到几十甚至几百的数量级，是时候接入延迟渲染的方法了。\n\n# 延迟渲染\n\n\n**延迟渲染**（Defer Rendering），或者**延迟着色法**（Defer Shading），是区别于**正向渲染**（Forward Shading）的一种计算场景光照的方式。\n\n正向渲染方法就是遍历场景中的每一个模型，计算一个模型的光照表现后再继续下一个模型的计算，根据深度测试的结果更新屏幕上最终像素显示的颜色。这种方法是很容易让人理解并实现的。但是当场景中的光照和模型数量变多的时候，模型重叠的区域会进行不必要的光照计算（被挡住的模型像素区域最终会被前面的模型遮挡，但是这篇被挡住的区域还是被计算了光照），而光照计算一般来说是渲染消耗的大头，这部分时间就被浪费了。\n\n而延迟渲染的想法则是将光照计算分成两部分。第一个部分叫做<strong>几何处理阶段</strong>（Geometry Pass），它先将光照计算所需要的模型信息（顶点位置、法线、颜色、材质属性等等）先渲染到多张贴图上（消耗低），经由深度检测保留最终在屏幕上显示的模型部分的这些信息。\n\n<!-- wp:image {\"sizeSlug\":\"large\",\"align\":\"center\"} -->\n<figure class=\"wp-block-image aligncenter size-large\"><img src=\"https://learnopengl-cn.github.io/img/05/08/deferred_g_buffer.png\" alt=\"\"/></figure>\n<!-- /wp:image -->\n\n\n第二部分叫做<strong>光照处理阶段</strong>（Lighting Pass），根据几何处理阶段保存的信息再去进行光照计算，这样就不会将算力浪费在计算被遮挡的模型部分的光照了，从而优化渲染的性能，也有赋予了能够更加方便的实现某些效果的能力（如SSAO）。\n\n<!-- wp:image {\"sizeSlug\":\"large\",\"align\":\"center\"} -->\n<figure class=\"wp-block-image aligncenter size-large\"><img src=\"https://learnopengl-cn.github.io/img/05/08/deferred_overview.png\" alt=\"\"/></figure>\n<!-- /wp:image -->\n\n# G缓冲\nG缓冲(G-buffer)是对所有用来储存光照相关的数据，并在最后的光照处理阶段中使用的所有纹理的总称。它是我们计算最终渲染输出中的缓存和中转站，为了实现延迟渲染，G-buffer中会包含如下几张纹理的数据：模型顶点位置数据；模型法线数据；模型漫反射颜色数据；材质数据（ao，roughness，metallic）等等。有了这些数据则能够实现Kong引擎的PBR光照计算，初始化G-buffer的代码如下：\n\n```c++\nvoid DeferBuffer::GenerateDeferRenderTextures(int width, int height)\n{\n\tglBindFramebuffer(GL_FRAMEBUFFER, g_buffer_);\n\n\t// 将当前视野的数据用贴图缓存\n\t// 位置数据\n\tglGenTextures(1, &g_position_);\n\tglBindTexture(GL_TEXTURE_2D, g_position_);\n\tglTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA32F, width, height, 0, GL_RGBA, GL_FLOAT, NULL);\n\tglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);\n\tglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);\n\tglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);\n\tglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);\n\tglFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, g_position_, 0);\n\n\t// 法线数据\n\tglGenTextures(1, &g_normal_);\n\tglBindTexture(GL_TEXTURE_2D, g_normal_);\n\tglTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA32F, width, height, 0, GL_RGBA, GL_FLOAT, NULL);\n\tglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);\n\tglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);\n\tglFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT1, GL_TEXTURE_2D, g_normal_, 0);\n\n\t// 顶点颜色数据\n\tglGenTextures(1, &g_albedo_);\n\tglBindTexture(GL_TEXTURE_2D, g_albedo_);\n\tglTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA, width, height, 0, GL_RGBA, GL_UNSIGNED_INT, NULL);\n\tglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);\n\tglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);\n\tglFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT2, GL_TEXTURE_2D, g_albedo_, 0);\n\n\t// orm数据（ao，roughness，metallic）\n\tglGenTextures(1, &g_orm_);\n\tglBindTexture(GL_TEXTURE_2D, g_orm_);\n\tglTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA, width, height, 0, GL_RGBA, GL_UNSIGNED_INT, NULL);\n\tglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);\n\tglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);\n\tglFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT3, GL_TEXTURE_2D, g_orm_, 0);\n\n\t// 生成renderbuffer\n\tglGenRenderbuffers(1, &g_rbo_);\n\tglBindRenderbuffer(GL_RENDERBUFFER, g_rbo_);\n\tglRenderbufferStorage(GL_RENDERBUFFER, GL_DEPTH_COMPONENT, width, height);\n\tglFramebufferRenderbuffer(GL_FRAMEBUFFER, GL_DEPTH_ATTACHMENT, GL_RENDERBUFFER, g_rbo_);\n\tglEnable(GL_DEPTH_TEST);\n\t\n\tunsigned int attachments[4] = {GL_COLOR_ATTACHMENT0, GL_COLOR_ATTACHMENT1, GL_COLOR_ATTACHMENT2, GL_COLOR_ATTACHMENT3};\n\tglDrawBuffers(4, attachments);\n\tglBindFramebuffer(GL_FRAMEBUFFER, 0);\n}\n```\n\n\n可以从上面的代码看到，我们利用了多渲染目标（multiple render targets）可以一次处理并输出到多个缓冲（GL_COLOR_ATTACHMENT0到3）。简化的几何处理着色器示例代码如下：\n\n```glsl\n// defer_geometry_pass.frag\nlayout(location = 0) out vec4 gPosition;\nlayout(location = 1) out vec4 gNormal;\nlayout(location = 2) out vec4 gAlbedo;\nlayout(location = 3) out vec4 gORM;\n\nin vec4 frag_pos;\nin vec3 frag_normal;\nin vec2 frag_uv;\n\nuniform vec4 albedo;    // color\nuniform float metallic;\nuniform float roughness;\nuniform float ao;\n\nvoid main()\n{\n    // 深度信息存储到position贴图的w值中\n    gPosition = frag_pos;\n    gNormal = vec4(frag_normal, 1.0);\n    gAlbedo = albedo;\n    gORM = vec4(ao, roughness, metallic, 1.0);\n}\n```\n\n上方的代码将我们所需要的世界坐标下的顶点坐标信息、法线信息、漫反射颜色和材质信息输出到了四张贴图。带着这四张贴图的信息，我们进入下一个阶段，光照处理阶段。下面是个简化的光照处理着色器代码：\n\n```glsl\nvoid main()\n{\n    vec3 frag_pos = texture(position_texture, TexCoords).xyz;\n    vec3 frag_normal = texture(normal_texture, TexCoords).rgb;\n    vec4 env_albedo = texture(albedo_texture, TexCoords);\n\n    vec3 orm = texture(orm_texture, TexCoords).rgb;\n    float ao = orm.x;\n    float env_roughness = orm.y;\n    float env_metallic = orm.z;\n\n    vec3 view = normalize(cam_pos - frag_pos);  //to_view\n\n    vec3 light_color = CalcLight(light_info, frag_normal, view,  frag_pos, material);\n\n    vec3 color = ambient + light_color;\n    FragColor = vec4(color, 1.0);\n}\n```\n\n# 结合延迟和正向渲染\n\n延迟渲染实现起来其实还是比较简单明了的，但是需要注意的是，有些材质并不能通过延迟渲染实现，比如说半透明这种需要进行alpha混合的材质，因此就会出现需要结合延迟渲染和正向渲染的情况。\n\n\n结合延迟渲染和正向渲染的时候，一般来说是先处理延迟渲染的部分。在处理完延迟渲染后，将延迟渲染的G-buffer的深度缓冲复制到最后输出屏幕的深度缓冲上（我这里最后会继续后处理，所以是会输出到后处理的FrameBuffer上）。如此一来，正向渲染的物体才可以和延迟渲染的场景有正确的深度遮挡结合，否则会出现正向渲染的物体永远在上的情况。实例代码如下所示：\n\n```c++\n// 需要将延迟渲染的深度缓冲复制到后面的后处理buffer上\nglBindFramebuffer(GL_READ_FRAMEBUFFER, defer_buffer_.g_buffer_);\nglBindFramebuffer(GL_DRAW_FRAMEBUFFER, post_process.GetScreenFrameBuffer());\nglBlitFramebuffer(0, 0, window_size.x, window_size.y, 0, 0, window_size.x, window_size.y, GL_DEPTH_BUFFER_BIT, \nGL_NEAREST);\n```\n\n# 延迟渲染的效能提升\n之前提过，延迟渲染最大的好处之一便是能够提升渲染的效率，这里大概做一个粗略的测试。下方是一个包含着1000个人物模型和200个点光源的场景，如果按照正常的正向渲染，这个场景在我的笔记本上的帧率大概在35左右：\n\n![非延迟渲染](no_defer_render.png)\n\n当使用延迟渲染的情况下，该场景的帧率可以提升到170左右：\n\n![延迟渲染](defer_render.png)\n\n当然上方这是个比较极端的场景，实际场景上可能不会有这么复杂的光源，以及模型可能不会像测试场景这样重叠，所以差距可能不会像测试场景那般明显。但是一般来说延迟渲染对渲染场景的性能提升会是比较客观的。\n\n## 基于延迟渲染的延伸\n延迟渲染的好处之一不仅仅体现在性能上，由于延迟渲染将很多有用的信息存储下来，基于延迟渲染我们可以实现非常多其他的效果。比如说屏幕空间环境光遮蔽SSAO（如下图）以及屏幕空间反射SSR等等，我计划在后面的文章详细介绍一下。\n\n![SSAO效果](ssao.png)\n","slug":"defer-render","published":1,"updated":"2024-10-19T09:43:16.358Z","comments":1,"layout":"post","photos":[],"_id":"cm5ht44vj000g3457hmrd48k7","content":"<p>想要在Kong引擎里面实现的场景慢慢复杂了起来，光源和模型的数量从原先的十以内的数量增长到几十甚至几百的数量级，是时候接入延迟渲染的方法了。</p>\n<h1 id=\"延迟渲染\"><a href=\"#延迟渲染\" class=\"headerlink\" title=\"延迟渲染\"></a>延迟渲染</h1><p><strong>延迟渲染</strong>（Defer Rendering），或者<strong>延迟着色法</strong>（Defer Shading），是区别于<strong>正向渲染</strong>（Forward Shading）的一种计算场景光照的方式。</p>\n<p>正向渲染方法就是遍历场景中的每一个模型，计算一个模型的光照表现后再继续下一个模型的计算，根据深度测试的结果更新屏幕上最终像素显示的颜色。这种方法是很容易让人理解并实现的。但是当场景中的光照和模型数量变多的时候，模型重叠的区域会进行不必要的光照计算（被挡住的模型像素区域最终会被前面的模型遮挡，但是这篇被挡住的区域还是被计算了光照），而光照计算一般来说是渲染消耗的大头，这部分时间就被浪费了。</p>\n<p>而延迟渲染的想法则是将光照计算分成两部分。第一个部分叫做<strong>几何处理阶段</strong>（Geometry Pass），它先将光照计算所需要的模型信息（顶点位置、法线、颜色、材质属性等等）先渲染到多张贴图上（消耗低），经由深度检测保留最终在屏幕上显示的模型部分的这些信息。</p>\n<!-- wp:image {\"sizeSlug\":\"large\",\"align\":\"center\"} -->\n<figure class=\"wp-block-image aligncenter size-large\"><img src=\"https://learnopengl-cn.github.io/img/05/08/deferred_g_buffer.png\" alt=\"\"/></figure>\n<!-- /wp:image -->\n\n\n<p>第二部分叫做<strong>光照处理阶段</strong>（Lighting Pass），根据几何处理阶段保存的信息再去进行光照计算，这样就不会将算力浪费在计算被遮挡的模型部分的光照了，从而优化渲染的性能，也有赋予了能够更加方便的实现某些效果的能力（如SSAO）。</p>\n<!-- wp:image {\"sizeSlug\":\"large\",\"align\":\"center\"} -->\n<figure class=\"wp-block-image aligncenter size-large\"><img src=\"https://learnopengl-cn.github.io/img/05/08/deferred_overview.png\" alt=\"\"/></figure>\n<!-- /wp:image -->\n\n<h1 id=\"G缓冲\"><a href=\"#G缓冲\" class=\"headerlink\" title=\"G缓冲\"></a>G缓冲</h1><p>G缓冲(G-buffer)是对所有用来储存光照相关的数据，并在最后的光照处理阶段中使用的所有纹理的总称。它是我们计算最终渲染输出中的缓存和中转站，为了实现延迟渲染，G-buffer中会包含如下几张纹理的数据：模型顶点位置数据；模型法线数据；模型漫反射颜色数据；材质数据（ao，roughness，metallic）等等。有了这些数据则能够实现Kong引擎的PBR光照计算，初始化G-buffer的代码如下：</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c++\"><span class=\"hljs-function\"><span class=\"hljs-type\">void</span> <span class=\"hljs-title\">DeferBuffer::GenerateDeferRenderTextures</span><span class=\"hljs-params\">(<span class=\"hljs-type\">int</span> width, <span class=\"hljs-type\">int</span> height)</span></span><br><span class=\"hljs-function\"></span>&#123;<br>\t<span class=\"hljs-built_in\">glBindFramebuffer</span>(GL_FRAMEBUFFER, g_buffer_);<br><br>\t<span class=\"hljs-comment\">// 将当前视野的数据用贴图缓存</span><br>\t<span class=\"hljs-comment\">// 位置数据</span><br>\t<span class=\"hljs-built_in\">glGenTextures</span>(<span class=\"hljs-number\">1</span>, &amp;g_position_);<br>\t<span class=\"hljs-built_in\">glBindTexture</span>(GL_TEXTURE_2D, g_position_);<br>\t<span class=\"hljs-built_in\">glTexImage2D</span>(GL_TEXTURE_2D, <span class=\"hljs-number\">0</span>, GL_RGBA32F, width, height, <span class=\"hljs-number\">0</span>, GL_RGBA, GL_FLOAT, <span class=\"hljs-literal\">NULL</span>);<br>\t<span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);<br>\t<span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);<br>\t<span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);<br>\t<span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);<br>\t<span class=\"hljs-built_in\">glFramebufferTexture2D</span>(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, g_position_, <span class=\"hljs-number\">0</span>);<br><br>\t<span class=\"hljs-comment\">// 法线数据</span><br>\t<span class=\"hljs-built_in\">glGenTextures</span>(<span class=\"hljs-number\">1</span>, &amp;g_normal_);<br>\t<span class=\"hljs-built_in\">glBindTexture</span>(GL_TEXTURE_2D, g_normal_);<br>\t<span class=\"hljs-built_in\">glTexImage2D</span>(GL_TEXTURE_2D, <span class=\"hljs-number\">0</span>, GL_RGBA32F, width, height, <span class=\"hljs-number\">0</span>, GL_RGBA, GL_FLOAT, <span class=\"hljs-literal\">NULL</span>);<br>\t<span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);<br>\t<span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);<br>\t<span class=\"hljs-built_in\">glFramebufferTexture2D</span>(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT1, GL_TEXTURE_2D, g_normal_, <span class=\"hljs-number\">0</span>);<br><br>\t<span class=\"hljs-comment\">// 顶点颜色数据</span><br>\t<span class=\"hljs-built_in\">glGenTextures</span>(<span class=\"hljs-number\">1</span>, &amp;g_albedo_);<br>\t<span class=\"hljs-built_in\">glBindTexture</span>(GL_TEXTURE_2D, g_albedo_);<br>\t<span class=\"hljs-built_in\">glTexImage2D</span>(GL_TEXTURE_2D, <span class=\"hljs-number\">0</span>, GL_RGBA, width, height, <span class=\"hljs-number\">0</span>, GL_RGBA, GL_UNSIGNED_INT, <span class=\"hljs-literal\">NULL</span>);<br>\t<span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);<br>\t<span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);<br>\t<span class=\"hljs-built_in\">glFramebufferTexture2D</span>(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT2, GL_TEXTURE_2D, g_albedo_, <span class=\"hljs-number\">0</span>);<br><br>\t<span class=\"hljs-comment\">// orm数据（ao，roughness，metallic）</span><br>\t<span class=\"hljs-built_in\">glGenTextures</span>(<span class=\"hljs-number\">1</span>, &amp;g_orm_);<br>\t<span class=\"hljs-built_in\">glBindTexture</span>(GL_TEXTURE_2D, g_orm_);<br>\t<span class=\"hljs-built_in\">glTexImage2D</span>(GL_TEXTURE_2D, <span class=\"hljs-number\">0</span>, GL_RGBA, width, height, <span class=\"hljs-number\">0</span>, GL_RGBA, GL_UNSIGNED_INT, <span class=\"hljs-literal\">NULL</span>);<br>\t<span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);<br>\t<span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);<br>\t<span class=\"hljs-built_in\">glFramebufferTexture2D</span>(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT3, GL_TEXTURE_2D, g_orm_, <span class=\"hljs-number\">0</span>);<br><br>\t<span class=\"hljs-comment\">// 生成renderbuffer</span><br>\t<span class=\"hljs-built_in\">glGenRenderbuffers</span>(<span class=\"hljs-number\">1</span>, &amp;g_rbo_);<br>\t<span class=\"hljs-built_in\">glBindRenderbuffer</span>(GL_RENDERBUFFER, g_rbo_);<br>\t<span class=\"hljs-built_in\">glRenderbufferStorage</span>(GL_RENDERBUFFER, GL_DEPTH_COMPONENT, width, height);<br>\t<span class=\"hljs-built_in\">glFramebufferRenderbuffer</span>(GL_FRAMEBUFFER, GL_DEPTH_ATTACHMENT, GL_RENDERBUFFER, g_rbo_);<br>\t<span class=\"hljs-built_in\">glEnable</span>(GL_DEPTH_TEST);<br>\t<br>\t<span class=\"hljs-type\">unsigned</span> <span class=\"hljs-type\">int</span> attachments[<span class=\"hljs-number\">4</span>] = &#123;GL_COLOR_ATTACHMENT0, GL_COLOR_ATTACHMENT1, GL_COLOR_ATTACHMENT2, GL_COLOR_ATTACHMENT3&#125;;<br>\t<span class=\"hljs-built_in\">glDrawBuffers</span>(<span class=\"hljs-number\">4</span>, attachments);<br>\t<span class=\"hljs-built_in\">glBindFramebuffer</span>(GL_FRAMEBUFFER, <span class=\"hljs-number\">0</span>);<br>&#125;<br></code></pre></td></tr></table></figure>\n\n\n<p>可以从上面的代码看到，我们利用了多渲染目标（multiple render targets）可以一次处理并输出到多个缓冲（GL_COLOR_ATTACHMENT0到3）。简化的几何处理着色器示例代码如下：</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-comment\">// defer_geometry_pass.frag</span><br><span class=\"hljs-keyword\">layout</span>(<span class=\"hljs-keyword\">location</span> = <span class=\"hljs-number\">0</span>) <span class=\"hljs-keyword\">out</span> <span class=\"hljs-type\">vec4</span> gPosition;<br><span class=\"hljs-keyword\">layout</span>(<span class=\"hljs-keyword\">location</span> = <span class=\"hljs-number\">1</span>) <span class=\"hljs-keyword\">out</span> <span class=\"hljs-type\">vec4</span> gNormal;<br><span class=\"hljs-keyword\">layout</span>(<span class=\"hljs-keyword\">location</span> = <span class=\"hljs-number\">2</span>) <span class=\"hljs-keyword\">out</span> <span class=\"hljs-type\">vec4</span> gAlbedo;<br><span class=\"hljs-keyword\">layout</span>(<span class=\"hljs-keyword\">location</span> = <span class=\"hljs-number\">3</span>) <span class=\"hljs-keyword\">out</span> <span class=\"hljs-type\">vec4</span> gORM;<br><br><span class=\"hljs-keyword\">in</span> <span class=\"hljs-type\">vec4</span> frag_pos;<br><span class=\"hljs-keyword\">in</span> <span class=\"hljs-type\">vec3</span> frag_normal;<br><span class=\"hljs-keyword\">in</span> <span class=\"hljs-type\">vec2</span> frag_uv;<br><br><span class=\"hljs-keyword\">uniform</span> <span class=\"hljs-type\">vec4</span> albedo;    <span class=\"hljs-comment\">// color</span><br><span class=\"hljs-keyword\">uniform</span> <span class=\"hljs-type\">float</span> metallic;<br><span class=\"hljs-keyword\">uniform</span> <span class=\"hljs-type\">float</span> roughness;<br><span class=\"hljs-keyword\">uniform</span> <span class=\"hljs-type\">float</span> ao;<br><br><span class=\"hljs-type\">void</span> main()<br>&#123;<br>    <span class=\"hljs-comment\">// 深度信息存储到position贴图的w值中</span><br>    gPosition = frag_pos;<br>    gNormal = <span class=\"hljs-type\">vec4</span>(frag_normal, <span class=\"hljs-number\">1.0</span>);<br>    gAlbedo = albedo;<br>    gORM = <span class=\"hljs-type\">vec4</span>(ao, roughness, metallic, <span class=\"hljs-number\">1.0</span>);<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<p>上方的代码将我们所需要的世界坐标下的顶点坐标信息、法线信息、漫反射颜色和材质信息输出到了四张贴图。带着这四张贴图的信息，我们进入下一个阶段，光照处理阶段。下面是个简化的光照处理着色器代码：</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-type\">void</span> main()<br>&#123;<br>    <span class=\"hljs-type\">vec3</span> frag_pos = <span class=\"hljs-built_in\">texture</span>(position_texture, TexCoords).xyz;<br>    <span class=\"hljs-type\">vec3</span> frag_normal = <span class=\"hljs-built_in\">texture</span>(normal_texture, TexCoords).rgb;<br>    <span class=\"hljs-type\">vec4</span> env_albedo = <span class=\"hljs-built_in\">texture</span>(albedo_texture, TexCoords);<br><br>    <span class=\"hljs-type\">vec3</span> orm = <span class=\"hljs-built_in\">texture</span>(orm_texture, TexCoords).rgb;<br>    <span class=\"hljs-type\">float</span> ao = orm.x;<br>    <span class=\"hljs-type\">float</span> env_roughness = orm.y;<br>    <span class=\"hljs-type\">float</span> env_metallic = orm.z;<br><br>    <span class=\"hljs-type\">vec3</span> view = <span class=\"hljs-built_in\">normalize</span>(cam_pos - frag_pos);  <span class=\"hljs-comment\">//to_view</span><br><br>    <span class=\"hljs-type\">vec3</span> light_color = CalcLight(light_info, frag_normal, view,  frag_pos, material);<br><br>    <span class=\"hljs-type\">vec3</span> color = ambient + light_color;<br>    FragColor = <span class=\"hljs-type\">vec4</span>(color, <span class=\"hljs-number\">1.0</span>);<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<h1 id=\"结合延迟和正向渲染\"><a href=\"#结合延迟和正向渲染\" class=\"headerlink\" title=\"结合延迟和正向渲染\"></a>结合延迟和正向渲染</h1><p>延迟渲染实现起来其实还是比较简单明了的，但是需要注意的是，有些材质并不能通过延迟渲染实现，比如说半透明这种需要进行alpha混合的材质，因此就会出现需要结合延迟渲染和正向渲染的情况。</p>\n<p>结合延迟渲染和正向渲染的时候，一般来说是先处理延迟渲染的部分。在处理完延迟渲染后，将延迟渲染的G-buffer的深度缓冲复制到最后输出屏幕的深度缓冲上（我这里最后会继续后处理，所以是会输出到后处理的FrameBuffer上）。如此一来，正向渲染的物体才可以和延迟渲染的场景有正确的深度遮挡结合，否则会出现正向渲染的物体永远在上的情况。实例代码如下所示：</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c++\"><span class=\"hljs-comment\">// 需要将延迟渲染的深度缓冲复制到后面的后处理buffer上</span><br><span class=\"hljs-built_in\">glBindFramebuffer</span>(GL_READ_FRAMEBUFFER, defer_buffer_.g_buffer_);<br><span class=\"hljs-built_in\">glBindFramebuffer</span>(GL_DRAW_FRAMEBUFFER, post_process.<span class=\"hljs-built_in\">GetScreenFrameBuffer</span>());<br><span class=\"hljs-built_in\">glBlitFramebuffer</span>(<span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">0</span>, window_size.x, window_size.y, <span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">0</span>, window_size.x, window_size.y, GL_DEPTH_BUFFER_BIT, <br>GL_NEAREST);<br></code></pre></td></tr></table></figure>\n\n<h1 id=\"延迟渲染的效能提升\"><a href=\"#延迟渲染的效能提升\" class=\"headerlink\" title=\"延迟渲染的效能提升\"></a>延迟渲染的效能提升</h1><p>之前提过，延迟渲染最大的好处之一便是能够提升渲染的效率，这里大概做一个粗略的测试。下方是一个包含着1000个人物模型和200个点光源的场景，如果按照正常的正向渲染，这个场景在我的笔记本上的帧率大概在35左右：</p>\n<p><img src=\"/2024/10/19/defer-render/no_defer_render.png\" alt=\"非延迟渲染\"></p>\n<p>当使用延迟渲染的情况下，该场景的帧率可以提升到170左右：</p>\n<p><img src=\"/2024/10/19/defer-render/defer_render.png\" alt=\"延迟渲染\"></p>\n<p>当然上方这是个比较极端的场景，实际场景上可能不会有这么复杂的光源，以及模型可能不会像测试场景这样重叠，所以差距可能不会像测试场景那般明显。但是一般来说延迟渲染对渲染场景的性能提升会是比较客观的。</p>\n<h2 id=\"基于延迟渲染的延伸\"><a href=\"#基于延迟渲染的延伸\" class=\"headerlink\" title=\"基于延迟渲染的延伸\"></a>基于延迟渲染的延伸</h2><p>延迟渲染的好处之一不仅仅体现在性能上，由于延迟渲染将很多有用的信息存储下来，基于延迟渲染我们可以实现非常多其他的效果。比如说屏幕空间环境光遮蔽SSAO（如下图）以及屏幕空间反射SSR等等，我计划在后面的文章详细介绍一下。</p>\n<p><img src=\"/2024/10/19/defer-render/ssao.png\" alt=\"SSAO效果\"></p>\n","excerpt":"","more":"<p>想要在Kong引擎里面实现的场景慢慢复杂了起来，光源和模型的数量从原先的十以内的数量增长到几十甚至几百的数量级，是时候接入延迟渲染的方法了。</p>\n<h1 id=\"延迟渲染\"><a href=\"#延迟渲染\" class=\"headerlink\" title=\"延迟渲染\"></a>延迟渲染</h1><p><strong>延迟渲染</strong>（Defer Rendering），或者<strong>延迟着色法</strong>（Defer Shading），是区别于<strong>正向渲染</strong>（Forward Shading）的一种计算场景光照的方式。</p>\n<p>正向渲染方法就是遍历场景中的每一个模型，计算一个模型的光照表现后再继续下一个模型的计算，根据深度测试的结果更新屏幕上最终像素显示的颜色。这种方法是很容易让人理解并实现的。但是当场景中的光照和模型数量变多的时候，模型重叠的区域会进行不必要的光照计算（被挡住的模型像素区域最终会被前面的模型遮挡，但是这篇被挡住的区域还是被计算了光照），而光照计算一般来说是渲染消耗的大头，这部分时间就被浪费了。</p>\n<p>而延迟渲染的想法则是将光照计算分成两部分。第一个部分叫做<strong>几何处理阶段</strong>（Geometry Pass），它先将光照计算所需要的模型信息（顶点位置、法线、颜色、材质属性等等）先渲染到多张贴图上（消耗低），经由深度检测保留最终在屏幕上显示的模型部分的这些信息。</p>\n<!-- wp:image {\"sizeSlug\":\"large\",\"align\":\"center\"} -->\n<figure class=\"wp-block-image aligncenter size-large\"><img src=\"https://learnopengl-cn.github.io/img/05/08/deferred_g_buffer.png\" alt=\"\"/></figure>\n<!-- /wp:image -->\n\n\n<p>第二部分叫做<strong>光照处理阶段</strong>（Lighting Pass），根据几何处理阶段保存的信息再去进行光照计算，这样就不会将算力浪费在计算被遮挡的模型部分的光照了，从而优化渲染的性能，也有赋予了能够更加方便的实现某些效果的能力（如SSAO）。</p>\n<!-- wp:image {\"sizeSlug\":\"large\",\"align\":\"center\"} -->\n<figure class=\"wp-block-image aligncenter size-large\"><img src=\"https://learnopengl-cn.github.io/img/05/08/deferred_overview.png\" alt=\"\"/></figure>\n<!-- /wp:image -->\n\n<h1 id=\"G缓冲\"><a href=\"#G缓冲\" class=\"headerlink\" title=\"G缓冲\"></a>G缓冲</h1><p>G缓冲(G-buffer)是对所有用来储存光照相关的数据，并在最后的光照处理阶段中使用的所有纹理的总称。它是我们计算最终渲染输出中的缓存和中转站，为了实现延迟渲染，G-buffer中会包含如下几张纹理的数据：模型顶点位置数据；模型法线数据；模型漫反射颜色数据；材质数据（ao，roughness，metallic）等等。有了这些数据则能够实现Kong引擎的PBR光照计算，初始化G-buffer的代码如下：</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c++\"><span class=\"hljs-function\"><span class=\"hljs-type\">void</span> <span class=\"hljs-title\">DeferBuffer::GenerateDeferRenderTextures</span><span class=\"hljs-params\">(<span class=\"hljs-type\">int</span> width, <span class=\"hljs-type\">int</span> height)</span></span><br><span class=\"hljs-function\"></span>&#123;<br>\t<span class=\"hljs-built_in\">glBindFramebuffer</span>(GL_FRAMEBUFFER, g_buffer_);<br><br>\t<span class=\"hljs-comment\">// 将当前视野的数据用贴图缓存</span><br>\t<span class=\"hljs-comment\">// 位置数据</span><br>\t<span class=\"hljs-built_in\">glGenTextures</span>(<span class=\"hljs-number\">1</span>, &amp;g_position_);<br>\t<span class=\"hljs-built_in\">glBindTexture</span>(GL_TEXTURE_2D, g_position_);<br>\t<span class=\"hljs-built_in\">glTexImage2D</span>(GL_TEXTURE_2D, <span class=\"hljs-number\">0</span>, GL_RGBA32F, width, height, <span class=\"hljs-number\">0</span>, GL_RGBA, GL_FLOAT, <span class=\"hljs-literal\">NULL</span>);<br>\t<span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);<br>\t<span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);<br>\t<span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);<br>\t<span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);<br>\t<span class=\"hljs-built_in\">glFramebufferTexture2D</span>(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, g_position_, <span class=\"hljs-number\">0</span>);<br><br>\t<span class=\"hljs-comment\">// 法线数据</span><br>\t<span class=\"hljs-built_in\">glGenTextures</span>(<span class=\"hljs-number\">1</span>, &amp;g_normal_);<br>\t<span class=\"hljs-built_in\">glBindTexture</span>(GL_TEXTURE_2D, g_normal_);<br>\t<span class=\"hljs-built_in\">glTexImage2D</span>(GL_TEXTURE_2D, <span class=\"hljs-number\">0</span>, GL_RGBA32F, width, height, <span class=\"hljs-number\">0</span>, GL_RGBA, GL_FLOAT, <span class=\"hljs-literal\">NULL</span>);<br>\t<span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);<br>\t<span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);<br>\t<span class=\"hljs-built_in\">glFramebufferTexture2D</span>(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT1, GL_TEXTURE_2D, g_normal_, <span class=\"hljs-number\">0</span>);<br><br>\t<span class=\"hljs-comment\">// 顶点颜色数据</span><br>\t<span class=\"hljs-built_in\">glGenTextures</span>(<span class=\"hljs-number\">1</span>, &amp;g_albedo_);<br>\t<span class=\"hljs-built_in\">glBindTexture</span>(GL_TEXTURE_2D, g_albedo_);<br>\t<span class=\"hljs-built_in\">glTexImage2D</span>(GL_TEXTURE_2D, <span class=\"hljs-number\">0</span>, GL_RGBA, width, height, <span class=\"hljs-number\">0</span>, GL_RGBA, GL_UNSIGNED_INT, <span class=\"hljs-literal\">NULL</span>);<br>\t<span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);<br>\t<span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);<br>\t<span class=\"hljs-built_in\">glFramebufferTexture2D</span>(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT2, GL_TEXTURE_2D, g_albedo_, <span class=\"hljs-number\">0</span>);<br><br>\t<span class=\"hljs-comment\">// orm数据（ao，roughness，metallic）</span><br>\t<span class=\"hljs-built_in\">glGenTextures</span>(<span class=\"hljs-number\">1</span>, &amp;g_orm_);<br>\t<span class=\"hljs-built_in\">glBindTexture</span>(GL_TEXTURE_2D, g_orm_);<br>\t<span class=\"hljs-built_in\">glTexImage2D</span>(GL_TEXTURE_2D, <span class=\"hljs-number\">0</span>, GL_RGBA, width, height, <span class=\"hljs-number\">0</span>, GL_RGBA, GL_UNSIGNED_INT, <span class=\"hljs-literal\">NULL</span>);<br>\t<span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);<br>\t<span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);<br>\t<span class=\"hljs-built_in\">glFramebufferTexture2D</span>(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT3, GL_TEXTURE_2D, g_orm_, <span class=\"hljs-number\">0</span>);<br><br>\t<span class=\"hljs-comment\">// 生成renderbuffer</span><br>\t<span class=\"hljs-built_in\">glGenRenderbuffers</span>(<span class=\"hljs-number\">1</span>, &amp;g_rbo_);<br>\t<span class=\"hljs-built_in\">glBindRenderbuffer</span>(GL_RENDERBUFFER, g_rbo_);<br>\t<span class=\"hljs-built_in\">glRenderbufferStorage</span>(GL_RENDERBUFFER, GL_DEPTH_COMPONENT, width, height);<br>\t<span class=\"hljs-built_in\">glFramebufferRenderbuffer</span>(GL_FRAMEBUFFER, GL_DEPTH_ATTACHMENT, GL_RENDERBUFFER, g_rbo_);<br>\t<span class=\"hljs-built_in\">glEnable</span>(GL_DEPTH_TEST);<br>\t<br>\t<span class=\"hljs-type\">unsigned</span> <span class=\"hljs-type\">int</span> attachments[<span class=\"hljs-number\">4</span>] = &#123;GL_COLOR_ATTACHMENT0, GL_COLOR_ATTACHMENT1, GL_COLOR_ATTACHMENT2, GL_COLOR_ATTACHMENT3&#125;;<br>\t<span class=\"hljs-built_in\">glDrawBuffers</span>(<span class=\"hljs-number\">4</span>, attachments);<br>\t<span class=\"hljs-built_in\">glBindFramebuffer</span>(GL_FRAMEBUFFER, <span class=\"hljs-number\">0</span>);<br>&#125;<br></code></pre></td></tr></table></figure>\n\n\n<p>可以从上面的代码看到，我们利用了多渲染目标（multiple render targets）可以一次处理并输出到多个缓冲（GL_COLOR_ATTACHMENT0到3）。简化的几何处理着色器示例代码如下：</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-comment\">// defer_geometry_pass.frag</span><br><span class=\"hljs-keyword\">layout</span>(<span class=\"hljs-keyword\">location</span> = <span class=\"hljs-number\">0</span>) <span class=\"hljs-keyword\">out</span> <span class=\"hljs-type\">vec4</span> gPosition;<br><span class=\"hljs-keyword\">layout</span>(<span class=\"hljs-keyword\">location</span> = <span class=\"hljs-number\">1</span>) <span class=\"hljs-keyword\">out</span> <span class=\"hljs-type\">vec4</span> gNormal;<br><span class=\"hljs-keyword\">layout</span>(<span class=\"hljs-keyword\">location</span> = <span class=\"hljs-number\">2</span>) <span class=\"hljs-keyword\">out</span> <span class=\"hljs-type\">vec4</span> gAlbedo;<br><span class=\"hljs-keyword\">layout</span>(<span class=\"hljs-keyword\">location</span> = <span class=\"hljs-number\">3</span>) <span class=\"hljs-keyword\">out</span> <span class=\"hljs-type\">vec4</span> gORM;<br><br><span class=\"hljs-keyword\">in</span> <span class=\"hljs-type\">vec4</span> frag_pos;<br><span class=\"hljs-keyword\">in</span> <span class=\"hljs-type\">vec3</span> frag_normal;<br><span class=\"hljs-keyword\">in</span> <span class=\"hljs-type\">vec2</span> frag_uv;<br><br><span class=\"hljs-keyword\">uniform</span> <span class=\"hljs-type\">vec4</span> albedo;    <span class=\"hljs-comment\">// color</span><br><span class=\"hljs-keyword\">uniform</span> <span class=\"hljs-type\">float</span> metallic;<br><span class=\"hljs-keyword\">uniform</span> <span class=\"hljs-type\">float</span> roughness;<br><span class=\"hljs-keyword\">uniform</span> <span class=\"hljs-type\">float</span> ao;<br><br><span class=\"hljs-type\">void</span> main()<br>&#123;<br>    <span class=\"hljs-comment\">// 深度信息存储到position贴图的w值中</span><br>    gPosition = frag_pos;<br>    gNormal = <span class=\"hljs-type\">vec4</span>(frag_normal, <span class=\"hljs-number\">1.0</span>);<br>    gAlbedo = albedo;<br>    gORM = <span class=\"hljs-type\">vec4</span>(ao, roughness, metallic, <span class=\"hljs-number\">1.0</span>);<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<p>上方的代码将我们所需要的世界坐标下的顶点坐标信息、法线信息、漫反射颜色和材质信息输出到了四张贴图。带着这四张贴图的信息，我们进入下一个阶段，光照处理阶段。下面是个简化的光照处理着色器代码：</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-type\">void</span> main()<br>&#123;<br>    <span class=\"hljs-type\">vec3</span> frag_pos = <span class=\"hljs-built_in\">texture</span>(position_texture, TexCoords).xyz;<br>    <span class=\"hljs-type\">vec3</span> frag_normal = <span class=\"hljs-built_in\">texture</span>(normal_texture, TexCoords).rgb;<br>    <span class=\"hljs-type\">vec4</span> env_albedo = <span class=\"hljs-built_in\">texture</span>(albedo_texture, TexCoords);<br><br>    <span class=\"hljs-type\">vec3</span> orm = <span class=\"hljs-built_in\">texture</span>(orm_texture, TexCoords).rgb;<br>    <span class=\"hljs-type\">float</span> ao = orm.x;<br>    <span class=\"hljs-type\">float</span> env_roughness = orm.y;<br>    <span class=\"hljs-type\">float</span> env_metallic = orm.z;<br><br>    <span class=\"hljs-type\">vec3</span> view = <span class=\"hljs-built_in\">normalize</span>(cam_pos - frag_pos);  <span class=\"hljs-comment\">//to_view</span><br><br>    <span class=\"hljs-type\">vec3</span> light_color = CalcLight(light_info, frag_normal, view,  frag_pos, material);<br><br>    <span class=\"hljs-type\">vec3</span> color = ambient + light_color;<br>    FragColor = <span class=\"hljs-type\">vec4</span>(color, <span class=\"hljs-number\">1.0</span>);<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<h1 id=\"结合延迟和正向渲染\"><a href=\"#结合延迟和正向渲染\" class=\"headerlink\" title=\"结合延迟和正向渲染\"></a>结合延迟和正向渲染</h1><p>延迟渲染实现起来其实还是比较简单明了的，但是需要注意的是，有些材质并不能通过延迟渲染实现，比如说半透明这种需要进行alpha混合的材质，因此就会出现需要结合延迟渲染和正向渲染的情况。</p>\n<p>结合延迟渲染和正向渲染的时候，一般来说是先处理延迟渲染的部分。在处理完延迟渲染后，将延迟渲染的G-buffer的深度缓冲复制到最后输出屏幕的深度缓冲上（我这里最后会继续后处理，所以是会输出到后处理的FrameBuffer上）。如此一来，正向渲染的物体才可以和延迟渲染的场景有正确的深度遮挡结合，否则会出现正向渲染的物体永远在上的情况。实例代码如下所示：</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c++\"><span class=\"hljs-comment\">// 需要将延迟渲染的深度缓冲复制到后面的后处理buffer上</span><br><span class=\"hljs-built_in\">glBindFramebuffer</span>(GL_READ_FRAMEBUFFER, defer_buffer_.g_buffer_);<br><span class=\"hljs-built_in\">glBindFramebuffer</span>(GL_DRAW_FRAMEBUFFER, post_process.<span class=\"hljs-built_in\">GetScreenFrameBuffer</span>());<br><span class=\"hljs-built_in\">glBlitFramebuffer</span>(<span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">0</span>, window_size.x, window_size.y, <span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">0</span>, window_size.x, window_size.y, GL_DEPTH_BUFFER_BIT, <br>GL_NEAREST);<br></code></pre></td></tr></table></figure>\n\n<h1 id=\"延迟渲染的效能提升\"><a href=\"#延迟渲染的效能提升\" class=\"headerlink\" title=\"延迟渲染的效能提升\"></a>延迟渲染的效能提升</h1><p>之前提过，延迟渲染最大的好处之一便是能够提升渲染的效率，这里大概做一个粗略的测试。下方是一个包含着1000个人物模型和200个点光源的场景，如果按照正常的正向渲染，这个场景在我的笔记本上的帧率大概在35左右：</p>\n<p><img src=\"/2024/10/19/defer-render/no_defer_render.png\" alt=\"非延迟渲染\"></p>\n<p>当使用延迟渲染的情况下，该场景的帧率可以提升到170左右：</p>\n<p><img src=\"/2024/10/19/defer-render/defer_render.png\" alt=\"延迟渲染\"></p>\n<p>当然上方这是个比较极端的场景，实际场景上可能不会有这么复杂的光源，以及模型可能不会像测试场景这样重叠，所以差距可能不会像测试场景那般明显。但是一般来说延迟渲染对渲染场景的性能提升会是比较客观的。</p>\n<h2 id=\"基于延迟渲染的延伸\"><a href=\"#基于延迟渲染的延伸\" class=\"headerlink\" title=\"基于延迟渲染的延伸\"></a>基于延迟渲染的延伸</h2><p>延迟渲染的好处之一不仅仅体现在性能上，由于延迟渲染将很多有用的信息存储下来，基于延迟渲染我们可以实现非常多其他的效果。比如说屏幕空间环境光遮蔽SSAO（如下图）以及屏幕空间反射SSR等等，我计划在后面的文章详细介绍一下。</p>\n<p><img src=\"/2024/10/19/defer-render/ssao.png\" alt=\"SSAO效果\"></p>\n"},{"title":"景深的简单实现","date":"2024-10-28T14:11:58.000Z","index_img":"/2024/10/28/depth-of-field/game1.jpg","banner_img":"/2024/10/28/depth-of-field/game1.jpg","_content":"\n# 关于景深\n景深是一个常在摄像领域出现的词，它一般指的是沿着摄像机或其他成像器的拍摄方向上，能够取得清晰图像的成像所测定的被摄物体前后距离范围。用大白话就是说，拥有浅景深的成像器拍摄出来的效果，是只有焦点附近的图像是清晰的，其他地方的图像都是模糊的；而拥有大景深则可以在离焦点很远的地方也能有清晰的图像。\n\n![一张浅景深的照片](dof_butterfly.JPG)\n\n有了景深效果的图像可以有重点的突出核心想要表达的内容，不仅仅在摄影摄像的领域有非常多的应用，在游戏领域内也是应用广泛，可以表现出很独特的风格化美术效果（如八方旅人的浅景深效果）。\n\n![八方旅人](game1.jpg)\n\n下面我来介绍一种基础的景深效果实现，这个方法也在最近接入了[KongEngine](https://github.com/ruochenhua/KongEngine)\n\n\n# 渲染散景\n浅景深的主要应用是通过调整不同焦距上物体的成像清晰程度来突出渲染画面的重点。清晰的部分我们已经掌握了，就是正常的将场景渲染出来，那不清晰的部分（或者叫做散景）也有不少的实现方式，一般是通过模糊算法来实现。\n\n模糊的算法有很多种，比如box blur，gaussian blur等等，效果最好的是扩张模糊(dilate blur)，这也是我们会采取的方法。\n\n## 扩张模糊（dilate blur)\n扩张模糊它的主要方式，是在给定一个模糊的窗口下，取得这个窗口下的最亮的颜色记录下来，然后将这个最亮的颜色扩张到整个窗口。这样一来经过扩张模糊的画面会有一种亮晶晶，并且很柔和的效果，很适合作为散景的表现效果。\n\ndilate blur的计算也比较简单，首先我们定义此次blur的窗口大小已经采样的间隔大小。\n\n``` glsl\n// 窗口的大小，数值越大扩散越大，消耗越高 \nint size = 5;\t\n// 采样间隔的大小，数值越大扩散越大，效果降低\nfloat separation = 1.0;\n```\n\n在定义了窗口的尺寸之后，我们在窗口的范围内记录最亮的像素颜色，并保存下来。**窗口的形状不限**，可以是矩形，或者圆形。我们这里实现采取圆形的窗口。\n\n``` glsl\n// 渲染场景的尺寸\nvec2 tex_size = vec2(textureSize(scene_texture, 0).xy);\n// 获取场景的原本颜色\nFragColor = texture(scene_texture, TexCoords);\n\nif(size <= 0) return;\nfloat mx = 0.0;\nvec4 cmx = FragColor;\n\nfor(int i = -size; i <= size; ++i)\n{\n\tfor(int j = -size; j <= size; ++j)\n\t{\n\t\t// dilate的形状可以多样，如圆形，矩形等等，根据采样点的形状来决定\n\t\t// 这里使用圆形的dilate\n\t\tif(distance(vec2(i,j), vec2(0)) > size) continue;\n\n\t\t// 采样区域内点的颜色，不要越界出去了\n\t\tvec2 sample_coord = TexCoords + vec2(i, j)*separation/tex_size;\n\t\tif(sample_coord.x > 1.0 || sample_coord.x < 0.0 || sample_coord.y > 1.0 || sample_coord.y < 0.0)\n\t\t\t\tcontinue;\n\n\t\t// 拿到采样点\n\t\tvec4 c = texture(scene_texture, sample_coord);\n\n\t\t// 和目标颜色做点乘，得到一个灰度值\n\t\tfloat mxt = dot(c.rgb, vec3(0.3, 0.59, 0.11));\n\n\t\t// 保存区域内灰度值最大的颜色\n\t\tif(mxt > mx)\n\t\t{\n\t\t\tmx = mxt;\n\t\t\tcmx = c;\n\t\t}\n\t}\n}\n```\n\n这里我们计算最亮区域的方式是通过和一个目标颜色**(0.3, 0.59, 0.11)**来做点乘，当然也可以通过和其他的目标颜色，或者其他的方式来实现。\n\n最后，我们得到窗口区域内最亮的颜色，我们的最终颜色是原本颜色和最亮颜色的差值。\n```glsl\n// 模糊采样的颜色和原始颜色的mix上下限\nfloat min_threshold = 0.1;\nfloat max_threshold = 0.3;\n\n// 最终颜色是原来颜色和区域内灰度值最大的采样颜色的混合，有上下限做限制\nFragColor.rgb = mix(FragColor.rgb, cmx.rgb, smoothstep(min_threshold, max_threshold, mx));\n```\n这里我们还是采用了一个上下限，尽量控制增亮的程度。\n\n## 散景的效果\n这里给出经过扩张模糊得到的散景效果。下面这张图是扩张模糊之前的效果。\n![扩张模糊前](dilate_before.png)\n下面这张图是扩张模糊之后的效果。\n![扩张模糊后](dilate_after.png)\n\n当然在实际的场景中，我们可能不会开这么大的扩散效果，此处只是作为对比。\n\n# 结合场景\n好了，现在我们有原本场景的渲染效果和扩散后的效果，实现最终的景深效果需要将这两者结合起来。我们需要一种方法来决定画面上哪些部分是需要采用清晰的图像，哪些是采用模糊的图像。\n\n为了实现这个效果我们需要获得画面上每个点的深度，或者每个点的实际世界坐标。幸运的是，我们在实现[延迟渲染](https://ruochenhua.github.io/2024/10/19/defer-render/)的时候已经将这些存放到缓冲中去了，接下来就是实现景深效果了。\n\n最终的代码如下：\n``` glsl\nout vec4 FragColor;\nin vec2 TexCoords;\n\nuniform sampler2D scene_texture;\nuniform sampler2D dilate_texture;\nuniform sampler2D position_texture;\n\n// 焦点距离\nuniform float focus_distance = 3.0;\nuniform vec2 focus_threshold;\n// 景深的上下限\nfloat min_dist = focus_threshold.x;\nfloat max_dist = focus_threshold.y;\n\nvoid main()\n{\n    vec4 focus_color = texture(scene_texture, TexCoords);\n    vec4 out_of_focus_color = texture(dilate_texture, TexCoords);\n    vec3 scene_position = texture(position_texture, TexCoords).xyz; \n    \n\t// 这里采用了画面每个点的世界坐标和相机的距离作为判定模糊的标准\n\t// 当然用深度信息也是可以的，性能上也更好，这里为了代码展示更好理解\n    vec3 cam_pos = matrix_ubo.cam_pos.xyz;\n\t\n    float blur_amout = smoothstep(min_dist, max_dist, abs(focus_distance - distance(scene_position, cam_pos)));\n    \n\t// 最后的颜色是焦距内和散景的混合\n    FragColor = mix(focus_color, out_of_focus_color, blur_amout);\n}\n```\n\n这段代码理解起来应该没有什么太大的难度。\n\n# 最终效果\n这里展示一下最终的效果。\n\n![近焦点](dof_near.png)\n![远焦点](dof_far.png)\n\n上面两张图分别展示了不同焦点的景深的表现结果。可以明显看到不同景深效果的加入可以很容易的将画面上想要着重表达出来的部分勾勒出来。优秀的散景效果也能给画面增加不少美感（虽然我这个测试场景也没有什么美感可言...）。","source":"_posts/depth-of-field.md","raw":"---\ntitle: 景深的简单实现\ndate: 2024-10-28 22:11:58\ncategories: \n\t- 技术漫谈\ntags: [3D, render, 渲染, 编程]\nindex_img: /2024/10/28/depth-of-field/game1.jpg\nbanner_img: /2024/10/28/depth-of-field/game1.jpg\n---\n\n# 关于景深\n景深是一个常在摄像领域出现的词，它一般指的是沿着摄像机或其他成像器的拍摄方向上，能够取得清晰图像的成像所测定的被摄物体前后距离范围。用大白话就是说，拥有浅景深的成像器拍摄出来的效果，是只有焦点附近的图像是清晰的，其他地方的图像都是模糊的；而拥有大景深则可以在离焦点很远的地方也能有清晰的图像。\n\n![一张浅景深的照片](dof_butterfly.JPG)\n\n有了景深效果的图像可以有重点的突出核心想要表达的内容，不仅仅在摄影摄像的领域有非常多的应用，在游戏领域内也是应用广泛，可以表现出很独特的风格化美术效果（如八方旅人的浅景深效果）。\n\n![八方旅人](game1.jpg)\n\n下面我来介绍一种基础的景深效果实现，这个方法也在最近接入了[KongEngine](https://github.com/ruochenhua/KongEngine)\n\n\n# 渲染散景\n浅景深的主要应用是通过调整不同焦距上物体的成像清晰程度来突出渲染画面的重点。清晰的部分我们已经掌握了，就是正常的将场景渲染出来，那不清晰的部分（或者叫做散景）也有不少的实现方式，一般是通过模糊算法来实现。\n\n模糊的算法有很多种，比如box blur，gaussian blur等等，效果最好的是扩张模糊(dilate blur)，这也是我们会采取的方法。\n\n## 扩张模糊（dilate blur)\n扩张模糊它的主要方式，是在给定一个模糊的窗口下，取得这个窗口下的最亮的颜色记录下来，然后将这个最亮的颜色扩张到整个窗口。这样一来经过扩张模糊的画面会有一种亮晶晶，并且很柔和的效果，很适合作为散景的表现效果。\n\ndilate blur的计算也比较简单，首先我们定义此次blur的窗口大小已经采样的间隔大小。\n\n``` glsl\n// 窗口的大小，数值越大扩散越大，消耗越高 \nint size = 5;\t\n// 采样间隔的大小，数值越大扩散越大，效果降低\nfloat separation = 1.0;\n```\n\n在定义了窗口的尺寸之后，我们在窗口的范围内记录最亮的像素颜色，并保存下来。**窗口的形状不限**，可以是矩形，或者圆形。我们这里实现采取圆形的窗口。\n\n``` glsl\n// 渲染场景的尺寸\nvec2 tex_size = vec2(textureSize(scene_texture, 0).xy);\n// 获取场景的原本颜色\nFragColor = texture(scene_texture, TexCoords);\n\nif(size <= 0) return;\nfloat mx = 0.0;\nvec4 cmx = FragColor;\n\nfor(int i = -size; i <= size; ++i)\n{\n\tfor(int j = -size; j <= size; ++j)\n\t{\n\t\t// dilate的形状可以多样，如圆形，矩形等等，根据采样点的形状来决定\n\t\t// 这里使用圆形的dilate\n\t\tif(distance(vec2(i,j), vec2(0)) > size) continue;\n\n\t\t// 采样区域内点的颜色，不要越界出去了\n\t\tvec2 sample_coord = TexCoords + vec2(i, j)*separation/tex_size;\n\t\tif(sample_coord.x > 1.0 || sample_coord.x < 0.0 || sample_coord.y > 1.0 || sample_coord.y < 0.0)\n\t\t\t\tcontinue;\n\n\t\t// 拿到采样点\n\t\tvec4 c = texture(scene_texture, sample_coord);\n\n\t\t// 和目标颜色做点乘，得到一个灰度值\n\t\tfloat mxt = dot(c.rgb, vec3(0.3, 0.59, 0.11));\n\n\t\t// 保存区域内灰度值最大的颜色\n\t\tif(mxt > mx)\n\t\t{\n\t\t\tmx = mxt;\n\t\t\tcmx = c;\n\t\t}\n\t}\n}\n```\n\n这里我们计算最亮区域的方式是通过和一个目标颜色**(0.3, 0.59, 0.11)**来做点乘，当然也可以通过和其他的目标颜色，或者其他的方式来实现。\n\n最后，我们得到窗口区域内最亮的颜色，我们的最终颜色是原本颜色和最亮颜色的差值。\n```glsl\n// 模糊采样的颜色和原始颜色的mix上下限\nfloat min_threshold = 0.1;\nfloat max_threshold = 0.3;\n\n// 最终颜色是原来颜色和区域内灰度值最大的采样颜色的混合，有上下限做限制\nFragColor.rgb = mix(FragColor.rgb, cmx.rgb, smoothstep(min_threshold, max_threshold, mx));\n```\n这里我们还是采用了一个上下限，尽量控制增亮的程度。\n\n## 散景的效果\n这里给出经过扩张模糊得到的散景效果。下面这张图是扩张模糊之前的效果。\n![扩张模糊前](dilate_before.png)\n下面这张图是扩张模糊之后的效果。\n![扩张模糊后](dilate_after.png)\n\n当然在实际的场景中，我们可能不会开这么大的扩散效果，此处只是作为对比。\n\n# 结合场景\n好了，现在我们有原本场景的渲染效果和扩散后的效果，实现最终的景深效果需要将这两者结合起来。我们需要一种方法来决定画面上哪些部分是需要采用清晰的图像，哪些是采用模糊的图像。\n\n为了实现这个效果我们需要获得画面上每个点的深度，或者每个点的实际世界坐标。幸运的是，我们在实现[延迟渲染](https://ruochenhua.github.io/2024/10/19/defer-render/)的时候已经将这些存放到缓冲中去了，接下来就是实现景深效果了。\n\n最终的代码如下：\n``` glsl\nout vec4 FragColor;\nin vec2 TexCoords;\n\nuniform sampler2D scene_texture;\nuniform sampler2D dilate_texture;\nuniform sampler2D position_texture;\n\n// 焦点距离\nuniform float focus_distance = 3.0;\nuniform vec2 focus_threshold;\n// 景深的上下限\nfloat min_dist = focus_threshold.x;\nfloat max_dist = focus_threshold.y;\n\nvoid main()\n{\n    vec4 focus_color = texture(scene_texture, TexCoords);\n    vec4 out_of_focus_color = texture(dilate_texture, TexCoords);\n    vec3 scene_position = texture(position_texture, TexCoords).xyz; \n    \n\t// 这里采用了画面每个点的世界坐标和相机的距离作为判定模糊的标准\n\t// 当然用深度信息也是可以的，性能上也更好，这里为了代码展示更好理解\n    vec3 cam_pos = matrix_ubo.cam_pos.xyz;\n\t\n    float blur_amout = smoothstep(min_dist, max_dist, abs(focus_distance - distance(scene_position, cam_pos)));\n    \n\t// 最后的颜色是焦距内和散景的混合\n    FragColor = mix(focus_color, out_of_focus_color, blur_amout);\n}\n```\n\n这段代码理解起来应该没有什么太大的难度。\n\n# 最终效果\n这里展示一下最终的效果。\n\n![近焦点](dof_near.png)\n![远焦点](dof_far.png)\n\n上面两张图分别展示了不同焦点的景深的表现结果。可以明显看到不同景深效果的加入可以很容易的将画面上想要着重表达出来的部分勾勒出来。优秀的散景效果也能给画面增加不少美感（虽然我这个测试场景也没有什么美感可言...）。","slug":"depth-of-field","published":1,"updated":"2024-10-28T15:19:19.418Z","comments":1,"layout":"post","photos":[],"_id":"cm5ht44vk000i3457fr8k5jn8","content":"<h1 id=\"关于景深\"><a href=\"#关于景深\" class=\"headerlink\" title=\"关于景深\"></a>关于景深</h1><p>景深是一个常在摄像领域出现的词，它一般指的是沿着摄像机或其他成像器的拍摄方向上，能够取得清晰图像的成像所测定的被摄物体前后距离范围。用大白话就是说，拥有浅景深的成像器拍摄出来的效果，是只有焦点附近的图像是清晰的，其他地方的图像都是模糊的；而拥有大景深则可以在离焦点很远的地方也能有清晰的图像。</p>\n<p><img src=\"/2024/10/28/depth-of-field/dof_butterfly.JPG\" alt=\"一张浅景深的照片\"></p>\n<p>有了景深效果的图像可以有重点的突出核心想要表达的内容，不仅仅在摄影摄像的领域有非常多的应用，在游戏领域内也是应用广泛，可以表现出很独特的风格化美术效果（如八方旅人的浅景深效果）。</p>\n<p><img src=\"/2024/10/28/depth-of-field/game1.jpg\" alt=\"八方旅人\"></p>\n<p>下面我来介绍一种基础的景深效果实现，这个方法也在最近接入了<a href=\"https://github.com/ruochenhua/KongEngine\">KongEngine</a></p>\n<h1 id=\"渲染散景\"><a href=\"#渲染散景\" class=\"headerlink\" title=\"渲染散景\"></a>渲染散景</h1><p>浅景深的主要应用是通过调整不同焦距上物体的成像清晰程度来突出渲染画面的重点。清晰的部分我们已经掌握了，就是正常的将场景渲染出来，那不清晰的部分（或者叫做散景）也有不少的实现方式，一般是通过模糊算法来实现。</p>\n<p>模糊的算法有很多种，比如box blur，gaussian blur等等，效果最好的是扩张模糊(dilate blur)，这也是我们会采取的方法。</p>\n<h2 id=\"扩张模糊（dilate-blur\"><a href=\"#扩张模糊（dilate-blur\" class=\"headerlink\" title=\"扩张模糊（dilate blur)\"></a>扩张模糊（dilate blur)</h2><p>扩张模糊它的主要方式，是在给定一个模糊的窗口下，取得这个窗口下的最亮的颜色记录下来，然后将这个最亮的颜色扩张到整个窗口。这样一来经过扩张模糊的画面会有一种亮晶晶，并且很柔和的效果，很适合作为散景的表现效果。</p>\n<p>dilate blur的计算也比较简单，首先我们定义此次blur的窗口大小已经采样的间隔大小。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-comment\">// 窗口的大小，数值越大扩散越大，消耗越高 </span><br><span class=\"hljs-type\">int</span> size = <span class=\"hljs-number\">5</span>;\t<br><span class=\"hljs-comment\">// 采样间隔的大小，数值越大扩散越大，效果降低</span><br><span class=\"hljs-type\">float</span> separation = <span class=\"hljs-number\">1.0</span>;<br></code></pre></td></tr></table></figure>\n\n<p>在定义了窗口的尺寸之后，我们在窗口的范围内记录最亮的像素颜色，并保存下来。<strong>窗口的形状不限</strong>，可以是矩形，或者圆形。我们这里实现采取圆形的窗口。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-comment\">// 渲染场景的尺寸</span><br><span class=\"hljs-type\">vec2</span> tex_size = <span class=\"hljs-type\">vec2</span>(<span class=\"hljs-built_in\">textureSize</span>(scene_texture, <span class=\"hljs-number\">0</span>).xy);<br><span class=\"hljs-comment\">// 获取场景的原本颜色</span><br>FragColor = <span class=\"hljs-built_in\">texture</span>(scene_texture, TexCoords);<br><br><span class=\"hljs-keyword\">if</span>(size &lt;= <span class=\"hljs-number\">0</span>) <span class=\"hljs-keyword\">return</span>;<br><span class=\"hljs-type\">float</span> mx = <span class=\"hljs-number\">0.0</span>;<br><span class=\"hljs-type\">vec4</span> cmx = FragColor;<br><br><span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">int</span> i = -size; i &lt;= size; ++i)<br>&#123;<br>\t<span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">int</span> j = -size; j &lt;= size; ++j)<br>\t&#123;<br>\t\t<span class=\"hljs-comment\">// dilate的形状可以多样，如圆形，矩形等等，根据采样点的形状来决定</span><br>\t\t<span class=\"hljs-comment\">// 这里使用圆形的dilate</span><br>\t\t<span class=\"hljs-keyword\">if</span>(<span class=\"hljs-built_in\">distance</span>(<span class=\"hljs-type\">vec2</span>(i,j), <span class=\"hljs-type\">vec2</span>(<span class=\"hljs-number\">0</span>)) &gt; size) <span class=\"hljs-keyword\">continue</span>;<br><br>\t\t<span class=\"hljs-comment\">// 采样区域内点的颜色，不要越界出去了</span><br>\t\t<span class=\"hljs-type\">vec2</span> sample_coord = TexCoords + <span class=\"hljs-type\">vec2</span>(i, j)*separation/tex_size;<br>\t\t<span class=\"hljs-keyword\">if</span>(sample_coord.x &gt; <span class=\"hljs-number\">1.0</span> || sample_coord.x &lt; <span class=\"hljs-number\">0.0</span> || sample_coord.y &gt; <span class=\"hljs-number\">1.0</span> || sample_coord.y &lt; <span class=\"hljs-number\">0.0</span>)<br>\t\t\t\t<span class=\"hljs-keyword\">continue</span>;<br><br>\t\t<span class=\"hljs-comment\">// 拿到采样点</span><br>\t\t<span class=\"hljs-type\">vec4</span> c = <span class=\"hljs-built_in\">texture</span>(scene_texture, sample_coord);<br><br>\t\t<span class=\"hljs-comment\">// 和目标颜色做点乘，得到一个灰度值</span><br>\t\t<span class=\"hljs-type\">float</span> mxt = <span class=\"hljs-built_in\">dot</span>(c.rgb, <span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">0.3</span>, <span class=\"hljs-number\">0.59</span>, <span class=\"hljs-number\">0.11</span>));<br><br>\t\t<span class=\"hljs-comment\">// 保存区域内灰度值最大的颜色</span><br>\t\t<span class=\"hljs-keyword\">if</span>(mxt &gt; mx)<br>\t\t&#123;<br>\t\t\tmx = mxt;<br>\t\t\tcmx = c;<br>\t\t&#125;<br>\t&#125;<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<p>这里我们计算最亮区域的方式是通过和一个目标颜色**(0.3, 0.59, 0.11)**来做点乘，当然也可以通过和其他的目标颜色，或者其他的方式来实现。</p>\n<p>最后，我们得到窗口区域内最亮的颜色，我们的最终颜色是原本颜色和最亮颜色的差值。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-comment\">// 模糊采样的颜色和原始颜色的mix上下限</span><br><span class=\"hljs-type\">float</span> min_threshold = <span class=\"hljs-number\">0.1</span>;<br><span class=\"hljs-type\">float</span> max_threshold = <span class=\"hljs-number\">0.3</span>;<br><br><span class=\"hljs-comment\">// 最终颜色是原来颜色和区域内灰度值最大的采样颜色的混合，有上下限做限制</span><br>FragColor.rgb = <span class=\"hljs-built_in\">mix</span>(FragColor.rgb, cmx.rgb, <span class=\"hljs-built_in\">smoothstep</span>(min_threshold, max_threshold, mx));<br></code></pre></td></tr></table></figure>\n<p>这里我们还是采用了一个上下限，尽量控制增亮的程度。</p>\n<h2 id=\"散景的效果\"><a href=\"#散景的效果\" class=\"headerlink\" title=\"散景的效果\"></a>散景的效果</h2><p>这里给出经过扩张模糊得到的散景效果。下面这张图是扩张模糊之前的效果。<br><img src=\"/2024/10/28/depth-of-field/dilate_before.png\" alt=\"扩张模糊前\"><br>下面这张图是扩张模糊之后的效果。<br><img src=\"/2024/10/28/depth-of-field/dilate_after.png\" alt=\"扩张模糊后\"></p>\n<p>当然在实际的场景中，我们可能不会开这么大的扩散效果，此处只是作为对比。</p>\n<h1 id=\"结合场景\"><a href=\"#结合场景\" class=\"headerlink\" title=\"结合场景\"></a>结合场景</h1><p>好了，现在我们有原本场景的渲染效果和扩散后的效果，实现最终的景深效果需要将这两者结合起来。我们需要一种方法来决定画面上哪些部分是需要采用清晰的图像，哪些是采用模糊的图像。</p>\n<p>为了实现这个效果我们需要获得画面上每个点的深度，或者每个点的实际世界坐标。幸运的是，我们在实现<a href=\"https://ruochenhua.github.io/2024/10/19/defer-render/\">延迟渲染</a>的时候已经将这些存放到缓冲中去了，接下来就是实现景深效果了。</p>\n<p>最终的代码如下：</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-keyword\">out</span> <span class=\"hljs-type\">vec4</span> FragColor;<br><span class=\"hljs-keyword\">in</span> <span class=\"hljs-type\">vec2</span> TexCoords;<br><br><span class=\"hljs-keyword\">uniform</span> <span class=\"hljs-type\">sampler2D</span> scene_texture;<br><span class=\"hljs-keyword\">uniform</span> <span class=\"hljs-type\">sampler2D</span> dilate_texture;<br><span class=\"hljs-keyword\">uniform</span> <span class=\"hljs-type\">sampler2D</span> position_texture;<br><br><span class=\"hljs-comment\">// 焦点距离</span><br><span class=\"hljs-keyword\">uniform</span> <span class=\"hljs-type\">float</span> focus_distance = <span class=\"hljs-number\">3.0</span>;<br><span class=\"hljs-keyword\">uniform</span> <span class=\"hljs-type\">vec2</span> focus_threshold;<br><span class=\"hljs-comment\">// 景深的上下限</span><br><span class=\"hljs-type\">float</span> min_dist = focus_threshold.x;<br><span class=\"hljs-type\">float</span> max_dist = focus_threshold.y;<br><br><span class=\"hljs-type\">void</span> main()<br>&#123;<br>    <span class=\"hljs-type\">vec4</span> focus_color = <span class=\"hljs-built_in\">texture</span>(scene_texture, TexCoords);<br>    <span class=\"hljs-type\">vec4</span> out_of_focus_color = <span class=\"hljs-built_in\">texture</span>(dilate_texture, TexCoords);<br>    <span class=\"hljs-type\">vec3</span> scene_position = <span class=\"hljs-built_in\">texture</span>(position_texture, TexCoords).xyz; <br>    <br>\t<span class=\"hljs-comment\">// 这里采用了画面每个点的世界坐标和相机的距离作为判定模糊的标准</span><br>\t<span class=\"hljs-comment\">// 当然用深度信息也是可以的，性能上也更好，这里为了代码展示更好理解</span><br>    <span class=\"hljs-type\">vec3</span> cam_pos = matrix_ubo.cam_pos.xyz;<br>\t<br>    <span class=\"hljs-type\">float</span> blur_amout = <span class=\"hljs-built_in\">smoothstep</span>(min_dist, max_dist, <span class=\"hljs-built_in\">abs</span>(focus_distance - <span class=\"hljs-built_in\">distance</span>(scene_position, cam_pos)));<br>    <br>\t<span class=\"hljs-comment\">// 最后的颜色是焦距内和散景的混合</span><br>    FragColor = <span class=\"hljs-built_in\">mix</span>(focus_color, out_of_focus_color, blur_amout);<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<p>这段代码理解起来应该没有什么太大的难度。</p>\n<h1 id=\"最终效果\"><a href=\"#最终效果\" class=\"headerlink\" title=\"最终效果\"></a>最终效果</h1><p>这里展示一下最终的效果。</p>\n<p><img src=\"/2024/10/28/depth-of-field/dof_near.png\" alt=\"近焦点\"><br><img src=\"/2024/10/28/depth-of-field/dof_far.png\" alt=\"远焦点\"></p>\n<p>上面两张图分别展示了不同焦点的景深的表现结果。可以明显看到不同景深效果的加入可以很容易的将画面上想要着重表达出来的部分勾勒出来。优秀的散景效果也能给画面增加不少美感（虽然我这个测试场景也没有什么美感可言…）。</p>\n","excerpt":"","more":"<h1 id=\"关于景深\"><a href=\"#关于景深\" class=\"headerlink\" title=\"关于景深\"></a>关于景深</h1><p>景深是一个常在摄像领域出现的词，它一般指的是沿着摄像机或其他成像器的拍摄方向上，能够取得清晰图像的成像所测定的被摄物体前后距离范围。用大白话就是说，拥有浅景深的成像器拍摄出来的效果，是只有焦点附近的图像是清晰的，其他地方的图像都是模糊的；而拥有大景深则可以在离焦点很远的地方也能有清晰的图像。</p>\n<p><img src=\"/2024/10/28/depth-of-field/dof_butterfly.JPG\" alt=\"一张浅景深的照片\"></p>\n<p>有了景深效果的图像可以有重点的突出核心想要表达的内容，不仅仅在摄影摄像的领域有非常多的应用，在游戏领域内也是应用广泛，可以表现出很独特的风格化美术效果（如八方旅人的浅景深效果）。</p>\n<p><img src=\"/2024/10/28/depth-of-field/game1.jpg\" alt=\"八方旅人\"></p>\n<p>下面我来介绍一种基础的景深效果实现，这个方法也在最近接入了<a href=\"https://github.com/ruochenhua/KongEngine\">KongEngine</a></p>\n<h1 id=\"渲染散景\"><a href=\"#渲染散景\" class=\"headerlink\" title=\"渲染散景\"></a>渲染散景</h1><p>浅景深的主要应用是通过调整不同焦距上物体的成像清晰程度来突出渲染画面的重点。清晰的部分我们已经掌握了，就是正常的将场景渲染出来，那不清晰的部分（或者叫做散景）也有不少的实现方式，一般是通过模糊算法来实现。</p>\n<p>模糊的算法有很多种，比如box blur，gaussian blur等等，效果最好的是扩张模糊(dilate blur)，这也是我们会采取的方法。</p>\n<h2 id=\"扩张模糊（dilate-blur\"><a href=\"#扩张模糊（dilate-blur\" class=\"headerlink\" title=\"扩张模糊（dilate blur)\"></a>扩张模糊（dilate blur)</h2><p>扩张模糊它的主要方式，是在给定一个模糊的窗口下，取得这个窗口下的最亮的颜色记录下来，然后将这个最亮的颜色扩张到整个窗口。这样一来经过扩张模糊的画面会有一种亮晶晶，并且很柔和的效果，很适合作为散景的表现效果。</p>\n<p>dilate blur的计算也比较简单，首先我们定义此次blur的窗口大小已经采样的间隔大小。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-comment\">// 窗口的大小，数值越大扩散越大，消耗越高 </span><br><span class=\"hljs-type\">int</span> size = <span class=\"hljs-number\">5</span>;\t<br><span class=\"hljs-comment\">// 采样间隔的大小，数值越大扩散越大，效果降低</span><br><span class=\"hljs-type\">float</span> separation = <span class=\"hljs-number\">1.0</span>;<br></code></pre></td></tr></table></figure>\n\n<p>在定义了窗口的尺寸之后，我们在窗口的范围内记录最亮的像素颜色，并保存下来。<strong>窗口的形状不限</strong>，可以是矩形，或者圆形。我们这里实现采取圆形的窗口。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-comment\">// 渲染场景的尺寸</span><br><span class=\"hljs-type\">vec2</span> tex_size = <span class=\"hljs-type\">vec2</span>(<span class=\"hljs-built_in\">textureSize</span>(scene_texture, <span class=\"hljs-number\">0</span>).xy);<br><span class=\"hljs-comment\">// 获取场景的原本颜色</span><br>FragColor = <span class=\"hljs-built_in\">texture</span>(scene_texture, TexCoords);<br><br><span class=\"hljs-keyword\">if</span>(size &lt;= <span class=\"hljs-number\">0</span>) <span class=\"hljs-keyword\">return</span>;<br><span class=\"hljs-type\">float</span> mx = <span class=\"hljs-number\">0.0</span>;<br><span class=\"hljs-type\">vec4</span> cmx = FragColor;<br><br><span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">int</span> i = -size; i &lt;= size; ++i)<br>&#123;<br>\t<span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">int</span> j = -size; j &lt;= size; ++j)<br>\t&#123;<br>\t\t<span class=\"hljs-comment\">// dilate的形状可以多样，如圆形，矩形等等，根据采样点的形状来决定</span><br>\t\t<span class=\"hljs-comment\">// 这里使用圆形的dilate</span><br>\t\t<span class=\"hljs-keyword\">if</span>(<span class=\"hljs-built_in\">distance</span>(<span class=\"hljs-type\">vec2</span>(i,j), <span class=\"hljs-type\">vec2</span>(<span class=\"hljs-number\">0</span>)) &gt; size) <span class=\"hljs-keyword\">continue</span>;<br><br>\t\t<span class=\"hljs-comment\">// 采样区域内点的颜色，不要越界出去了</span><br>\t\t<span class=\"hljs-type\">vec2</span> sample_coord = TexCoords + <span class=\"hljs-type\">vec2</span>(i, j)*separation/tex_size;<br>\t\t<span class=\"hljs-keyword\">if</span>(sample_coord.x &gt; <span class=\"hljs-number\">1.0</span> || sample_coord.x &lt; <span class=\"hljs-number\">0.0</span> || sample_coord.y &gt; <span class=\"hljs-number\">1.0</span> || sample_coord.y &lt; <span class=\"hljs-number\">0.0</span>)<br>\t\t\t\t<span class=\"hljs-keyword\">continue</span>;<br><br>\t\t<span class=\"hljs-comment\">// 拿到采样点</span><br>\t\t<span class=\"hljs-type\">vec4</span> c = <span class=\"hljs-built_in\">texture</span>(scene_texture, sample_coord);<br><br>\t\t<span class=\"hljs-comment\">// 和目标颜色做点乘，得到一个灰度值</span><br>\t\t<span class=\"hljs-type\">float</span> mxt = <span class=\"hljs-built_in\">dot</span>(c.rgb, <span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">0.3</span>, <span class=\"hljs-number\">0.59</span>, <span class=\"hljs-number\">0.11</span>));<br><br>\t\t<span class=\"hljs-comment\">// 保存区域内灰度值最大的颜色</span><br>\t\t<span class=\"hljs-keyword\">if</span>(mxt &gt; mx)<br>\t\t&#123;<br>\t\t\tmx = mxt;<br>\t\t\tcmx = c;<br>\t\t&#125;<br>\t&#125;<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<p>这里我们计算最亮区域的方式是通过和一个目标颜色**(0.3, 0.59, 0.11)**来做点乘，当然也可以通过和其他的目标颜色，或者其他的方式来实现。</p>\n<p>最后，我们得到窗口区域内最亮的颜色，我们的最终颜色是原本颜色和最亮颜色的差值。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-comment\">// 模糊采样的颜色和原始颜色的mix上下限</span><br><span class=\"hljs-type\">float</span> min_threshold = <span class=\"hljs-number\">0.1</span>;<br><span class=\"hljs-type\">float</span> max_threshold = <span class=\"hljs-number\">0.3</span>;<br><br><span class=\"hljs-comment\">// 最终颜色是原来颜色和区域内灰度值最大的采样颜色的混合，有上下限做限制</span><br>FragColor.rgb = <span class=\"hljs-built_in\">mix</span>(FragColor.rgb, cmx.rgb, <span class=\"hljs-built_in\">smoothstep</span>(min_threshold, max_threshold, mx));<br></code></pre></td></tr></table></figure>\n<p>这里我们还是采用了一个上下限，尽量控制增亮的程度。</p>\n<h2 id=\"散景的效果\"><a href=\"#散景的效果\" class=\"headerlink\" title=\"散景的效果\"></a>散景的效果</h2><p>这里给出经过扩张模糊得到的散景效果。下面这张图是扩张模糊之前的效果。<br><img src=\"/2024/10/28/depth-of-field/dilate_before.png\" alt=\"扩张模糊前\"><br>下面这张图是扩张模糊之后的效果。<br><img src=\"/2024/10/28/depth-of-field/dilate_after.png\" alt=\"扩张模糊后\"></p>\n<p>当然在实际的场景中，我们可能不会开这么大的扩散效果，此处只是作为对比。</p>\n<h1 id=\"结合场景\"><a href=\"#结合场景\" class=\"headerlink\" title=\"结合场景\"></a>结合场景</h1><p>好了，现在我们有原本场景的渲染效果和扩散后的效果，实现最终的景深效果需要将这两者结合起来。我们需要一种方法来决定画面上哪些部分是需要采用清晰的图像，哪些是采用模糊的图像。</p>\n<p>为了实现这个效果我们需要获得画面上每个点的深度，或者每个点的实际世界坐标。幸运的是，我们在实现<a href=\"https://ruochenhua.github.io/2024/10/19/defer-render/\">延迟渲染</a>的时候已经将这些存放到缓冲中去了，接下来就是实现景深效果了。</p>\n<p>最终的代码如下：</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-keyword\">out</span> <span class=\"hljs-type\">vec4</span> FragColor;<br><span class=\"hljs-keyword\">in</span> <span class=\"hljs-type\">vec2</span> TexCoords;<br><br><span class=\"hljs-keyword\">uniform</span> <span class=\"hljs-type\">sampler2D</span> scene_texture;<br><span class=\"hljs-keyword\">uniform</span> <span class=\"hljs-type\">sampler2D</span> dilate_texture;<br><span class=\"hljs-keyword\">uniform</span> <span class=\"hljs-type\">sampler2D</span> position_texture;<br><br><span class=\"hljs-comment\">// 焦点距离</span><br><span class=\"hljs-keyword\">uniform</span> <span class=\"hljs-type\">float</span> focus_distance = <span class=\"hljs-number\">3.0</span>;<br><span class=\"hljs-keyword\">uniform</span> <span class=\"hljs-type\">vec2</span> focus_threshold;<br><span class=\"hljs-comment\">// 景深的上下限</span><br><span class=\"hljs-type\">float</span> min_dist = focus_threshold.x;<br><span class=\"hljs-type\">float</span> max_dist = focus_threshold.y;<br><br><span class=\"hljs-type\">void</span> main()<br>&#123;<br>    <span class=\"hljs-type\">vec4</span> focus_color = <span class=\"hljs-built_in\">texture</span>(scene_texture, TexCoords);<br>    <span class=\"hljs-type\">vec4</span> out_of_focus_color = <span class=\"hljs-built_in\">texture</span>(dilate_texture, TexCoords);<br>    <span class=\"hljs-type\">vec3</span> scene_position = <span class=\"hljs-built_in\">texture</span>(position_texture, TexCoords).xyz; <br>    <br>\t<span class=\"hljs-comment\">// 这里采用了画面每个点的世界坐标和相机的距离作为判定模糊的标准</span><br>\t<span class=\"hljs-comment\">// 当然用深度信息也是可以的，性能上也更好，这里为了代码展示更好理解</span><br>    <span class=\"hljs-type\">vec3</span> cam_pos = matrix_ubo.cam_pos.xyz;<br>\t<br>    <span class=\"hljs-type\">float</span> blur_amout = <span class=\"hljs-built_in\">smoothstep</span>(min_dist, max_dist, <span class=\"hljs-built_in\">abs</span>(focus_distance - <span class=\"hljs-built_in\">distance</span>(scene_position, cam_pos)));<br>    <br>\t<span class=\"hljs-comment\">// 最后的颜色是焦距内和散景的混合</span><br>    FragColor = <span class=\"hljs-built_in\">mix</span>(focus_color, out_of_focus_color, blur_amout);<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<p>这段代码理解起来应该没有什么太大的难度。</p>\n<h1 id=\"最终效果\"><a href=\"#最终效果\" class=\"headerlink\" title=\"最终效果\"></a>最终效果</h1><p>这里展示一下最终的效果。</p>\n<p><img src=\"/2024/10/28/depth-of-field/dof_near.png\" alt=\"近焦点\"><br><img src=\"/2024/10/28/depth-of-field/dof_far.png\" alt=\"远焦点\"></p>\n<p>上面两张图分别展示了不同焦点的景深的表现结果。可以明显看到不同景深效果的加入可以很容易的将画面上想要着重表达出来的部分勾勒出来。优秀的散景效果也能给画面增加不少美感（虽然我这个测试场景也没有什么美感可言…）。</p>\n"},{"title":"反射阴影贴图","date":"2024-11-24T09:45:42.000Z","index_img":"/2024/11/24/reflective-shadow-map/rsm_on_suit.png","banner_img":"/2024/11/24/reflective-shadow-map/rsm_on_suit.png","_content":"\n# 反射阴影贴图简介\n反射阴影贴图（Reflective Shadow Map）是实现全局光照效果的一个非常经典的方法，它是在这篇[论文](https://users.soe.ucsc.edu/~pang/160/s13/proposal/mijallen/proposal/media/p203-dachsbacher.pdf)中被提出。\n\n## 直接光照和间接光照\n它的名字中带有“阴影贴图（Shadow Map）”几个字，所以乍看之下这个方法似乎是用来解决阴影问题，或者是提升阴影效果的。其实不然，它是用来解决**间接光照**的问题的方法。\n\n在一般的场景中，光照可以大致分为两类：\n - 一类是直接光照，也就是物体被光源直接照亮的部分。这个类型的光照是比较好计算的，通过光源的入射角，物体表面的法线和材质，以及观察的方向等等，利用**PBR**的方法能够得到非常不错的效果，这个流程在KongEngine中已经基本实现了。\n - 另外一个类型是间接光照，它代表的是光线经过一次甚至多次反射后照亮物体的部分。相对于直接光照，间接光照十分复杂，因为光线可能经过多次反射，想要实时的计算光的多次反射的完整路径是很难实现的。但是如果不包含间接光照的话，场景的真实度会大打折扣。在最基础的PBR渲染框架中，我们可以选择手动输入一个环境光照（Ambient Light）的颜色，可以简单的表现全局光照，但是真实性还是不够。\n\n反射阴影贴图（下面简称**RSM**）这个方法，就是用于解决实时模拟间接光照的问题。\n\n## RSM算法基本介绍\n\nRSM的核心思想是将被光源直接照亮的区域再次作为光源进行光照计算，这就是光的一次反射。RSM只计算一次光反射，因为一般来说光的第一次反射的能量残留相对来说是最大的，对场景来说有较为显著的影响，后面的反射对场景影响较小，为了性能考量可以忽略。\n\n那么怎么知道光照直接亮了哪些区域呢？其实非常简单，在参考实现阴影贴图（Shadow Map）的概念，从光源视角下进行渲染，不在阴影中的区域就是被光源直接照亮的。另外，由于光线在漫反射时会被照亮区域的材质所影响（比如说白色的光线从红色的墙反射会变成红色，因为其他颜色被吸收，同时不同粗糙度的物质反射方式也不一样），以及照亮区域的位置和法线也会影响计算光反射的方向，因此我们在计算阴影贴图的时候，还需要保存照亮区域的颜色、世界位置、法线等数据。\n\n下面是需要记录的数据的截图，从左到右分别是：深度、位置坐标、法线、颜色。\n![光源方向记录的数据](rsm_info.png)\n\n\n因此接下来光照计算可以分为以下几步：\n1. 首先从光源的位置和方向渲染场景，将光源视角的信息（深度，世界位置，世界法线等等）缓存到buffer中。\n2. 计算光照直接对环境的影响。\n3. 将第一步缓存的光源保存的信息加入到场景中光照的计算，加上阴影（阴影贴图）和间接光照（反射阴影贴图）。\n\n\n# 实现反射阴影贴图的步骤\n下面是在KongEngine中实现RSM的步骤。\n## 一些前期准备\n\nRSM需要将一些信息存储到buffer中，所以很首先需要设置新的缓冲。\n```c++\nglGenFramebuffers(1, &rsm_fbo);\nglBindFramebuffer(GL_FRAMEBUFFER, rsm_fbo);\n\n// 位置数据\nglGenTextures(1, &rsm_world_position);\nglBindTexture(GL_TEXTURE_2D, rsm_world_position);\nglTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA32F, SHADOW_RESOLUTION, SHADOW_RESOLUTION, 0, GL_RGBA, GL_FLOAT, nullptr);\nglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);\nglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);\nglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);\nglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);\n\nglFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, rsm_world_position, 0);\n\n// 法线数据\nglGenTextures(1, &rsm_world_normal);\nglBindTexture(GL_TEXTURE_2D, rsm_world_normal);\nglTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA32F, SHADOW_RESOLUTION, SHADOW_RESOLUTION, 0, GL_RGBA, GL_FLOAT, nullptr);\nglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);\nglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);\nglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);\nglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);\nglFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT1, GL_TEXTURE_2D, rsm_world_normal, 0);\n\n// flux数据\nglGenTextures(1, &rsm_world_flux);\nglBindTexture(GL_TEXTURE_2D, rsm_world_flux);\nglTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA32F, SHADOW_RESOLUTION, SHADOW_RESOLUTION, 0, GL_RGBA, GL_FLOAT, nullptr);\nglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);\nglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);\nglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);\nglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);\nglFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT2, GL_TEXTURE_2D, rsm_world_flux, 0);\n\n// 生成renderbuffer\nglGenRenderbuffers(1, &rsm_depth);\nglBindRenderbuffer(GL_RENDERBUFFER, rsm_depth);\nglRenderbufferStorage(GL_RENDERBUFFER, GL_DEPTH_COMPONENT, SHADOW_RESOLUTION, SHADOW_RESOLUTION);\nglFramebufferRenderbuffer(GL_FRAMEBUFFER, GL_DEPTH_ATTACHMENT, GL_RENDERBUFFER, rsm_depth);\nglEnable(GL_DEPTH_TEST);\n\nGLuint g_attachments[] = {GL_COLOR_ATTACHMENT0, GL_COLOR_ATTACHMENT1, GL_COLOR_ATTACHMENT2}; \nglDrawBuffers(3, g_attachments);\n```\n上面的部分用于构建RSM的缓冲，这些内容和之前的Shadowmap的流程类似，也可以考虑将其和Shadowmap的缓冲合并，不过为了方便自己理解目前是新建了一个。\n\n另外RSM也新建了一个独立的shader\n```c++\nmap<EShaderType, string> shader_path_map = {\n    {EShaderType::vs, CSceneLoader::ToResourcePath(\"shader/shadow/reflective_shadowmap.vert\")},\n    {EShaderType::fs, CSceneLoader::ToResourcePath(\"shader/shadow/reflective_shadowmap.frag\")}\n};\nrsm_shader = make_shared<Shader>(shader_path_map);\n```\n\nshader的内容十分简单，*顶点着色器*简单的将顶点的世界坐标和法线传给片段着色器。\n```glsl\n#version 450 compatibility\n\n// Input vertex data, different for all executions of this shader.\nlayout(location = 0) in vec3 in_pos;\nlayout(location = 1) in vec3 in_normal;\n\n// Values that stay constant for the whole mesh.\nuniform mat4 light_space_mat;\nuniform mat4 model;\n\nout vec4 frag_pos;\nout vec3 frag_normal;\n\nvoid main(){\n\tgl_Position =  light_space_mat * model * vec4(in_pos, 1);\n    frag_pos = model * vec4(in_pos, 1);\n\tfrag_normal = normalize(mat3(transpose(inverse(model))) * in_normal);\n}\n```\n由于这个shader是从光源视角渲染的，所以**gl_Position**是由光源的**light_space_mat**对世界坐标做变换。\n\n*片段着色器*将RSM所需的内容存储起来。\n```glsl\n#version 450 compatibility\n\nlayout(location = 0) out vec4 world_pos;\nlayout(location = 1) out vec4 world_normal;\nlayout(location = 2) out vec4 world_flux;\n\nin vec4 frag_pos;\nin vec3 frag_normal;\n\nuniform vec4 albedo;\nuniform float light_intensity;\n\nvoid main()\n{\n\tworld_pos = frag_pos;\n    world_normal = vec4(frag_normal, 1);\n    world_flux = albedo;// * light_intensity;\n}\n```\n我们这里分别将**世界坐标、世界法线和颜色**存储了到了贴图中。\n方便起见，KongEngine暂时只支持平行光源的RSM效果，点光源的目前不支持。\n\n现在我们的平行光源已经有了RSM相关的信息了，在计算光照的时候将这些贴图信息传到光照计算的shader中。\n\n```c++\nglActiveTexture(GL_TEXTURE0 + DIRLIGHT_RSM_WORLD_POS);\nglBindTexture(GL_TEXTURE_2D, rsm_world_pos);\nglActiveTexture(GL_TEXTURE0 + DIRLIGHT_RSM_WORLD_NORMAL);\nglBindTexture(GL_TEXTURE_2D, rsm_world_normal);\nglActiveTexture(GL_TEXTURE0 + DIRLIGHT_RSM_WORLD_FLUX);\nglBindTexture(GL_TEXTURE_2D, rsm_world_flux);\n```\n\n## 间接光源的判定\nRSM的实际原理如下面两张图所示：\n![rsm原理图1](rsm_principle_1.png)\n\n假如当前我们片段着色器计算的是**X**点的光照，在这个场景中，x点被桌子的阴影挡住了，并没有被光源直接照亮，如左图所示。所以x点的直接光照为0。\n\n接下来是间接光照的部分。如上面所说，我们将被光源直接照亮的部分当做光源，这里先以被光源直接照射的两个点Xp和Xq来做判断。Xp点被光源照亮，他的法线是Np，光在Xp点散射后是有可能到达X点的，在数学上的判断就是Np和Xp到X连线的点乘大于0。而Xq的法线Nq和Xq到X点连线的点乘小于0，可以从图上看到光在Xq点散射后是无法到达X点的。\n\n当然我们还知道，在计算PBR的时候，不同的材质的光线散射形状是不一致的，在图中的表现就是，光线散射后沿着XpX方向的分量，比沿着XpY方向的分量是要小的。因此间接光源的法线和两点之间的连线的点乘大小有这判定间接光源亮度的作用。\n\n下面这张图和原理图1是一样的，强化一下理解。\n![rsm原理图2](rsm_principle_2.png)\n\n## 采样间接光源\n之前说到，需要用被光源照亮的点作为间接光源。如果渲染屏幕上像素点的时候对所有照亮的点都去做判断的话，理论上是可以得到最好的效果，但是性能上会有极大的消耗；相反如果采样点过少的话，计算速度虽然是上去了但是效果会大打折扣。\n\n因此一个优化方法是通过重要性采样。我们判断离当前渲染点越近的间接光照光源对当前点的最终效果影响就越大，因此离当前点近的间接光源采样点就会越多。并且，为了弥补远处的采样点过少可能带来的问题，引入权重的概念，随着采样点离当前点越近，权重越小。\n\n![rsm采样点选择](rsm_sample.png)\n\n下面是采样点初始化的示例代码：\n```c++\n// rsm采样点初始化\nstd::default_random_engine e;\nstd::uniform_real_distribution<float> u(0.0, 1.0);\nfloat pi_num = pi<float>();\nfor(int i = 0; i < rsm_sample_count; i++)\n{\n    float xi1 = u(e);\n    float xi2 = u(e);\n    \n    rsm_samples_and_weights.push_back(vec4(xi1*sin(2*pi_num*xi2), xi1*cos(2*pi_num*xi2), xi1*xi1, 0.0));\n}\n```\n\n结合上面的两个思想，下面是部分最终代码的呈现，位于defer_pbr.frag中。\n```glsl\nfloat max_sample_radius = 128.;\nfor (int i = 0; i < rsm_sample_count; ++i) \n{\n    vec3 rsm_sample_and_weight = rsm_samples_and_weights[i].xyz;\n    vec2 uv = proj_coord.xy + max_sample_radius * rsm_sample_and_weight.xy * texel_size;\n    vec3 flux = texture(rsm_world_flux, uv).rgb;\n    vec3 x_p = texture(rsm_world_pos, uv).xyz;\n    vec3 n_p = texture(rsm_world_normal, uv).xyz;\n\n    vec3 r = normalize(frag_world_pos.xyz - x_p);\n\n    float d2 = dot(r, r);\n    vec3 e_p = flux * (max(0.0, dot(n_p, r)) * max(0.0, dot(in_normal, -r))) * rsm_sample_and_weight.z;\n    //e_p *= pow(rsm_sample_offsets[i].x / d2, 2);\n    env_color += e_p;\n}\nenv_color /= rsm_sample_count;\n```\n\n# 最终效果\n结合了反射阴影贴图后，场景会有一些间接光照效果了，下面是同一个场景的表现效果。\n![rsm关闭1](rsm_off_sphere.png)\n![rsm开启1](rsm_on_sphere.png)\n\n这里是另外一组。\n![rsm关闭2](rsm_off_suit.png)\n![rsm开启2](rsm_on_suit.png)\n\n可以看到开启rsm后，靠近红色和绿色墙壁，且没有被光源直接照亮（处于阴影）的部分，被墙壁的散射光源间接点亮了。灰色的球体和人物模型“沾染”上了墙壁的颜色。这种间接光照的影响使得场景变得更加的真实。\n\n但是，当前的rsm也并不是完美的，比如说目前rsm缺乏判定间接光源是否可达，在第二个例子中，人物模型的右肩上的间接光源呈现的是黄色，也就是红色和绿色的间接光照结合起来的颜色。但是右肩理论上不应该出现红色的分量，因为红色的部分会被身体部位阻挡。渲染点的法线应该也会影响间接光的表现。\n\n另外就是被当成间接光源的只有被光源照亮且存储起来的部分区域，也就是说间接光源的采样范围相对来说还是比较局限的，不可能采样非常大的区域。在KongEngine中由于采用了CSM来处理阴影，RSM的范围和CSM的最小级的阴影范围采样是一致的，这种处理显然无法照顾大的场景。\n\n场景的间接光照还需要进一步的去优化，RSM只是其中一个小部分。","source":"_posts/reflective-shadow-map.md","raw":"---\ntitle: 反射阴影贴图\ndate: 2024-11-24 17:45:42\ncategories: \n\t- 技术漫谈\ntags: [3D, render, 渲染, 编程]\nindex_img: /2024/11/24/reflective-shadow-map/rsm_on_suit.png\nbanner_img: /2024/11/24/reflective-shadow-map/rsm_on_suit.png\n---\n\n# 反射阴影贴图简介\n反射阴影贴图（Reflective Shadow Map）是实现全局光照效果的一个非常经典的方法，它是在这篇[论文](https://users.soe.ucsc.edu/~pang/160/s13/proposal/mijallen/proposal/media/p203-dachsbacher.pdf)中被提出。\n\n## 直接光照和间接光照\n它的名字中带有“阴影贴图（Shadow Map）”几个字，所以乍看之下这个方法似乎是用来解决阴影问题，或者是提升阴影效果的。其实不然，它是用来解决**间接光照**的问题的方法。\n\n在一般的场景中，光照可以大致分为两类：\n - 一类是直接光照，也就是物体被光源直接照亮的部分。这个类型的光照是比较好计算的，通过光源的入射角，物体表面的法线和材质，以及观察的方向等等，利用**PBR**的方法能够得到非常不错的效果，这个流程在KongEngine中已经基本实现了。\n - 另外一个类型是间接光照，它代表的是光线经过一次甚至多次反射后照亮物体的部分。相对于直接光照，间接光照十分复杂，因为光线可能经过多次反射，想要实时的计算光的多次反射的完整路径是很难实现的。但是如果不包含间接光照的话，场景的真实度会大打折扣。在最基础的PBR渲染框架中，我们可以选择手动输入一个环境光照（Ambient Light）的颜色，可以简单的表现全局光照，但是真实性还是不够。\n\n反射阴影贴图（下面简称**RSM**）这个方法，就是用于解决实时模拟间接光照的问题。\n\n## RSM算法基本介绍\n\nRSM的核心思想是将被光源直接照亮的区域再次作为光源进行光照计算，这就是光的一次反射。RSM只计算一次光反射，因为一般来说光的第一次反射的能量残留相对来说是最大的，对场景来说有较为显著的影响，后面的反射对场景影响较小，为了性能考量可以忽略。\n\n那么怎么知道光照直接亮了哪些区域呢？其实非常简单，在参考实现阴影贴图（Shadow Map）的概念，从光源视角下进行渲染，不在阴影中的区域就是被光源直接照亮的。另外，由于光线在漫反射时会被照亮区域的材质所影响（比如说白色的光线从红色的墙反射会变成红色，因为其他颜色被吸收，同时不同粗糙度的物质反射方式也不一样），以及照亮区域的位置和法线也会影响计算光反射的方向，因此我们在计算阴影贴图的时候，还需要保存照亮区域的颜色、世界位置、法线等数据。\n\n下面是需要记录的数据的截图，从左到右分别是：深度、位置坐标、法线、颜色。\n![光源方向记录的数据](rsm_info.png)\n\n\n因此接下来光照计算可以分为以下几步：\n1. 首先从光源的位置和方向渲染场景，将光源视角的信息（深度，世界位置，世界法线等等）缓存到buffer中。\n2. 计算光照直接对环境的影响。\n3. 将第一步缓存的光源保存的信息加入到场景中光照的计算，加上阴影（阴影贴图）和间接光照（反射阴影贴图）。\n\n\n# 实现反射阴影贴图的步骤\n下面是在KongEngine中实现RSM的步骤。\n## 一些前期准备\n\nRSM需要将一些信息存储到buffer中，所以很首先需要设置新的缓冲。\n```c++\nglGenFramebuffers(1, &rsm_fbo);\nglBindFramebuffer(GL_FRAMEBUFFER, rsm_fbo);\n\n// 位置数据\nglGenTextures(1, &rsm_world_position);\nglBindTexture(GL_TEXTURE_2D, rsm_world_position);\nglTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA32F, SHADOW_RESOLUTION, SHADOW_RESOLUTION, 0, GL_RGBA, GL_FLOAT, nullptr);\nglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);\nglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);\nglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);\nglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);\n\nglFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, rsm_world_position, 0);\n\n// 法线数据\nglGenTextures(1, &rsm_world_normal);\nglBindTexture(GL_TEXTURE_2D, rsm_world_normal);\nglTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA32F, SHADOW_RESOLUTION, SHADOW_RESOLUTION, 0, GL_RGBA, GL_FLOAT, nullptr);\nglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);\nglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);\nglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);\nglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);\nglFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT1, GL_TEXTURE_2D, rsm_world_normal, 0);\n\n// flux数据\nglGenTextures(1, &rsm_world_flux);\nglBindTexture(GL_TEXTURE_2D, rsm_world_flux);\nglTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA32F, SHADOW_RESOLUTION, SHADOW_RESOLUTION, 0, GL_RGBA, GL_FLOAT, nullptr);\nglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);\nglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);\nglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);\nglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);\nglFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT2, GL_TEXTURE_2D, rsm_world_flux, 0);\n\n// 生成renderbuffer\nglGenRenderbuffers(1, &rsm_depth);\nglBindRenderbuffer(GL_RENDERBUFFER, rsm_depth);\nglRenderbufferStorage(GL_RENDERBUFFER, GL_DEPTH_COMPONENT, SHADOW_RESOLUTION, SHADOW_RESOLUTION);\nglFramebufferRenderbuffer(GL_FRAMEBUFFER, GL_DEPTH_ATTACHMENT, GL_RENDERBUFFER, rsm_depth);\nglEnable(GL_DEPTH_TEST);\n\nGLuint g_attachments[] = {GL_COLOR_ATTACHMENT0, GL_COLOR_ATTACHMENT1, GL_COLOR_ATTACHMENT2}; \nglDrawBuffers(3, g_attachments);\n```\n上面的部分用于构建RSM的缓冲，这些内容和之前的Shadowmap的流程类似，也可以考虑将其和Shadowmap的缓冲合并，不过为了方便自己理解目前是新建了一个。\n\n另外RSM也新建了一个独立的shader\n```c++\nmap<EShaderType, string> shader_path_map = {\n    {EShaderType::vs, CSceneLoader::ToResourcePath(\"shader/shadow/reflective_shadowmap.vert\")},\n    {EShaderType::fs, CSceneLoader::ToResourcePath(\"shader/shadow/reflective_shadowmap.frag\")}\n};\nrsm_shader = make_shared<Shader>(shader_path_map);\n```\n\nshader的内容十分简单，*顶点着色器*简单的将顶点的世界坐标和法线传给片段着色器。\n```glsl\n#version 450 compatibility\n\n// Input vertex data, different for all executions of this shader.\nlayout(location = 0) in vec3 in_pos;\nlayout(location = 1) in vec3 in_normal;\n\n// Values that stay constant for the whole mesh.\nuniform mat4 light_space_mat;\nuniform mat4 model;\n\nout vec4 frag_pos;\nout vec3 frag_normal;\n\nvoid main(){\n\tgl_Position =  light_space_mat * model * vec4(in_pos, 1);\n    frag_pos = model * vec4(in_pos, 1);\n\tfrag_normal = normalize(mat3(transpose(inverse(model))) * in_normal);\n}\n```\n由于这个shader是从光源视角渲染的，所以**gl_Position**是由光源的**light_space_mat**对世界坐标做变换。\n\n*片段着色器*将RSM所需的内容存储起来。\n```glsl\n#version 450 compatibility\n\nlayout(location = 0) out vec4 world_pos;\nlayout(location = 1) out vec4 world_normal;\nlayout(location = 2) out vec4 world_flux;\n\nin vec4 frag_pos;\nin vec3 frag_normal;\n\nuniform vec4 albedo;\nuniform float light_intensity;\n\nvoid main()\n{\n\tworld_pos = frag_pos;\n    world_normal = vec4(frag_normal, 1);\n    world_flux = albedo;// * light_intensity;\n}\n```\n我们这里分别将**世界坐标、世界法线和颜色**存储了到了贴图中。\n方便起见，KongEngine暂时只支持平行光源的RSM效果，点光源的目前不支持。\n\n现在我们的平行光源已经有了RSM相关的信息了，在计算光照的时候将这些贴图信息传到光照计算的shader中。\n\n```c++\nglActiveTexture(GL_TEXTURE0 + DIRLIGHT_RSM_WORLD_POS);\nglBindTexture(GL_TEXTURE_2D, rsm_world_pos);\nglActiveTexture(GL_TEXTURE0 + DIRLIGHT_RSM_WORLD_NORMAL);\nglBindTexture(GL_TEXTURE_2D, rsm_world_normal);\nglActiveTexture(GL_TEXTURE0 + DIRLIGHT_RSM_WORLD_FLUX);\nglBindTexture(GL_TEXTURE_2D, rsm_world_flux);\n```\n\n## 间接光源的判定\nRSM的实际原理如下面两张图所示：\n![rsm原理图1](rsm_principle_1.png)\n\n假如当前我们片段着色器计算的是**X**点的光照，在这个场景中，x点被桌子的阴影挡住了，并没有被光源直接照亮，如左图所示。所以x点的直接光照为0。\n\n接下来是间接光照的部分。如上面所说，我们将被光源直接照亮的部分当做光源，这里先以被光源直接照射的两个点Xp和Xq来做判断。Xp点被光源照亮，他的法线是Np，光在Xp点散射后是有可能到达X点的，在数学上的判断就是Np和Xp到X连线的点乘大于0。而Xq的法线Nq和Xq到X点连线的点乘小于0，可以从图上看到光在Xq点散射后是无法到达X点的。\n\n当然我们还知道，在计算PBR的时候，不同的材质的光线散射形状是不一致的，在图中的表现就是，光线散射后沿着XpX方向的分量，比沿着XpY方向的分量是要小的。因此间接光源的法线和两点之间的连线的点乘大小有这判定间接光源亮度的作用。\n\n下面这张图和原理图1是一样的，强化一下理解。\n![rsm原理图2](rsm_principle_2.png)\n\n## 采样间接光源\n之前说到，需要用被光源照亮的点作为间接光源。如果渲染屏幕上像素点的时候对所有照亮的点都去做判断的话，理论上是可以得到最好的效果，但是性能上会有极大的消耗；相反如果采样点过少的话，计算速度虽然是上去了但是效果会大打折扣。\n\n因此一个优化方法是通过重要性采样。我们判断离当前渲染点越近的间接光照光源对当前点的最终效果影响就越大，因此离当前点近的间接光源采样点就会越多。并且，为了弥补远处的采样点过少可能带来的问题，引入权重的概念，随着采样点离当前点越近，权重越小。\n\n![rsm采样点选择](rsm_sample.png)\n\n下面是采样点初始化的示例代码：\n```c++\n// rsm采样点初始化\nstd::default_random_engine e;\nstd::uniform_real_distribution<float> u(0.0, 1.0);\nfloat pi_num = pi<float>();\nfor(int i = 0; i < rsm_sample_count; i++)\n{\n    float xi1 = u(e);\n    float xi2 = u(e);\n    \n    rsm_samples_and_weights.push_back(vec4(xi1*sin(2*pi_num*xi2), xi1*cos(2*pi_num*xi2), xi1*xi1, 0.0));\n}\n```\n\n结合上面的两个思想，下面是部分最终代码的呈现，位于defer_pbr.frag中。\n```glsl\nfloat max_sample_radius = 128.;\nfor (int i = 0; i < rsm_sample_count; ++i) \n{\n    vec3 rsm_sample_and_weight = rsm_samples_and_weights[i].xyz;\n    vec2 uv = proj_coord.xy + max_sample_radius * rsm_sample_and_weight.xy * texel_size;\n    vec3 flux = texture(rsm_world_flux, uv).rgb;\n    vec3 x_p = texture(rsm_world_pos, uv).xyz;\n    vec3 n_p = texture(rsm_world_normal, uv).xyz;\n\n    vec3 r = normalize(frag_world_pos.xyz - x_p);\n\n    float d2 = dot(r, r);\n    vec3 e_p = flux * (max(0.0, dot(n_p, r)) * max(0.0, dot(in_normal, -r))) * rsm_sample_and_weight.z;\n    //e_p *= pow(rsm_sample_offsets[i].x / d2, 2);\n    env_color += e_p;\n}\nenv_color /= rsm_sample_count;\n```\n\n# 最终效果\n结合了反射阴影贴图后，场景会有一些间接光照效果了，下面是同一个场景的表现效果。\n![rsm关闭1](rsm_off_sphere.png)\n![rsm开启1](rsm_on_sphere.png)\n\n这里是另外一组。\n![rsm关闭2](rsm_off_suit.png)\n![rsm开启2](rsm_on_suit.png)\n\n可以看到开启rsm后，靠近红色和绿色墙壁，且没有被光源直接照亮（处于阴影）的部分，被墙壁的散射光源间接点亮了。灰色的球体和人物模型“沾染”上了墙壁的颜色。这种间接光照的影响使得场景变得更加的真实。\n\n但是，当前的rsm也并不是完美的，比如说目前rsm缺乏判定间接光源是否可达，在第二个例子中，人物模型的右肩上的间接光源呈现的是黄色，也就是红色和绿色的间接光照结合起来的颜色。但是右肩理论上不应该出现红色的分量，因为红色的部分会被身体部位阻挡。渲染点的法线应该也会影响间接光的表现。\n\n另外就是被当成间接光源的只有被光源照亮且存储起来的部分区域，也就是说间接光源的采样范围相对来说还是比较局限的，不可能采样非常大的区域。在KongEngine中由于采用了CSM来处理阴影，RSM的范围和CSM的最小级的阴影范围采样是一致的，这种处理显然无法照顾大的场景。\n\n场景的间接光照还需要进一步的去优化，RSM只是其中一个小部分。","slug":"reflective-shadow-map","published":1,"updated":"2024-12-01T12:12:58.064Z","comments":1,"layout":"post","photos":[],"_id":"cm5ht44vk000l3457fhgx5jjx","content":"<h1 id=\"反射阴影贴图简介\"><a href=\"#反射阴影贴图简介\" class=\"headerlink\" title=\"反射阴影贴图简介\"></a>反射阴影贴图简介</h1><p>反射阴影贴图（Reflective Shadow Map）是实现全局光照效果的一个非常经典的方法，它是在这篇<a href=\"https://users.soe.ucsc.edu/~pang/160/s13/proposal/mijallen/proposal/media/p203-dachsbacher.pdf\">论文</a>中被提出。</p>\n<h2 id=\"直接光照和间接光照\"><a href=\"#直接光照和间接光照\" class=\"headerlink\" title=\"直接光照和间接光照\"></a>直接光照和间接光照</h2><p>它的名字中带有“阴影贴图（Shadow Map）”几个字，所以乍看之下这个方法似乎是用来解决阴影问题，或者是提升阴影效果的。其实不然，它是用来解决<strong>间接光照</strong>的问题的方法。</p>\n<p>在一般的场景中，光照可以大致分为两类：</p>\n<ul>\n<li>一类是直接光照，也就是物体被光源直接照亮的部分。这个类型的光照是比较好计算的，通过光源的入射角，物体表面的法线和材质，以及观察的方向等等，利用<strong>PBR</strong>的方法能够得到非常不错的效果，这个流程在KongEngine中已经基本实现了。</li>\n<li>另外一个类型是间接光照，它代表的是光线经过一次甚至多次反射后照亮物体的部分。相对于直接光照，间接光照十分复杂，因为光线可能经过多次反射，想要实时的计算光的多次反射的完整路径是很难实现的。但是如果不包含间接光照的话，场景的真实度会大打折扣。在最基础的PBR渲染框架中，我们可以选择手动输入一个环境光照（Ambient Light）的颜色，可以简单的表现全局光照，但是真实性还是不够。</li>\n</ul>\n<p>反射阴影贴图（下面简称<strong>RSM</strong>）这个方法，就是用于解决实时模拟间接光照的问题。</p>\n<h2 id=\"RSM算法基本介绍\"><a href=\"#RSM算法基本介绍\" class=\"headerlink\" title=\"RSM算法基本介绍\"></a>RSM算法基本介绍</h2><p>RSM的核心思想是将被光源直接照亮的区域再次作为光源进行光照计算，这就是光的一次反射。RSM只计算一次光反射，因为一般来说光的第一次反射的能量残留相对来说是最大的，对场景来说有较为显著的影响，后面的反射对场景影响较小，为了性能考量可以忽略。</p>\n<p>那么怎么知道光照直接亮了哪些区域呢？其实非常简单，在参考实现阴影贴图（Shadow Map）的概念，从光源视角下进行渲染，不在阴影中的区域就是被光源直接照亮的。另外，由于光线在漫反射时会被照亮区域的材质所影响（比如说白色的光线从红色的墙反射会变成红色，因为其他颜色被吸收，同时不同粗糙度的物质反射方式也不一样），以及照亮区域的位置和法线也会影响计算光反射的方向，因此我们在计算阴影贴图的时候，还需要保存照亮区域的颜色、世界位置、法线等数据。</p>\n<p>下面是需要记录的数据的截图，从左到右分别是：深度、位置坐标、法线、颜色。<br><img src=\"/2024/11/24/reflective-shadow-map/rsm_info.png\" alt=\"光源方向记录的数据\"></p>\n<p>因此接下来光照计算可以分为以下几步：</p>\n<ol>\n<li>首先从光源的位置和方向渲染场景，将光源视角的信息（深度，世界位置，世界法线等等）缓存到buffer中。</li>\n<li>计算光照直接对环境的影响。</li>\n<li>将第一步缓存的光源保存的信息加入到场景中光照的计算，加上阴影（阴影贴图）和间接光照（反射阴影贴图）。</li>\n</ol>\n<h1 id=\"实现反射阴影贴图的步骤\"><a href=\"#实现反射阴影贴图的步骤\" class=\"headerlink\" title=\"实现反射阴影贴图的步骤\"></a>实现反射阴影贴图的步骤</h1><p>下面是在KongEngine中实现RSM的步骤。</p>\n<h2 id=\"一些前期准备\"><a href=\"#一些前期准备\" class=\"headerlink\" title=\"一些前期准备\"></a>一些前期准备</h2><p>RSM需要将一些信息存储到buffer中，所以很首先需要设置新的缓冲。</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c++\"><span class=\"hljs-built_in\">glGenFramebuffers</span>(<span class=\"hljs-number\">1</span>, &amp;rsm_fbo);<br><span class=\"hljs-built_in\">glBindFramebuffer</span>(GL_FRAMEBUFFER, rsm_fbo);<br><br><span class=\"hljs-comment\">// 位置数据</span><br><span class=\"hljs-built_in\">glGenTextures</span>(<span class=\"hljs-number\">1</span>, &amp;rsm_world_position);<br><span class=\"hljs-built_in\">glBindTexture</span>(GL_TEXTURE_2D, rsm_world_position);<br><span class=\"hljs-built_in\">glTexImage2D</span>(GL_TEXTURE_2D, <span class=\"hljs-number\">0</span>, GL_RGBA32F, SHADOW_RESOLUTION, SHADOW_RESOLUTION, <span class=\"hljs-number\">0</span>, GL_RGBA, GL_FLOAT, <span class=\"hljs-literal\">nullptr</span>);<br><span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);<br><span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);<br><span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);<br><span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);<br><br><span class=\"hljs-built_in\">glFramebufferTexture2D</span>(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, rsm_world_position, <span class=\"hljs-number\">0</span>);<br><br><span class=\"hljs-comment\">// 法线数据</span><br><span class=\"hljs-built_in\">glGenTextures</span>(<span class=\"hljs-number\">1</span>, &amp;rsm_world_normal);<br><span class=\"hljs-built_in\">glBindTexture</span>(GL_TEXTURE_2D, rsm_world_normal);<br><span class=\"hljs-built_in\">glTexImage2D</span>(GL_TEXTURE_2D, <span class=\"hljs-number\">0</span>, GL_RGBA32F, SHADOW_RESOLUTION, SHADOW_RESOLUTION, <span class=\"hljs-number\">0</span>, GL_RGBA, GL_FLOAT, <span class=\"hljs-literal\">nullptr</span>);<br><span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);<br><span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);<br><span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);<br><span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);<br><span class=\"hljs-built_in\">glFramebufferTexture2D</span>(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT1, GL_TEXTURE_2D, rsm_world_normal, <span class=\"hljs-number\">0</span>);<br><br><span class=\"hljs-comment\">// flux数据</span><br><span class=\"hljs-built_in\">glGenTextures</span>(<span class=\"hljs-number\">1</span>, &amp;rsm_world_flux);<br><span class=\"hljs-built_in\">glBindTexture</span>(GL_TEXTURE_2D, rsm_world_flux);<br><span class=\"hljs-built_in\">glTexImage2D</span>(GL_TEXTURE_2D, <span class=\"hljs-number\">0</span>, GL_RGBA32F, SHADOW_RESOLUTION, SHADOW_RESOLUTION, <span class=\"hljs-number\">0</span>, GL_RGBA, GL_FLOAT, <span class=\"hljs-literal\">nullptr</span>);<br><span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);<br><span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);<br><span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);<br><span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);<br><span class=\"hljs-built_in\">glFramebufferTexture2D</span>(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT2, GL_TEXTURE_2D, rsm_world_flux, <span class=\"hljs-number\">0</span>);<br><br><span class=\"hljs-comment\">// 生成renderbuffer</span><br><span class=\"hljs-built_in\">glGenRenderbuffers</span>(<span class=\"hljs-number\">1</span>, &amp;rsm_depth);<br><span class=\"hljs-built_in\">glBindRenderbuffer</span>(GL_RENDERBUFFER, rsm_depth);<br><span class=\"hljs-built_in\">glRenderbufferStorage</span>(GL_RENDERBUFFER, GL_DEPTH_COMPONENT, SHADOW_RESOLUTION, SHADOW_RESOLUTION);<br><span class=\"hljs-built_in\">glFramebufferRenderbuffer</span>(GL_FRAMEBUFFER, GL_DEPTH_ATTACHMENT, GL_RENDERBUFFER, rsm_depth);<br><span class=\"hljs-built_in\">glEnable</span>(GL_DEPTH_TEST);<br><br>GLuint g_attachments[] = &#123;GL_COLOR_ATTACHMENT0, GL_COLOR_ATTACHMENT1, GL_COLOR_ATTACHMENT2&#125;; <br><span class=\"hljs-built_in\">glDrawBuffers</span>(<span class=\"hljs-number\">3</span>, g_attachments);<br></code></pre></td></tr></table></figure>\n<p>上面的部分用于构建RSM的缓冲，这些内容和之前的Shadowmap的流程类似，也可以考虑将其和Shadowmap的缓冲合并，不过为了方便自己理解目前是新建了一个。</p>\n<p>另外RSM也新建了一个独立的shader</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c++\">map&lt;EShaderType, string&gt; shader_path_map = &#123;<br>    &#123;EShaderType::vs, CSceneLoader::<span class=\"hljs-built_in\">ToResourcePath</span>(<span class=\"hljs-string\">&quot;shader/shadow/reflective_shadowmap.vert&quot;</span>)&#125;,<br>    &#123;EShaderType::fs, CSceneLoader::<span class=\"hljs-built_in\">ToResourcePath</span>(<span class=\"hljs-string\">&quot;shader/shadow/reflective_shadowmap.frag&quot;</span>)&#125;<br>&#125;;<br>rsm_shader = <span class=\"hljs-built_in\">make_shared</span>&lt;Shader&gt;(shader_path_map);<br></code></pre></td></tr></table></figure>\n\n<p>shader的内容十分简单，<em>顶点着色器</em>简单的将顶点的世界坐标和法线传给片段着色器。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-meta\">#version 450 compatibility</span><br><br><span class=\"hljs-comment\">// Input vertex data, different for all executions of this shader.</span><br><span class=\"hljs-keyword\">layout</span>(<span class=\"hljs-keyword\">location</span> = <span class=\"hljs-number\">0</span>) <span class=\"hljs-keyword\">in</span> <span class=\"hljs-type\">vec3</span> in_pos;<br><span class=\"hljs-keyword\">layout</span>(<span class=\"hljs-keyword\">location</span> = <span class=\"hljs-number\">1</span>) <span class=\"hljs-keyword\">in</span> <span class=\"hljs-type\">vec3</span> in_normal;<br><br><span class=\"hljs-comment\">// Values that stay constant for the whole mesh.</span><br><span class=\"hljs-keyword\">uniform</span> <span class=\"hljs-type\">mat4</span> light_space_mat;<br><span class=\"hljs-keyword\">uniform</span> <span class=\"hljs-type\">mat4</span> model;<br><br><span class=\"hljs-keyword\">out</span> <span class=\"hljs-type\">vec4</span> frag_pos;<br><span class=\"hljs-keyword\">out</span> <span class=\"hljs-type\">vec3</span> frag_normal;<br><br><span class=\"hljs-type\">void</span> main()&#123;<br>\t<span class=\"hljs-built_in\">gl_Position</span> =  light_space_mat * model * <span class=\"hljs-type\">vec4</span>(in_pos, <span class=\"hljs-number\">1</span>);<br>    frag_pos = model * <span class=\"hljs-type\">vec4</span>(in_pos, <span class=\"hljs-number\">1</span>);<br>\tfrag_normal = <span class=\"hljs-built_in\">normalize</span>(<span class=\"hljs-type\">mat3</span>(<span class=\"hljs-built_in\">transpose</span>(<span class=\"hljs-built_in\">inverse</span>(model))) * in_normal);<br>&#125;<br></code></pre></td></tr></table></figure>\n<p>由于这个shader是从光源视角渲染的，所以<strong>gl_Position</strong>是由光源的<strong>light_space_mat</strong>对世界坐标做变换。</p>\n<p><em>片段着色器</em>将RSM所需的内容存储起来。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-meta\">#version 450 compatibility</span><br><br><span class=\"hljs-keyword\">layout</span>(<span class=\"hljs-keyword\">location</span> = <span class=\"hljs-number\">0</span>) <span class=\"hljs-keyword\">out</span> <span class=\"hljs-type\">vec4</span> world_pos;<br><span class=\"hljs-keyword\">layout</span>(<span class=\"hljs-keyword\">location</span> = <span class=\"hljs-number\">1</span>) <span class=\"hljs-keyword\">out</span> <span class=\"hljs-type\">vec4</span> world_normal;<br><span class=\"hljs-keyword\">layout</span>(<span class=\"hljs-keyword\">location</span> = <span class=\"hljs-number\">2</span>) <span class=\"hljs-keyword\">out</span> <span class=\"hljs-type\">vec4</span> world_flux;<br><br><span class=\"hljs-keyword\">in</span> <span class=\"hljs-type\">vec4</span> frag_pos;<br><span class=\"hljs-keyword\">in</span> <span class=\"hljs-type\">vec3</span> frag_normal;<br><br><span class=\"hljs-keyword\">uniform</span> <span class=\"hljs-type\">vec4</span> albedo;<br><span class=\"hljs-keyword\">uniform</span> <span class=\"hljs-type\">float</span> light_intensity;<br><br><span class=\"hljs-type\">void</span> main()<br>&#123;<br>\tworld_pos = frag_pos;<br>    world_normal = <span class=\"hljs-type\">vec4</span>(frag_normal, <span class=\"hljs-number\">1</span>);<br>    world_flux = albedo;<span class=\"hljs-comment\">// * light_intensity;</span><br>&#125;<br></code></pre></td></tr></table></figure>\n<p>我们这里分别将<strong>世界坐标、世界法线和颜色</strong>存储了到了贴图中。<br>方便起见，KongEngine暂时只支持平行光源的RSM效果，点光源的目前不支持。</p>\n<p>现在我们的平行光源已经有了RSM相关的信息了，在计算光照的时候将这些贴图信息传到光照计算的shader中。</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c++\"><span class=\"hljs-built_in\">glActiveTexture</span>(GL_TEXTURE0 + DIRLIGHT_RSM_WORLD_POS);<br><span class=\"hljs-built_in\">glBindTexture</span>(GL_TEXTURE_2D, rsm_world_pos);<br><span class=\"hljs-built_in\">glActiveTexture</span>(GL_TEXTURE0 + DIRLIGHT_RSM_WORLD_NORMAL);<br><span class=\"hljs-built_in\">glBindTexture</span>(GL_TEXTURE_2D, rsm_world_normal);<br><span class=\"hljs-built_in\">glActiveTexture</span>(GL_TEXTURE0 + DIRLIGHT_RSM_WORLD_FLUX);<br><span class=\"hljs-built_in\">glBindTexture</span>(GL_TEXTURE_2D, rsm_world_flux);<br></code></pre></td></tr></table></figure>\n\n<h2 id=\"间接光源的判定\"><a href=\"#间接光源的判定\" class=\"headerlink\" title=\"间接光源的判定\"></a>间接光源的判定</h2><p>RSM的实际原理如下面两张图所示：<br><img src=\"/2024/11/24/reflective-shadow-map/rsm_principle_1.png\" alt=\"rsm原理图1\"></p>\n<p>假如当前我们片段着色器计算的是<strong>X</strong>点的光照，在这个场景中，x点被桌子的阴影挡住了，并没有被光源直接照亮，如左图所示。所以x点的直接光照为0。</p>\n<p>接下来是间接光照的部分。如上面所说，我们将被光源直接照亮的部分当做光源，这里先以被光源直接照射的两个点Xp和Xq来做判断。Xp点被光源照亮，他的法线是Np，光在Xp点散射后是有可能到达X点的，在数学上的判断就是Np和Xp到X连线的点乘大于0。而Xq的法线Nq和Xq到X点连线的点乘小于0，可以从图上看到光在Xq点散射后是无法到达X点的。</p>\n<p>当然我们还知道，在计算PBR的时候，不同的材质的光线散射形状是不一致的，在图中的表现就是，光线散射后沿着XpX方向的分量，比沿着XpY方向的分量是要小的。因此间接光源的法线和两点之间的连线的点乘大小有这判定间接光源亮度的作用。</p>\n<p>下面这张图和原理图1是一样的，强化一下理解。<br><img src=\"/2024/11/24/reflective-shadow-map/rsm_principle_2.png\" alt=\"rsm原理图2\"></p>\n<h2 id=\"采样间接光源\"><a href=\"#采样间接光源\" class=\"headerlink\" title=\"采样间接光源\"></a>采样间接光源</h2><p>之前说到，需要用被光源照亮的点作为间接光源。如果渲染屏幕上像素点的时候对所有照亮的点都去做判断的话，理论上是可以得到最好的效果，但是性能上会有极大的消耗；相反如果采样点过少的话，计算速度虽然是上去了但是效果会大打折扣。</p>\n<p>因此一个优化方法是通过重要性采样。我们判断离当前渲染点越近的间接光照光源对当前点的最终效果影响就越大，因此离当前点近的间接光源采样点就会越多。并且，为了弥补远处的采样点过少可能带来的问题，引入权重的概念，随着采样点离当前点越近，权重越小。</p>\n<p><img src=\"/2024/11/24/reflective-shadow-map/rsm_sample.png\" alt=\"rsm采样点选择\"></p>\n<p>下面是采样点初始化的示例代码：</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c++\"><span class=\"hljs-comment\">// rsm采样点初始化</span><br>std::default_random_engine e;<br><span class=\"hljs-function\">std::uniform_real_distribution&lt;<span class=\"hljs-type\">float</span>&gt; <span class=\"hljs-title\">u</span><span class=\"hljs-params\">(<span class=\"hljs-number\">0.0</span>, <span class=\"hljs-number\">1.0</span>)</span></span>;<br><span class=\"hljs-type\">float</span> pi_num = <span class=\"hljs-built_in\">pi</span>&lt;<span class=\"hljs-type\">float</span>&gt;();<br><span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">int</span> i = <span class=\"hljs-number\">0</span>; i &lt; rsm_sample_count; i++)<br>&#123;<br>    <span class=\"hljs-type\">float</span> xi1 = <span class=\"hljs-built_in\">u</span>(e);<br>    <span class=\"hljs-type\">float</span> xi2 = <span class=\"hljs-built_in\">u</span>(e);<br>    <br>    rsm_samples_and_weights.<span class=\"hljs-built_in\">push_back</span>(<span class=\"hljs-built_in\">vec4</span>(xi1*<span class=\"hljs-built_in\">sin</span>(<span class=\"hljs-number\">2</span>*pi_num*xi2), xi1*<span class=\"hljs-built_in\">cos</span>(<span class=\"hljs-number\">2</span>*pi_num*xi2), xi1*xi1, <span class=\"hljs-number\">0.0</span>));<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<p>结合上面的两个思想，下面是部分最终代码的呈现，位于defer_pbr.frag中。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-type\">float</span> max_sample_radius = <span class=\"hljs-number\">128.</span>;<br><span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> i = <span class=\"hljs-number\">0</span>; i &lt; rsm_sample_count; ++i) <br>&#123;<br>    <span class=\"hljs-type\">vec3</span> rsm_sample_and_weight = rsm_samples_and_weights[i].xyz;<br>    <span class=\"hljs-type\">vec2</span> uv = proj_coord.xy + max_sample_radius * rsm_sample_and_weight.xy * texel_size;<br>    <span class=\"hljs-type\">vec3</span> flux = <span class=\"hljs-built_in\">texture</span>(rsm_world_flux, uv).rgb;<br>    <span class=\"hljs-type\">vec3</span> x_p = <span class=\"hljs-built_in\">texture</span>(rsm_world_pos, uv).xyz;<br>    <span class=\"hljs-type\">vec3</span> n_p = <span class=\"hljs-built_in\">texture</span>(rsm_world_normal, uv).xyz;<br><br>    <span class=\"hljs-type\">vec3</span> r = <span class=\"hljs-built_in\">normalize</span>(frag_world_pos.xyz - x_p);<br><br>    <span class=\"hljs-type\">float</span> d2 = <span class=\"hljs-built_in\">dot</span>(r, r);<br>    <span class=\"hljs-type\">vec3</span> e_p = flux * (<span class=\"hljs-built_in\">max</span>(<span class=\"hljs-number\">0.0</span>, <span class=\"hljs-built_in\">dot</span>(n_p, r)) * <span class=\"hljs-built_in\">max</span>(<span class=\"hljs-number\">0.0</span>, <span class=\"hljs-built_in\">dot</span>(in_normal, -r))) * rsm_sample_and_weight.z;<br>    <span class=\"hljs-comment\">//e_p *= pow(rsm_sample_offsets[i].x / d2, 2);</span><br>    env_color += e_p;<br>&#125;<br>env_color /= rsm_sample_count;<br></code></pre></td></tr></table></figure>\n\n<h1 id=\"最终效果\"><a href=\"#最终效果\" class=\"headerlink\" title=\"最终效果\"></a>最终效果</h1><p>结合了反射阴影贴图后，场景会有一些间接光照效果了，下面是同一个场景的表现效果。<br><img src=\"/2024/11/24/reflective-shadow-map/rsm_off_sphere.png\" alt=\"rsm关闭1\"><br><img src=\"/2024/11/24/reflective-shadow-map/rsm_on_sphere.png\" alt=\"rsm开启1\"></p>\n<p>这里是另外一组。<br><img src=\"/2024/11/24/reflective-shadow-map/rsm_off_suit.png\" alt=\"rsm关闭2\"><br><img src=\"/2024/11/24/reflective-shadow-map/rsm_on_suit.png\" alt=\"rsm开启2\"></p>\n<p>可以看到开启rsm后，靠近红色和绿色墙壁，且没有被光源直接照亮（处于阴影）的部分，被墙壁的散射光源间接点亮了。灰色的球体和人物模型“沾染”上了墙壁的颜色。这种间接光照的影响使得场景变得更加的真实。</p>\n<p>但是，当前的rsm也并不是完美的，比如说目前rsm缺乏判定间接光源是否可达，在第二个例子中，人物模型的右肩上的间接光源呈现的是黄色，也就是红色和绿色的间接光照结合起来的颜色。但是右肩理论上不应该出现红色的分量，因为红色的部分会被身体部位阻挡。渲染点的法线应该也会影响间接光的表现。</p>\n<p>另外就是被当成间接光源的只有被光源照亮且存储起来的部分区域，也就是说间接光源的采样范围相对来说还是比较局限的，不可能采样非常大的区域。在KongEngine中由于采用了CSM来处理阴影，RSM的范围和CSM的最小级的阴影范围采样是一致的，这种处理显然无法照顾大的场景。</p>\n<p>场景的间接光照还需要进一步的去优化，RSM只是其中一个小部分。</p>\n","excerpt":"","more":"<h1 id=\"反射阴影贴图简介\"><a href=\"#反射阴影贴图简介\" class=\"headerlink\" title=\"反射阴影贴图简介\"></a>反射阴影贴图简介</h1><p>反射阴影贴图（Reflective Shadow Map）是实现全局光照效果的一个非常经典的方法，它是在这篇<a href=\"https://users.soe.ucsc.edu/~pang/160/s13/proposal/mijallen/proposal/media/p203-dachsbacher.pdf\">论文</a>中被提出。</p>\n<h2 id=\"直接光照和间接光照\"><a href=\"#直接光照和间接光照\" class=\"headerlink\" title=\"直接光照和间接光照\"></a>直接光照和间接光照</h2><p>它的名字中带有“阴影贴图（Shadow Map）”几个字，所以乍看之下这个方法似乎是用来解决阴影问题，或者是提升阴影效果的。其实不然，它是用来解决<strong>间接光照</strong>的问题的方法。</p>\n<p>在一般的场景中，光照可以大致分为两类：</p>\n<ul>\n<li>一类是直接光照，也就是物体被光源直接照亮的部分。这个类型的光照是比较好计算的，通过光源的入射角，物体表面的法线和材质，以及观察的方向等等，利用<strong>PBR</strong>的方法能够得到非常不错的效果，这个流程在KongEngine中已经基本实现了。</li>\n<li>另外一个类型是间接光照，它代表的是光线经过一次甚至多次反射后照亮物体的部分。相对于直接光照，间接光照十分复杂，因为光线可能经过多次反射，想要实时的计算光的多次反射的完整路径是很难实现的。但是如果不包含间接光照的话，场景的真实度会大打折扣。在最基础的PBR渲染框架中，我们可以选择手动输入一个环境光照（Ambient Light）的颜色，可以简单的表现全局光照，但是真实性还是不够。</li>\n</ul>\n<p>反射阴影贴图（下面简称<strong>RSM</strong>）这个方法，就是用于解决实时模拟间接光照的问题。</p>\n<h2 id=\"RSM算法基本介绍\"><a href=\"#RSM算法基本介绍\" class=\"headerlink\" title=\"RSM算法基本介绍\"></a>RSM算法基本介绍</h2><p>RSM的核心思想是将被光源直接照亮的区域再次作为光源进行光照计算，这就是光的一次反射。RSM只计算一次光反射，因为一般来说光的第一次反射的能量残留相对来说是最大的，对场景来说有较为显著的影响，后面的反射对场景影响较小，为了性能考量可以忽略。</p>\n<p>那么怎么知道光照直接亮了哪些区域呢？其实非常简单，在参考实现阴影贴图（Shadow Map）的概念，从光源视角下进行渲染，不在阴影中的区域就是被光源直接照亮的。另外，由于光线在漫反射时会被照亮区域的材质所影响（比如说白色的光线从红色的墙反射会变成红色，因为其他颜色被吸收，同时不同粗糙度的物质反射方式也不一样），以及照亮区域的位置和法线也会影响计算光反射的方向，因此我们在计算阴影贴图的时候，还需要保存照亮区域的颜色、世界位置、法线等数据。</p>\n<p>下面是需要记录的数据的截图，从左到右分别是：深度、位置坐标、法线、颜色。<br><img src=\"/2024/11/24/reflective-shadow-map/rsm_info.png\" alt=\"光源方向记录的数据\"></p>\n<p>因此接下来光照计算可以分为以下几步：</p>\n<ol>\n<li>首先从光源的位置和方向渲染场景，将光源视角的信息（深度，世界位置，世界法线等等）缓存到buffer中。</li>\n<li>计算光照直接对环境的影响。</li>\n<li>将第一步缓存的光源保存的信息加入到场景中光照的计算，加上阴影（阴影贴图）和间接光照（反射阴影贴图）。</li>\n</ol>\n<h1 id=\"实现反射阴影贴图的步骤\"><a href=\"#实现反射阴影贴图的步骤\" class=\"headerlink\" title=\"实现反射阴影贴图的步骤\"></a>实现反射阴影贴图的步骤</h1><p>下面是在KongEngine中实现RSM的步骤。</p>\n<h2 id=\"一些前期准备\"><a href=\"#一些前期准备\" class=\"headerlink\" title=\"一些前期准备\"></a>一些前期准备</h2><p>RSM需要将一些信息存储到buffer中，所以很首先需要设置新的缓冲。</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c++\"><span class=\"hljs-built_in\">glGenFramebuffers</span>(<span class=\"hljs-number\">1</span>, &amp;rsm_fbo);<br><span class=\"hljs-built_in\">glBindFramebuffer</span>(GL_FRAMEBUFFER, rsm_fbo);<br><br><span class=\"hljs-comment\">// 位置数据</span><br><span class=\"hljs-built_in\">glGenTextures</span>(<span class=\"hljs-number\">1</span>, &amp;rsm_world_position);<br><span class=\"hljs-built_in\">glBindTexture</span>(GL_TEXTURE_2D, rsm_world_position);<br><span class=\"hljs-built_in\">glTexImage2D</span>(GL_TEXTURE_2D, <span class=\"hljs-number\">0</span>, GL_RGBA32F, SHADOW_RESOLUTION, SHADOW_RESOLUTION, <span class=\"hljs-number\">0</span>, GL_RGBA, GL_FLOAT, <span class=\"hljs-literal\">nullptr</span>);<br><span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);<br><span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);<br><span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);<br><span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);<br><br><span class=\"hljs-built_in\">glFramebufferTexture2D</span>(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, rsm_world_position, <span class=\"hljs-number\">0</span>);<br><br><span class=\"hljs-comment\">// 法线数据</span><br><span class=\"hljs-built_in\">glGenTextures</span>(<span class=\"hljs-number\">1</span>, &amp;rsm_world_normal);<br><span class=\"hljs-built_in\">glBindTexture</span>(GL_TEXTURE_2D, rsm_world_normal);<br><span class=\"hljs-built_in\">glTexImage2D</span>(GL_TEXTURE_2D, <span class=\"hljs-number\">0</span>, GL_RGBA32F, SHADOW_RESOLUTION, SHADOW_RESOLUTION, <span class=\"hljs-number\">0</span>, GL_RGBA, GL_FLOAT, <span class=\"hljs-literal\">nullptr</span>);<br><span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);<br><span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);<br><span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);<br><span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);<br><span class=\"hljs-built_in\">glFramebufferTexture2D</span>(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT1, GL_TEXTURE_2D, rsm_world_normal, <span class=\"hljs-number\">0</span>);<br><br><span class=\"hljs-comment\">// flux数据</span><br><span class=\"hljs-built_in\">glGenTextures</span>(<span class=\"hljs-number\">1</span>, &amp;rsm_world_flux);<br><span class=\"hljs-built_in\">glBindTexture</span>(GL_TEXTURE_2D, rsm_world_flux);<br><span class=\"hljs-built_in\">glTexImage2D</span>(GL_TEXTURE_2D, <span class=\"hljs-number\">0</span>, GL_RGBA32F, SHADOW_RESOLUTION, SHADOW_RESOLUTION, <span class=\"hljs-number\">0</span>, GL_RGBA, GL_FLOAT, <span class=\"hljs-literal\">nullptr</span>);<br><span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);<br><span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);<br><span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);<br><span class=\"hljs-built_in\">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);<br><span class=\"hljs-built_in\">glFramebufferTexture2D</span>(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT2, GL_TEXTURE_2D, rsm_world_flux, <span class=\"hljs-number\">0</span>);<br><br><span class=\"hljs-comment\">// 生成renderbuffer</span><br><span class=\"hljs-built_in\">glGenRenderbuffers</span>(<span class=\"hljs-number\">1</span>, &amp;rsm_depth);<br><span class=\"hljs-built_in\">glBindRenderbuffer</span>(GL_RENDERBUFFER, rsm_depth);<br><span class=\"hljs-built_in\">glRenderbufferStorage</span>(GL_RENDERBUFFER, GL_DEPTH_COMPONENT, SHADOW_RESOLUTION, SHADOW_RESOLUTION);<br><span class=\"hljs-built_in\">glFramebufferRenderbuffer</span>(GL_FRAMEBUFFER, GL_DEPTH_ATTACHMENT, GL_RENDERBUFFER, rsm_depth);<br><span class=\"hljs-built_in\">glEnable</span>(GL_DEPTH_TEST);<br><br>GLuint g_attachments[] = &#123;GL_COLOR_ATTACHMENT0, GL_COLOR_ATTACHMENT1, GL_COLOR_ATTACHMENT2&#125;; <br><span class=\"hljs-built_in\">glDrawBuffers</span>(<span class=\"hljs-number\">3</span>, g_attachments);<br></code></pre></td></tr></table></figure>\n<p>上面的部分用于构建RSM的缓冲，这些内容和之前的Shadowmap的流程类似，也可以考虑将其和Shadowmap的缓冲合并，不过为了方便自己理解目前是新建了一个。</p>\n<p>另外RSM也新建了一个独立的shader</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c++\">map&lt;EShaderType, string&gt; shader_path_map = &#123;<br>    &#123;EShaderType::vs, CSceneLoader::<span class=\"hljs-built_in\">ToResourcePath</span>(<span class=\"hljs-string\">&quot;shader/shadow/reflective_shadowmap.vert&quot;</span>)&#125;,<br>    &#123;EShaderType::fs, CSceneLoader::<span class=\"hljs-built_in\">ToResourcePath</span>(<span class=\"hljs-string\">&quot;shader/shadow/reflective_shadowmap.frag&quot;</span>)&#125;<br>&#125;;<br>rsm_shader = <span class=\"hljs-built_in\">make_shared</span>&lt;Shader&gt;(shader_path_map);<br></code></pre></td></tr></table></figure>\n\n<p>shader的内容十分简单，<em>顶点着色器</em>简单的将顶点的世界坐标和法线传给片段着色器。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-meta\">#version 450 compatibility</span><br><br><span class=\"hljs-comment\">// Input vertex data, different for all executions of this shader.</span><br><span class=\"hljs-keyword\">layout</span>(<span class=\"hljs-keyword\">location</span> = <span class=\"hljs-number\">0</span>) <span class=\"hljs-keyword\">in</span> <span class=\"hljs-type\">vec3</span> in_pos;<br><span class=\"hljs-keyword\">layout</span>(<span class=\"hljs-keyword\">location</span> = <span class=\"hljs-number\">1</span>) <span class=\"hljs-keyword\">in</span> <span class=\"hljs-type\">vec3</span> in_normal;<br><br><span class=\"hljs-comment\">// Values that stay constant for the whole mesh.</span><br><span class=\"hljs-keyword\">uniform</span> <span class=\"hljs-type\">mat4</span> light_space_mat;<br><span class=\"hljs-keyword\">uniform</span> <span class=\"hljs-type\">mat4</span> model;<br><br><span class=\"hljs-keyword\">out</span> <span class=\"hljs-type\">vec4</span> frag_pos;<br><span class=\"hljs-keyword\">out</span> <span class=\"hljs-type\">vec3</span> frag_normal;<br><br><span class=\"hljs-type\">void</span> main()&#123;<br>\t<span class=\"hljs-built_in\">gl_Position</span> =  light_space_mat * model * <span class=\"hljs-type\">vec4</span>(in_pos, <span class=\"hljs-number\">1</span>);<br>    frag_pos = model * <span class=\"hljs-type\">vec4</span>(in_pos, <span class=\"hljs-number\">1</span>);<br>\tfrag_normal = <span class=\"hljs-built_in\">normalize</span>(<span class=\"hljs-type\">mat3</span>(<span class=\"hljs-built_in\">transpose</span>(<span class=\"hljs-built_in\">inverse</span>(model))) * in_normal);<br>&#125;<br></code></pre></td></tr></table></figure>\n<p>由于这个shader是从光源视角渲染的，所以<strong>gl_Position</strong>是由光源的<strong>light_space_mat</strong>对世界坐标做变换。</p>\n<p><em>片段着色器</em>将RSM所需的内容存储起来。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-meta\">#version 450 compatibility</span><br><br><span class=\"hljs-keyword\">layout</span>(<span class=\"hljs-keyword\">location</span> = <span class=\"hljs-number\">0</span>) <span class=\"hljs-keyword\">out</span> <span class=\"hljs-type\">vec4</span> world_pos;<br><span class=\"hljs-keyword\">layout</span>(<span class=\"hljs-keyword\">location</span> = <span class=\"hljs-number\">1</span>) <span class=\"hljs-keyword\">out</span> <span class=\"hljs-type\">vec4</span> world_normal;<br><span class=\"hljs-keyword\">layout</span>(<span class=\"hljs-keyword\">location</span> = <span class=\"hljs-number\">2</span>) <span class=\"hljs-keyword\">out</span> <span class=\"hljs-type\">vec4</span> world_flux;<br><br><span class=\"hljs-keyword\">in</span> <span class=\"hljs-type\">vec4</span> frag_pos;<br><span class=\"hljs-keyword\">in</span> <span class=\"hljs-type\">vec3</span> frag_normal;<br><br><span class=\"hljs-keyword\">uniform</span> <span class=\"hljs-type\">vec4</span> albedo;<br><span class=\"hljs-keyword\">uniform</span> <span class=\"hljs-type\">float</span> light_intensity;<br><br><span class=\"hljs-type\">void</span> main()<br>&#123;<br>\tworld_pos = frag_pos;<br>    world_normal = <span class=\"hljs-type\">vec4</span>(frag_normal, <span class=\"hljs-number\">1</span>);<br>    world_flux = albedo;<span class=\"hljs-comment\">// * light_intensity;</span><br>&#125;<br></code></pre></td></tr></table></figure>\n<p>我们这里分别将<strong>世界坐标、世界法线和颜色</strong>存储了到了贴图中。<br>方便起见，KongEngine暂时只支持平行光源的RSM效果，点光源的目前不支持。</p>\n<p>现在我们的平行光源已经有了RSM相关的信息了，在计算光照的时候将这些贴图信息传到光照计算的shader中。</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c++\"><span class=\"hljs-built_in\">glActiveTexture</span>(GL_TEXTURE0 + DIRLIGHT_RSM_WORLD_POS);<br><span class=\"hljs-built_in\">glBindTexture</span>(GL_TEXTURE_2D, rsm_world_pos);<br><span class=\"hljs-built_in\">glActiveTexture</span>(GL_TEXTURE0 + DIRLIGHT_RSM_WORLD_NORMAL);<br><span class=\"hljs-built_in\">glBindTexture</span>(GL_TEXTURE_2D, rsm_world_normal);<br><span class=\"hljs-built_in\">glActiveTexture</span>(GL_TEXTURE0 + DIRLIGHT_RSM_WORLD_FLUX);<br><span class=\"hljs-built_in\">glBindTexture</span>(GL_TEXTURE_2D, rsm_world_flux);<br></code></pre></td></tr></table></figure>\n\n<h2 id=\"间接光源的判定\"><a href=\"#间接光源的判定\" class=\"headerlink\" title=\"间接光源的判定\"></a>间接光源的判定</h2><p>RSM的实际原理如下面两张图所示：<br><img src=\"/2024/11/24/reflective-shadow-map/rsm_principle_1.png\" alt=\"rsm原理图1\"></p>\n<p>假如当前我们片段着色器计算的是<strong>X</strong>点的光照，在这个场景中，x点被桌子的阴影挡住了，并没有被光源直接照亮，如左图所示。所以x点的直接光照为0。</p>\n<p>接下来是间接光照的部分。如上面所说，我们将被光源直接照亮的部分当做光源，这里先以被光源直接照射的两个点Xp和Xq来做判断。Xp点被光源照亮，他的法线是Np，光在Xp点散射后是有可能到达X点的，在数学上的判断就是Np和Xp到X连线的点乘大于0。而Xq的法线Nq和Xq到X点连线的点乘小于0，可以从图上看到光在Xq点散射后是无法到达X点的。</p>\n<p>当然我们还知道，在计算PBR的时候，不同的材质的光线散射形状是不一致的，在图中的表现就是，光线散射后沿着XpX方向的分量，比沿着XpY方向的分量是要小的。因此间接光源的法线和两点之间的连线的点乘大小有这判定间接光源亮度的作用。</p>\n<p>下面这张图和原理图1是一样的，强化一下理解。<br><img src=\"/2024/11/24/reflective-shadow-map/rsm_principle_2.png\" alt=\"rsm原理图2\"></p>\n<h2 id=\"采样间接光源\"><a href=\"#采样间接光源\" class=\"headerlink\" title=\"采样间接光源\"></a>采样间接光源</h2><p>之前说到，需要用被光源照亮的点作为间接光源。如果渲染屏幕上像素点的时候对所有照亮的点都去做判断的话，理论上是可以得到最好的效果，但是性能上会有极大的消耗；相反如果采样点过少的话，计算速度虽然是上去了但是效果会大打折扣。</p>\n<p>因此一个优化方法是通过重要性采样。我们判断离当前渲染点越近的间接光照光源对当前点的最终效果影响就越大，因此离当前点近的间接光源采样点就会越多。并且，为了弥补远处的采样点过少可能带来的问题，引入权重的概念，随着采样点离当前点越近，权重越小。</p>\n<p><img src=\"/2024/11/24/reflective-shadow-map/rsm_sample.png\" alt=\"rsm采样点选择\"></p>\n<p>下面是采样点初始化的示例代码：</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c++\"><span class=\"hljs-comment\">// rsm采样点初始化</span><br>std::default_random_engine e;<br><span class=\"hljs-function\">std::uniform_real_distribution&lt;<span class=\"hljs-type\">float</span>&gt; <span class=\"hljs-title\">u</span><span class=\"hljs-params\">(<span class=\"hljs-number\">0.0</span>, <span class=\"hljs-number\">1.0</span>)</span></span>;<br><span class=\"hljs-type\">float</span> pi_num = <span class=\"hljs-built_in\">pi</span>&lt;<span class=\"hljs-type\">float</span>&gt;();<br><span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">int</span> i = <span class=\"hljs-number\">0</span>; i &lt; rsm_sample_count; i++)<br>&#123;<br>    <span class=\"hljs-type\">float</span> xi1 = <span class=\"hljs-built_in\">u</span>(e);<br>    <span class=\"hljs-type\">float</span> xi2 = <span class=\"hljs-built_in\">u</span>(e);<br>    <br>    rsm_samples_and_weights.<span class=\"hljs-built_in\">push_back</span>(<span class=\"hljs-built_in\">vec4</span>(xi1*<span class=\"hljs-built_in\">sin</span>(<span class=\"hljs-number\">2</span>*pi_num*xi2), xi1*<span class=\"hljs-built_in\">cos</span>(<span class=\"hljs-number\">2</span>*pi_num*xi2), xi1*xi1, <span class=\"hljs-number\">0.0</span>));<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<p>结合上面的两个思想，下面是部分最终代码的呈现，位于defer_pbr.frag中。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-type\">float</span> max_sample_radius = <span class=\"hljs-number\">128.</span>;<br><span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> i = <span class=\"hljs-number\">0</span>; i &lt; rsm_sample_count; ++i) <br>&#123;<br>    <span class=\"hljs-type\">vec3</span> rsm_sample_and_weight = rsm_samples_and_weights[i].xyz;<br>    <span class=\"hljs-type\">vec2</span> uv = proj_coord.xy + max_sample_radius * rsm_sample_and_weight.xy * texel_size;<br>    <span class=\"hljs-type\">vec3</span> flux = <span class=\"hljs-built_in\">texture</span>(rsm_world_flux, uv).rgb;<br>    <span class=\"hljs-type\">vec3</span> x_p = <span class=\"hljs-built_in\">texture</span>(rsm_world_pos, uv).xyz;<br>    <span class=\"hljs-type\">vec3</span> n_p = <span class=\"hljs-built_in\">texture</span>(rsm_world_normal, uv).xyz;<br><br>    <span class=\"hljs-type\">vec3</span> r = <span class=\"hljs-built_in\">normalize</span>(frag_world_pos.xyz - x_p);<br><br>    <span class=\"hljs-type\">float</span> d2 = <span class=\"hljs-built_in\">dot</span>(r, r);<br>    <span class=\"hljs-type\">vec3</span> e_p = flux * (<span class=\"hljs-built_in\">max</span>(<span class=\"hljs-number\">0.0</span>, <span class=\"hljs-built_in\">dot</span>(n_p, r)) * <span class=\"hljs-built_in\">max</span>(<span class=\"hljs-number\">0.0</span>, <span class=\"hljs-built_in\">dot</span>(in_normal, -r))) * rsm_sample_and_weight.z;<br>    <span class=\"hljs-comment\">//e_p *= pow(rsm_sample_offsets[i].x / d2, 2);</span><br>    env_color += e_p;<br>&#125;<br>env_color /= rsm_sample_count;<br></code></pre></td></tr></table></figure>\n\n<h1 id=\"最终效果\"><a href=\"#最终效果\" class=\"headerlink\" title=\"最终效果\"></a>最终效果</h1><p>结合了反射阴影贴图后，场景会有一些间接光照效果了，下面是同一个场景的表现效果。<br><img src=\"/2024/11/24/reflective-shadow-map/rsm_off_sphere.png\" alt=\"rsm关闭1\"><br><img src=\"/2024/11/24/reflective-shadow-map/rsm_on_sphere.png\" alt=\"rsm开启1\"></p>\n<p>这里是另外一组。<br><img src=\"/2024/11/24/reflective-shadow-map/rsm_off_suit.png\" alt=\"rsm关闭2\"><br><img src=\"/2024/11/24/reflective-shadow-map/rsm_on_suit.png\" alt=\"rsm开启2\"></p>\n<p>可以看到开启rsm后，靠近红色和绿色墙壁，且没有被光源直接照亮（处于阴影）的部分，被墙壁的散射光源间接点亮了。灰色的球体和人物模型“沾染”上了墙壁的颜色。这种间接光照的影响使得场景变得更加的真实。</p>\n<p>但是，当前的rsm也并不是完美的，比如说目前rsm缺乏判定间接光源是否可达，在第二个例子中，人物模型的右肩上的间接光源呈现的是黄色，也就是红色和绿色的间接光照结合起来的颜色。但是右肩理论上不应该出现红色的分量，因为红色的部分会被身体部位阻挡。渲染点的法线应该也会影响间接光的表现。</p>\n<p>另外就是被当成间接光源的只有被光源照亮且存储起来的部分区域，也就是说间接光源的采样范围相对来说还是比较局限的，不可能采样非常大的区域。在KongEngine中由于采用了CSM来处理阴影，RSM的范围和CSM的最小级的阴影范围采样是一致的，这种处理显然无法照顾大的场景。</p>\n<p>场景的间接光照还需要进一步的去优化，RSM只是其中一个小部分。</p>\n"},{"title":"虚幻引擎5基于AI的贴图生成方法","date":"2024-12-22T11:35:55.000Z","index_img":"/2024/12/22/ue-ai-texture-generation/texgen_bp.png","banner_img":"/2024/12/22/ue-ai-texture-generation/texgen_bp.png","_content":"\n# 前言\n今年年初，由于公司部门的变动，我从原来的云服务部门调到了新成立的AI部门。\nAI是最近最火热的东西，我虽然肯定算不上专业，但是也稍微有粗略的接触过一点点。再加上部门新成立没有什么业务上的压力，我便花了点时间去做了些预言，这个工具便是其中之一。\n\n这个工具是为了研究如何将AIGC和3D工作流结合起来的成果。当时选了几个方向，包括AIGC贴图、AIGC模型和3D结合controlnet来辅助AIGC文生图等等。研究的过程中用Unreal Engine搭了些简单的demo，很可惜后续有真正的项目推进后，这些预研的内容也并未有进一步的推进了，觉得有些许可惜，于是便打算在这里记录一下。\n\n同时这个项目也上传到了Git，有兴趣的欢迎查看：[UETextureGeneration](https://github.com/ruochenhua/UETextureGeneration)。\n\n# 介绍\n## 大体流程\n这个demo的流程非常简单，参照一般文生图的流程，填写提示词、负提示词、生成步数和种子等信息。这些参数将传入给到文生图的Python脚本，脚本会运行一个大模型来创建对应的结果。\n\n![运行工具得到生成的贴图](run_texgen.png)\n\n## 安装步骤\n下面介绍一下这个工具所需要的准备工作。\n\n  -  首先我们需要找到引擎的python地址，如C:\\UnrealEngine\\UE_5.3\\Engine\\Binaries\\ThirdParty\\Python3\\Win64，找到这个路径的python.exe文件。虚幻引擎是以这个Python来运行Python脚本，所以我们对应的Python库需要安装在这个路径之下。\n  \n  -  记录下上面的Python的路径，打开cmd或其他命令行工具，以:\n   \"C:\\UnrealEngine\\UE_5.3\\Engine\\Binaries\\ThirdParty\\Python3\\Win64\\python.exe\" -m pip install XXX \n   \n        这种格式来使用pip安装对应的库。\n    \n  -  需要安装的库包括以下这些：\n     - transformers, diffusers, accelerate（hugging face）    \n     - pytorch（https://pytorch.org/get-started/locally/， https://pytorch.org/get-started/previous-versions/)\n     - numpy\n     - opencv-python\n\n\npython的版本可能不支持最新的pytorch版本，如ue5.3使用的python 3.9.7只能支持到pytorch2.1，需要根据版本来安装合适的版本，numpy支持到1.24.1版本，opencv-python支持到4.6.0版本。\n\nue5.5的Python升级到了3.11，所以可以支持到更新的版本了，请在安装前检查一下，要不然很容易出现问题。\n\n## 文件布局\n三个脚本放在**Scripts**文件夹下，分别对应着漫反射贴图生成、法线和置换贴图生成以及提升贴图分辨率三个流程。\n\n下面简单介绍一下这个几个脚本对应的能力。\n## 漫反射贴图生成：\n脚本名称：RunTexGen.py\n\n漫反射贴图生成使用模型：[texture diffusion](https://huggingface.co/dream-textures/texture-diffusion)\n\n模型基于stable diffusion 2 base，通过DreamBooth微调，可通过文生图的方式生成材质的漫反射贴图，尽量不包含光照和阴影信息。\n\n## 法线贴图生成：\n脚本名称：RunNormalGen.py\n\n为了有更加真实的光照表现效果，贴图一般会配合法线贴图使用。\n从漫反射贴图生成法线贴图的库有好几个，比如说[deepbump](https://github.com/HugoTini/DeepBump)，demo使用的是[Material-Map-Generator(MMG)](https://github.com/joeyballentine/Material-Map-Generator)，因为它还可以生成DisplacementMap和RoughnessMap，这两张贴图同样可以增强模型在3D光照环境下的表现。\n\n### 置换贴图：\n上文提到了置换贴图（DisplacementMap），这个资源也是一个提升模型显示效果的手段。法线贴图增加模型细节是不需要修改模型本身的顶点形状的，只是通过提供更为细致的平面法线信息辅助光照计算。\n\n置换贴图则是可以真实的修改模型的形状。\n\n在UE中可以使用模型工具通过**DisplacementMap**来丰富模型的细节，也有另外一种方法**视差遮挡映射（ParallaxOcclusionMapping)**，我在UE中使用的是这种方法。\n\n## Upscale：\n脚本名称：RunUpScale.py\n\n使用[Upscale Pipeline stable-diffusion-x4-upscaler-img2img](https://huggingface.co/radames/stable-diffusion-x4-upscaler-img2img)，将原有的贴图分辨率从512X512提升到2048X2048，模型精度有比较大的提升，但是显存需求显著增大，并且消耗时间显著增长。\n\n# 使用\n最简单的方式是参照下面的蓝图方式调用即可。\n![脚本调用蓝图](texgen_bp.png)\n![参数包裹函数](texgen_bp_macro.png)\n\n最初调用的时候由于需要下载对应的模型，所以时间会相对来说久一些。并且由于是从hugging face上下载模型，可能需要梯子。\n![调用后的命令行](texgen_cmd.png)","source":"_posts/ue-ai-texture-generation.md","raw":"---\ntitle: 虚幻引擎5基于AI的贴图生成方法\ndate: 2024-12-22 19:35:55\ncategories: \n\t- 技术漫谈\ntags: [虚幻引擎, AIGC]\nindex_img: /2024/12/22/ue-ai-texture-generation/texgen_bp.png\nbanner_img: /2024/12/22/ue-ai-texture-generation/texgen_bp.png\n---\n\n# 前言\n今年年初，由于公司部门的变动，我从原来的云服务部门调到了新成立的AI部门。\nAI是最近最火热的东西，我虽然肯定算不上专业，但是也稍微有粗略的接触过一点点。再加上部门新成立没有什么业务上的压力，我便花了点时间去做了些预言，这个工具便是其中之一。\n\n这个工具是为了研究如何将AIGC和3D工作流结合起来的成果。当时选了几个方向，包括AIGC贴图、AIGC模型和3D结合controlnet来辅助AIGC文生图等等。研究的过程中用Unreal Engine搭了些简单的demo，很可惜后续有真正的项目推进后，这些预研的内容也并未有进一步的推进了，觉得有些许可惜，于是便打算在这里记录一下。\n\n同时这个项目也上传到了Git，有兴趣的欢迎查看：[UETextureGeneration](https://github.com/ruochenhua/UETextureGeneration)。\n\n# 介绍\n## 大体流程\n这个demo的流程非常简单，参照一般文生图的流程，填写提示词、负提示词、生成步数和种子等信息。这些参数将传入给到文生图的Python脚本，脚本会运行一个大模型来创建对应的结果。\n\n![运行工具得到生成的贴图](run_texgen.png)\n\n## 安装步骤\n下面介绍一下这个工具所需要的准备工作。\n\n  -  首先我们需要找到引擎的python地址，如C:\\UnrealEngine\\UE_5.3\\Engine\\Binaries\\ThirdParty\\Python3\\Win64，找到这个路径的python.exe文件。虚幻引擎是以这个Python来运行Python脚本，所以我们对应的Python库需要安装在这个路径之下。\n  \n  -  记录下上面的Python的路径，打开cmd或其他命令行工具，以:\n   \"C:\\UnrealEngine\\UE_5.3\\Engine\\Binaries\\ThirdParty\\Python3\\Win64\\python.exe\" -m pip install XXX \n   \n        这种格式来使用pip安装对应的库。\n    \n  -  需要安装的库包括以下这些：\n     - transformers, diffusers, accelerate（hugging face）    \n     - pytorch（https://pytorch.org/get-started/locally/， https://pytorch.org/get-started/previous-versions/)\n     - numpy\n     - opencv-python\n\n\npython的版本可能不支持最新的pytorch版本，如ue5.3使用的python 3.9.7只能支持到pytorch2.1，需要根据版本来安装合适的版本，numpy支持到1.24.1版本，opencv-python支持到4.6.0版本。\n\nue5.5的Python升级到了3.11，所以可以支持到更新的版本了，请在安装前检查一下，要不然很容易出现问题。\n\n## 文件布局\n三个脚本放在**Scripts**文件夹下，分别对应着漫反射贴图生成、法线和置换贴图生成以及提升贴图分辨率三个流程。\n\n下面简单介绍一下这个几个脚本对应的能力。\n## 漫反射贴图生成：\n脚本名称：RunTexGen.py\n\n漫反射贴图生成使用模型：[texture diffusion](https://huggingface.co/dream-textures/texture-diffusion)\n\n模型基于stable diffusion 2 base，通过DreamBooth微调，可通过文生图的方式生成材质的漫反射贴图，尽量不包含光照和阴影信息。\n\n## 法线贴图生成：\n脚本名称：RunNormalGen.py\n\n为了有更加真实的光照表现效果，贴图一般会配合法线贴图使用。\n从漫反射贴图生成法线贴图的库有好几个，比如说[deepbump](https://github.com/HugoTini/DeepBump)，demo使用的是[Material-Map-Generator(MMG)](https://github.com/joeyballentine/Material-Map-Generator)，因为它还可以生成DisplacementMap和RoughnessMap，这两张贴图同样可以增强模型在3D光照环境下的表现。\n\n### 置换贴图：\n上文提到了置换贴图（DisplacementMap），这个资源也是一个提升模型显示效果的手段。法线贴图增加模型细节是不需要修改模型本身的顶点形状的，只是通过提供更为细致的平面法线信息辅助光照计算。\n\n置换贴图则是可以真实的修改模型的形状。\n\n在UE中可以使用模型工具通过**DisplacementMap**来丰富模型的细节，也有另外一种方法**视差遮挡映射（ParallaxOcclusionMapping)**，我在UE中使用的是这种方法。\n\n## Upscale：\n脚本名称：RunUpScale.py\n\n使用[Upscale Pipeline stable-diffusion-x4-upscaler-img2img](https://huggingface.co/radames/stable-diffusion-x4-upscaler-img2img)，将原有的贴图分辨率从512X512提升到2048X2048，模型精度有比较大的提升，但是显存需求显著增大，并且消耗时间显著增长。\n\n# 使用\n最简单的方式是参照下面的蓝图方式调用即可。\n![脚本调用蓝图](texgen_bp.png)\n![参数包裹函数](texgen_bp_macro.png)\n\n最初调用的时候由于需要下载对应的模型，所以时间会相对来说久一些。并且由于是从hugging face上下载模型，可能需要梯子。\n![调用后的命令行](texgen_cmd.png)","slug":"ue-ai-texture-generation","published":1,"updated":"2024-12-22T13:35:11.233Z","comments":1,"layout":"post","photos":[],"_id":"cm5ht44vq002e345762oy6kcy","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><p>今年年初，由于公司部门的变动，我从原来的云服务部门调到了新成立的AI部门。<br>AI是最近最火热的东西，我虽然肯定算不上专业，但是也稍微有粗略的接触过一点点。再加上部门新成立没有什么业务上的压力，我便花了点时间去做了些预言，这个工具便是其中之一。</p>\n<p>这个工具是为了研究如何将AIGC和3D工作流结合起来的成果。当时选了几个方向，包括AIGC贴图、AIGC模型和3D结合controlnet来辅助AIGC文生图等等。研究的过程中用Unreal Engine搭了些简单的demo，很可惜后续有真正的项目推进后，这些预研的内容也并未有进一步的推进了，觉得有些许可惜，于是便打算在这里记录一下。</p>\n<p>同时这个项目也上传到了Git，有兴趣的欢迎查看：<a href=\"https://github.com/ruochenhua/UETextureGeneration\">UETextureGeneration</a>。</p>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><h2 id=\"大体流程\"><a href=\"#大体流程\" class=\"headerlink\" title=\"大体流程\"></a>大体流程</h2><p>这个demo的流程非常简单，参照一般文生图的流程，填写提示词、负提示词、生成步数和种子等信息。这些参数将传入给到文生图的Python脚本，脚本会运行一个大模型来创建对应的结果。</p>\n<p><img src=\"/2024/12/22/ue-ai-texture-generation/run_texgen.png\" alt=\"运行工具得到生成的贴图\"></p>\n<h2 id=\"安装步骤\"><a href=\"#安装步骤\" class=\"headerlink\" title=\"安装步骤\"></a>安装步骤</h2><p>下面介绍一下这个工具所需要的准备工作。</p>\n<ul>\n<li><p>首先我们需要找到引擎的python地址，如C:\\UnrealEngine\\UE_5.3\\Engine\\Binaries\\ThirdParty\\Python3\\Win64，找到这个路径的python.exe文件。虚幻引擎是以这个Python来运行Python脚本，所以我们对应的Python库需要安装在这个路径之下。</p>\n</li>\n<li><p>记录下上面的Python的路径，打开cmd或其他命令行工具，以:<br>   “C:\\UnrealEngine\\UE_5.3\\Engine\\Binaries\\ThirdParty\\Python3\\Win64\\python.exe” -m pip install XXX </p>\n<p>   这种格式来使用pip安装对应的库。</p>\n</li>\n<li><p>需要安装的库包括以下这些：</p>\n<ul>\n<li>transformers, diffusers, accelerate（hugging face）    </li>\n<li>pytorch（<a href=\"https://pytorch.org/get-started/locally/%EF%BC%8C\">https://pytorch.org/get-started/locally/，</a> <a href=\"https://pytorch.org/get-started/previous-versions/\">https://pytorch.org/get-started/previous-versions/</a>)</li>\n<li>numpy</li>\n<li>opencv-python</li>\n</ul>\n</li>\n</ul>\n<p>python的版本可能不支持最新的pytorch版本，如ue5.3使用的python 3.9.7只能支持到pytorch2.1，需要根据版本来安装合适的版本，numpy支持到1.24.1版本，opencv-python支持到4.6.0版本。</p>\n<p>ue5.5的Python升级到了3.11，所以可以支持到更新的版本了，请在安装前检查一下，要不然很容易出现问题。</p>\n<h2 id=\"文件布局\"><a href=\"#文件布局\" class=\"headerlink\" title=\"文件布局\"></a>文件布局</h2><p>三个脚本放在<strong>Scripts</strong>文件夹下，分别对应着漫反射贴图生成、法线和置换贴图生成以及提升贴图分辨率三个流程。</p>\n<p>下面简单介绍一下这个几个脚本对应的能力。</p>\n<h2 id=\"漫反射贴图生成：\"><a href=\"#漫反射贴图生成：\" class=\"headerlink\" title=\"漫反射贴图生成：\"></a>漫反射贴图生成：</h2><p>脚本名称：RunTexGen.py</p>\n<p>漫反射贴图生成使用模型：<a href=\"https://huggingface.co/dream-textures/texture-diffusion\">texture diffusion</a></p>\n<p>模型基于stable diffusion 2 base，通过DreamBooth微调，可通过文生图的方式生成材质的漫反射贴图，尽量不包含光照和阴影信息。</p>\n<h2 id=\"法线贴图生成：\"><a href=\"#法线贴图生成：\" class=\"headerlink\" title=\"法线贴图生成：\"></a>法线贴图生成：</h2><p>脚本名称：RunNormalGen.py</p>\n<p>为了有更加真实的光照表现效果，贴图一般会配合法线贴图使用。<br>从漫反射贴图生成法线贴图的库有好几个，比如说<a href=\"https://github.com/HugoTini/DeepBump\">deepbump</a>，demo使用的是<a href=\"https://github.com/joeyballentine/Material-Map-Generator\">Material-Map-Generator(MMG)</a>，因为它还可以生成DisplacementMap和RoughnessMap，这两张贴图同样可以增强模型在3D光照环境下的表现。</p>\n<h3 id=\"置换贴图：\"><a href=\"#置换贴图：\" class=\"headerlink\" title=\"置换贴图：\"></a>置换贴图：</h3><p>上文提到了置换贴图（DisplacementMap），这个资源也是一个提升模型显示效果的手段。法线贴图增加模型细节是不需要修改模型本身的顶点形状的，只是通过提供更为细致的平面法线信息辅助光照计算。</p>\n<p>置换贴图则是可以真实的修改模型的形状。</p>\n<p>在UE中可以使用模型工具通过<strong>DisplacementMap</strong>来丰富模型的细节，也有另外一种方法**视差遮挡映射（ParallaxOcclusionMapping)**，我在UE中使用的是这种方法。</p>\n<h2 id=\"Upscale：\"><a href=\"#Upscale：\" class=\"headerlink\" title=\"Upscale：\"></a>Upscale：</h2><p>脚本名称：RunUpScale.py</p>\n<p>使用<a href=\"https://huggingface.co/radames/stable-diffusion-x4-upscaler-img2img\">Upscale Pipeline stable-diffusion-x4-upscaler-img2img</a>，将原有的贴图分辨率从512X512提升到2048X2048，模型精度有比较大的提升，但是显存需求显著增大，并且消耗时间显著增长。</p>\n<h1 id=\"使用\"><a href=\"#使用\" class=\"headerlink\" title=\"使用\"></a>使用</h1><p>最简单的方式是参照下面的蓝图方式调用即可。<br><img src=\"/2024/12/22/ue-ai-texture-generation/texgen_bp.png\" alt=\"脚本调用蓝图\"><br><img src=\"/2024/12/22/ue-ai-texture-generation/texgen_bp_macro.png\" alt=\"参数包裹函数\"></p>\n<p>最初调用的时候由于需要下载对应的模型，所以时间会相对来说久一些。并且由于是从hugging face上下载模型，可能需要梯子。<br><img src=\"/2024/12/22/ue-ai-texture-generation/texgen_cmd.png\" alt=\"调用后的命令行\"></p>\n","excerpt":"","more":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><p>今年年初，由于公司部门的变动，我从原来的云服务部门调到了新成立的AI部门。<br>AI是最近最火热的东西，我虽然肯定算不上专业，但是也稍微有粗略的接触过一点点。再加上部门新成立没有什么业务上的压力，我便花了点时间去做了些预言，这个工具便是其中之一。</p>\n<p>这个工具是为了研究如何将AIGC和3D工作流结合起来的成果。当时选了几个方向，包括AIGC贴图、AIGC模型和3D结合controlnet来辅助AIGC文生图等等。研究的过程中用Unreal Engine搭了些简单的demo，很可惜后续有真正的项目推进后，这些预研的内容也并未有进一步的推进了，觉得有些许可惜，于是便打算在这里记录一下。</p>\n<p>同时这个项目也上传到了Git，有兴趣的欢迎查看：<a href=\"https://github.com/ruochenhua/UETextureGeneration\">UETextureGeneration</a>。</p>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><h2 id=\"大体流程\"><a href=\"#大体流程\" class=\"headerlink\" title=\"大体流程\"></a>大体流程</h2><p>这个demo的流程非常简单，参照一般文生图的流程，填写提示词、负提示词、生成步数和种子等信息。这些参数将传入给到文生图的Python脚本，脚本会运行一个大模型来创建对应的结果。</p>\n<p><img src=\"/2024/12/22/ue-ai-texture-generation/run_texgen.png\" alt=\"运行工具得到生成的贴图\"></p>\n<h2 id=\"安装步骤\"><a href=\"#安装步骤\" class=\"headerlink\" title=\"安装步骤\"></a>安装步骤</h2><p>下面介绍一下这个工具所需要的准备工作。</p>\n<ul>\n<li><p>首先我们需要找到引擎的python地址，如C:\\UnrealEngine\\UE_5.3\\Engine\\Binaries\\ThirdParty\\Python3\\Win64，找到这个路径的python.exe文件。虚幻引擎是以这个Python来运行Python脚本，所以我们对应的Python库需要安装在这个路径之下。</p>\n</li>\n<li><p>记录下上面的Python的路径，打开cmd或其他命令行工具，以:<br>   “C:\\UnrealEngine\\UE_5.3\\Engine\\Binaries\\ThirdParty\\Python3\\Win64\\python.exe” -m pip install XXX </p>\n<p>   这种格式来使用pip安装对应的库。</p>\n</li>\n<li><p>需要安装的库包括以下这些：</p>\n<ul>\n<li>transformers, diffusers, accelerate（hugging face）    </li>\n<li>pytorch（<a href=\"https://pytorch.org/get-started/locally/%EF%BC%8C\">https://pytorch.org/get-started/locally/，</a> <a href=\"https://pytorch.org/get-started/previous-versions/\">https://pytorch.org/get-started/previous-versions/</a>)</li>\n<li>numpy</li>\n<li>opencv-python</li>\n</ul>\n</li>\n</ul>\n<p>python的版本可能不支持最新的pytorch版本，如ue5.3使用的python 3.9.7只能支持到pytorch2.1，需要根据版本来安装合适的版本，numpy支持到1.24.1版本，opencv-python支持到4.6.0版本。</p>\n<p>ue5.5的Python升级到了3.11，所以可以支持到更新的版本了，请在安装前检查一下，要不然很容易出现问题。</p>\n<h2 id=\"文件布局\"><a href=\"#文件布局\" class=\"headerlink\" title=\"文件布局\"></a>文件布局</h2><p>三个脚本放在<strong>Scripts</strong>文件夹下，分别对应着漫反射贴图生成、法线和置换贴图生成以及提升贴图分辨率三个流程。</p>\n<p>下面简单介绍一下这个几个脚本对应的能力。</p>\n<h2 id=\"漫反射贴图生成：\"><a href=\"#漫反射贴图生成：\" class=\"headerlink\" title=\"漫反射贴图生成：\"></a>漫反射贴图生成：</h2><p>脚本名称：RunTexGen.py</p>\n<p>漫反射贴图生成使用模型：<a href=\"https://huggingface.co/dream-textures/texture-diffusion\">texture diffusion</a></p>\n<p>模型基于stable diffusion 2 base，通过DreamBooth微调，可通过文生图的方式生成材质的漫反射贴图，尽量不包含光照和阴影信息。</p>\n<h2 id=\"法线贴图生成：\"><a href=\"#法线贴图生成：\" class=\"headerlink\" title=\"法线贴图生成：\"></a>法线贴图生成：</h2><p>脚本名称：RunNormalGen.py</p>\n<p>为了有更加真实的光照表现效果，贴图一般会配合法线贴图使用。<br>从漫反射贴图生成法线贴图的库有好几个，比如说<a href=\"https://github.com/HugoTini/DeepBump\">deepbump</a>，demo使用的是<a href=\"https://github.com/joeyballentine/Material-Map-Generator\">Material-Map-Generator(MMG)</a>，因为它还可以生成DisplacementMap和RoughnessMap，这两张贴图同样可以增强模型在3D光照环境下的表现。</p>\n<h3 id=\"置换贴图：\"><a href=\"#置换贴图：\" class=\"headerlink\" title=\"置换贴图：\"></a>置换贴图：</h3><p>上文提到了置换贴图（DisplacementMap），这个资源也是一个提升模型显示效果的手段。法线贴图增加模型细节是不需要修改模型本身的顶点形状的，只是通过提供更为细致的平面法线信息辅助光照计算。</p>\n<p>置换贴图则是可以真实的修改模型的形状。</p>\n<p>在UE中可以使用模型工具通过<strong>DisplacementMap</strong>来丰富模型的细节，也有另外一种方法**视差遮挡映射（ParallaxOcclusionMapping)**，我在UE中使用的是这种方法。</p>\n<h2 id=\"Upscale：\"><a href=\"#Upscale：\" class=\"headerlink\" title=\"Upscale：\"></a>Upscale：</h2><p>脚本名称：RunUpScale.py</p>\n<p>使用<a href=\"https://huggingface.co/radames/stable-diffusion-x4-upscaler-img2img\">Upscale Pipeline stable-diffusion-x4-upscaler-img2img</a>，将原有的贴图分辨率从512X512提升到2048X2048，模型精度有比较大的提升，但是显存需求显著增大，并且消耗时间显著增长。</p>\n<h1 id=\"使用\"><a href=\"#使用\" class=\"headerlink\" title=\"使用\"></a>使用</h1><p>最简单的方式是参照下面的蓝图方式调用即可。<br><img src=\"/2024/12/22/ue-ai-texture-generation/texgen_bp.png\" alt=\"脚本调用蓝图\"><br><img src=\"/2024/12/22/ue-ai-texture-generation/texgen_bp_macro.png\" alt=\"参数包裹函数\"></p>\n<p>最初调用的时候由于需要下载对应的模型，所以时间会相对来说久一些。并且由于是从hugging face上下载模型，可能需要梯子。<br><img src=\"/2024/12/22/ue-ai-texture-generation/texgen_cmd.png\" alt=\"调用后的命令行\"></p>\n"},{"title":"软阴影的实现（PCF和PCSS）","date":"2024-12-08T02:29:07.000Z","index_img":"/2024/12/08/soft-shadow/soft-shadow-thumbnail.png","banner_img":"/2024/12/08/soft-shadow/soft-shadow-thumbnail.png","_content":"\n# 什么是软阴影\n在3D中实现阴影最基础的方法是使用阴影贴图shadowmap，根据shadowmap中存储的信息来判定当前渲染的像素是否在阴影当中。\n\n阴影贴图的方法很好理解，但是仅仅基于阴影贴图的阴影效果，在阴影的边缘会有锯齿的情况出现。这往往是由于阴影贴图的分辨率不够导致的，然而一味的提升阴影贴图的分辨率也不是方法，毕竟实时渲染的性能也是需要考虑的一个方面。\n\n那么该如何在可接受的性能表现下实现软阴影的效果呢，下面详细介绍两种方法：Percentage Closer Filtering（PCF）以及Percentage Closer Soft Shadows（PCSS）。\n\n# 柔和阴影边缘-PCF\n下面是一个普通的阴影效果：\n![普通的阴影效果](normal-shadow.png)\n这个阴影贴图的分辨率是2048，这是在[CSM](https://ruochenhua.github.io/2024/10/13/cascade-shadow-map/)的最低一级的阴影效果。可以看到阴影边缘的锯齿感非常的强烈，同时由于采样精度的问题，模型的腿上也出现了不正确的阴影区域。最简单的方法就是通过提高阴影贴图的分辨率来缓解这个问题，但是显而易见这不是最好的解决方案，而Percentage Closer Filtering（后简称PCF）可以帮助我们解决这个问题。\n\n## 什么是PCF\nPercentage Closer Filtering（PCF）是一种在计算机图形学中用于生成软阴影的技术。它主要用于解决硬阴影（如简单的阴影映射产生的锐利阴影边缘）不符合真实场景光照效果的问题。\n\n与简单的阴影映射不同，PCF 在判断像素是否在阴影中时，不是只比较单个点的深度。它会在像素点周围的一定区域内进行多次采样。例如，在一个以像素点为中心的小区域（通常是方形或圆形区域）内，对多个采样点进行深度比较。这些采样点的位置可以是均匀分布，也可以采用更复杂的分布方式，如泊松分布，以获得更自然的效果。\n\n对于每个采样点，比较其深度和阴影图中的深度来判断是否在阴影中。然后统计在阴影中的采样点的比例。设采样点总数为**N**，处于阴影中的采样点数量为**n**，则阴影强度可以通过公式计算**shadow=n/N**得到。这个阴影强度用于确定像素最终的阴影效果。如果阴影强度为1，表示像素完全处于阴影中；如果阴影强度为0，表示像素完全不在阴影中；介于两者之间的值表示不同程度的软阴影效果。\n\n## PCF的实现\n转换为代码如下：\n```glsl\nfloat CalculatePCFShadow(float current_depth, sampler2D shadow_map,  vec2 uv, int radius)\n{\n    float shadow = 0.0;\n    vec2 texel_size = 1.0 / vec2(textureSize(shadow_map, 0));\n    for (int x = -radius; x <= radius; ++x)\n    {\n        for (int y = -radius; y <= radius; ++y)\n        {\n            float pcf_depth = texture(shadow_map, vec2(uv + vec2(x, y) * texel_size)).r;\n            shadow += current_depth > pcf_depth ? 1.0 : 0.0;\n        }\n    }\n    shadow /= pow((1+radius*2),2.0);\n    return shadow;\n}\n```\n\n这里我使用了矩形的采样区域，可以看到其实当采样区域的半径为1的时候，采样点个数为9，阴影边缘的锯齿感已经得到了明显的改善。模型腿上也没有出现错误的阴影区域，效果大大提升。\n\n![PCF阴影，采样半径1](pcf-shadow-1.png)\n\n当采样半径提升为3，采样点个数为49时，阴影的边缘软化效果更明显了，不过付出了5倍的性能消耗，提升确并没有非常明显。\n\n![PCF阴影，采样半径3](pcf-shadow-3.png) \n\n# 半影的产生-PCSS\n## 什么是本影和半影\n在实际的场景中，我们观察阴影，会发现下面这种情况：\n\n![现实阴影](real_shadow.png)\n物体的深暗影子周围还有一片区域是浅浅的暗影。深暗影子的区域我们称之为“**本影**”，而浅暗影子的区域我们称之为“**半影**”。\n\n这种现象在体积光照（或者区域光照）的情况下很容易出现。其原因是，很多光源是有范围的，如下图假设有一个光源的大小用L1到L2，光源的右边有一个物体。\n\n![本影和半影的原理](umbra-principle.png)\n\n光源最上点位L1的位置，照向物体的时候，产生的阴影范围是**A区域**以及下方的**B区域**，上方的**B区域**会被L1照亮；L2点产生的阴影范围是**A区域**和上方的**B区域**，下方的**B区域**会被L2照亮。所以我们可以看到，**区域A**是光源完全的光都会被挡住的区域，所以他的阴影是最深的，是为**本影**。而两个**区域B**是挡住了光源的部分区域，同时被光源的另外一部分照亮的，是为**半影**。\n\n![本影和半影的对照区域](umbra-contrast.png)\n\n## 什么是PCSS\n在弄明白什么是本影和半影之后，我们来介绍一下PCSS是什么。\n\nPercentage Closer Soft Shadows（PCSS）即百分比渐近软阴影，是计算机图形学中用于生成更逼真软阴影的一种技术，它是在 Percentage Closer Filtering（PCF）基础上发展而来的。\n\n在PCF的基础上，PCSS还额外考虑了光源、遮挡物和接收阴影的物体之间的几何关系，通过这些关系来调整用于计算阴影强度的采样区域大小。通过根据阴影的不同情况动态调整采样区域的大小，PCSS能生成更自然、更符合物理规律的软阴影。\n\n这里是提出PCSS的[论文](https://developer.download.nvidia.com/shaderlibrary/docs/shadow_PCSS.pdf)。\n\n其原理总结起来就是根据采样一个区域内处于阴影的比例，来动态的调节这个区域对应的阴影的采样范围。\n\n## PCSS的实现步骤\nPCSS的实现步骤如下：\n\n首先，计算平均的遮挡物距离。在阴影图中，以当前像素点为中心，在一个初始的较小采样区域内查找深度值小于当前像素点深度的采样点，这些采样点对应的物体即为遮挡物。通过计算这些遮挡物采样点深度的平均值，得到平均遮挡物距离**d_blocker**。\n\n![采样平均遮挡物距离1](get-d_blocker-1.png)\n\n对应代码为：\n```glsl\nfloat FindBlockerDepth(sampler2D shadowmap, vec2 uv, float d_receiver, float radius)\n{\n    float blocker_depth_sum = 0.0;\n    int blocker_count = 0;\n    \n    // 以当前像素为中心,半径为radius的范围采样\n    for (float y = -radius; y <= radius; y++) {\n        for (float x = -radius; x <= radius; x++) {\n            vec2 offset = vec2(x, y) * 1.0 / vec2(textureSize(shadowmap, 0));\n            float sampleDepth = TextureProjBilinear(shadowmap, uv + offset);\n            if (sampleDepth < d_receiver) {\n                blocker_depth_sum += sampleDepth;\n                blocker_count++;\n            }\n        }\n    }\n    return blocker_count > 0? blocker_depth_sum / float(blocker_count) : 0.0;\n}\n```\n其中**d_receiver**为当前像素点到光源的深度值，这个值可以将当前像素点位置变换到光源的投影下得到，在处理阴影贴图的时候就需要拿到了。TextureProjBilinear是获取shadowmap深度值的方法，里面采用了双线性插值的方法，不过对PCSS来说不一定需要使用这个方法。\n\n可以看到这个阶段，PCSS搜索了一个阴影贴图里面的区域（下图红色区域），记录下了这个区域的被阻挡范围的平均深度。\n\n![采样平均遮挡物距离2](get-d_blocker-2.png)\n\n\n然后根据这个范围，以及三角形相似原理，估算出半影半径。\n\n![计算半影半径](penumbra.png)\n其中d_receiver、d_blocker我们已知，W_light是光源的范围大小，可以根据实际情况来调整。用图上右方的公式，得出半影的采样范围W_penumbra。\n\n代码如下：\n```glsl\n// 计算遮挡物范围半径（基于相似三角形原理）\nfloat EstimateBlockerSearchRadius(vec2 uv, float d_receiver, float d_blocker, float light_size)\n{\n    if (d_blocker == 0.0) return 0.0;\n    return (d_receiver - d_blocker) * (light_size / d_blocker);\n}\n```\n\n最后，根据估算出的半影半径，扩大采样区域，然后在这个更大的区域内进行采样，并按照 PCF 的方式计算阴影强度。这样，离光源较近或遮挡物较近的地方，半影半径较小，阴影较实；离光源较远或遮挡物较远的地方，半影半径较大，阴影较虚，从而实现了更自然的软阴影效果。\n\n代码如下：\n```glsl\nfloat shadow_sum = 0.0f;\nfor(int pcss_i = 0; pcss_i < pcss_sample_count; pcss_i++)\n{\n    // 可以使用泊松采样盘等方法获取更自然的采样点位置，这里简单均匀采样\n    vec2 offset = vec2(cos(float(pcss_i) * 2.0 * 3.1415926 / float(pcss_sample_count)),\n    sin(float(pcss_i) * 2.0 * 3.1415926 / float(pcss_sample_count))) * blocker_radius;\n\n    vec4 sampleLightSpacePos = vec4(proj_coord.xy + offset, proj_coord.z, 1.0);\n    float sampleDepth = TextureProjBilinear(shadow_map, proj_coord.xy+offset);\n    shadow_sum += sampleDepth < d_recv? 1.0 : 0.0;\n}\nshadow = shadow_sum / pcss_sample_count;\n```\n\n## PCSS的效果\n下面是PCSS开启和关闭的效果对比，其中PCSS关闭下PCF的采样半径是3：\n![PCSS关闭](PCSS_OFF.png)\n\n![PCSS开启](PCSS_ON.png)\n\n可以看到开启了PCSS的效果后，遮挡物体的阴影区域，随着离遮挡物越来越远，出现了越来越明显的半影效果，效果更加自然和真实。\n\n# 总结\n## Percentage Closer Filtering（PCF）的作用\n1. 软阴影生成基础：PCF 是一种用于生成软阴影的基础技术。它基于阴影映射，在判断像素是否在阴影中时，不是只比较单个点的深度，而是在像素点周围一定区域内进行多次采样。\n\n2. 阴影强度计算：通过统计采样区域内处于阴影中的采样点比例来计算阴影强度。这种方式能有效避免硬阴影边缘的锯齿问题，使阴影边缘过渡更加自然，产生软阴影效果，提升了阴影的真实感。\n\n3. 平衡性能和效果：相对一些复杂的物理软阴影算法，PCF 较为简单，在性能和效果之间取得了较好的平衡，适用于实时渲染场景，如游戏。\n\n## Percentage Closer Soft Shadows（PCSS）的作用\n1. 动态软阴影生成：PCSS 在 PCF 基础上进一步改进。它能够根据光源、遮挡物和接收阴影物体之间的几何关系动态调整采样区域的大小。\n\n2. 更自然的阴影过渡：通过计算平均遮挡物距离和估算半影半径，根据半影半径调整采样区域进行采样计算阴影强度。这样生成的软阴影更加符合物理规律，阴影从完全阴影到完全光照的过渡更加自然、真实，在需要高逼真度渲染的场景中能显著提升视觉质量。","source":"_posts/soft-shadow.md","raw":"---\ntitle: 软阴影的实现（PCF和PCSS）\ndate: 2024-12-08 10:29:07\ncategories: \n\t- 技术漫谈\ntags: [3D, 渲染, 编程, 阴影]\nindex_img: /2024/12/08/soft-shadow/soft-shadow-thumbnail.png\nbanner_img: /2024/12/08/soft-shadow/soft-shadow-thumbnail.png\n---\n\n# 什么是软阴影\n在3D中实现阴影最基础的方法是使用阴影贴图shadowmap，根据shadowmap中存储的信息来判定当前渲染的像素是否在阴影当中。\n\n阴影贴图的方法很好理解，但是仅仅基于阴影贴图的阴影效果，在阴影的边缘会有锯齿的情况出现。这往往是由于阴影贴图的分辨率不够导致的，然而一味的提升阴影贴图的分辨率也不是方法，毕竟实时渲染的性能也是需要考虑的一个方面。\n\n那么该如何在可接受的性能表现下实现软阴影的效果呢，下面详细介绍两种方法：Percentage Closer Filtering（PCF）以及Percentage Closer Soft Shadows（PCSS）。\n\n# 柔和阴影边缘-PCF\n下面是一个普通的阴影效果：\n![普通的阴影效果](normal-shadow.png)\n这个阴影贴图的分辨率是2048，这是在[CSM](https://ruochenhua.github.io/2024/10/13/cascade-shadow-map/)的最低一级的阴影效果。可以看到阴影边缘的锯齿感非常的强烈，同时由于采样精度的问题，模型的腿上也出现了不正确的阴影区域。最简单的方法就是通过提高阴影贴图的分辨率来缓解这个问题，但是显而易见这不是最好的解决方案，而Percentage Closer Filtering（后简称PCF）可以帮助我们解决这个问题。\n\n## 什么是PCF\nPercentage Closer Filtering（PCF）是一种在计算机图形学中用于生成软阴影的技术。它主要用于解决硬阴影（如简单的阴影映射产生的锐利阴影边缘）不符合真实场景光照效果的问题。\n\n与简单的阴影映射不同，PCF 在判断像素是否在阴影中时，不是只比较单个点的深度。它会在像素点周围的一定区域内进行多次采样。例如，在一个以像素点为中心的小区域（通常是方形或圆形区域）内，对多个采样点进行深度比较。这些采样点的位置可以是均匀分布，也可以采用更复杂的分布方式，如泊松分布，以获得更自然的效果。\n\n对于每个采样点，比较其深度和阴影图中的深度来判断是否在阴影中。然后统计在阴影中的采样点的比例。设采样点总数为**N**，处于阴影中的采样点数量为**n**，则阴影强度可以通过公式计算**shadow=n/N**得到。这个阴影强度用于确定像素最终的阴影效果。如果阴影强度为1，表示像素完全处于阴影中；如果阴影强度为0，表示像素完全不在阴影中；介于两者之间的值表示不同程度的软阴影效果。\n\n## PCF的实现\n转换为代码如下：\n```glsl\nfloat CalculatePCFShadow(float current_depth, sampler2D shadow_map,  vec2 uv, int radius)\n{\n    float shadow = 0.0;\n    vec2 texel_size = 1.0 / vec2(textureSize(shadow_map, 0));\n    for (int x = -radius; x <= radius; ++x)\n    {\n        for (int y = -radius; y <= radius; ++y)\n        {\n            float pcf_depth = texture(shadow_map, vec2(uv + vec2(x, y) * texel_size)).r;\n            shadow += current_depth > pcf_depth ? 1.0 : 0.0;\n        }\n    }\n    shadow /= pow((1+radius*2),2.0);\n    return shadow;\n}\n```\n\n这里我使用了矩形的采样区域，可以看到其实当采样区域的半径为1的时候，采样点个数为9，阴影边缘的锯齿感已经得到了明显的改善。模型腿上也没有出现错误的阴影区域，效果大大提升。\n\n![PCF阴影，采样半径1](pcf-shadow-1.png)\n\n当采样半径提升为3，采样点个数为49时，阴影的边缘软化效果更明显了，不过付出了5倍的性能消耗，提升确并没有非常明显。\n\n![PCF阴影，采样半径3](pcf-shadow-3.png) \n\n# 半影的产生-PCSS\n## 什么是本影和半影\n在实际的场景中，我们观察阴影，会发现下面这种情况：\n\n![现实阴影](real_shadow.png)\n物体的深暗影子周围还有一片区域是浅浅的暗影。深暗影子的区域我们称之为“**本影**”，而浅暗影子的区域我们称之为“**半影**”。\n\n这种现象在体积光照（或者区域光照）的情况下很容易出现。其原因是，很多光源是有范围的，如下图假设有一个光源的大小用L1到L2，光源的右边有一个物体。\n\n![本影和半影的原理](umbra-principle.png)\n\n光源最上点位L1的位置，照向物体的时候，产生的阴影范围是**A区域**以及下方的**B区域**，上方的**B区域**会被L1照亮；L2点产生的阴影范围是**A区域**和上方的**B区域**，下方的**B区域**会被L2照亮。所以我们可以看到，**区域A**是光源完全的光都会被挡住的区域，所以他的阴影是最深的，是为**本影**。而两个**区域B**是挡住了光源的部分区域，同时被光源的另外一部分照亮的，是为**半影**。\n\n![本影和半影的对照区域](umbra-contrast.png)\n\n## 什么是PCSS\n在弄明白什么是本影和半影之后，我们来介绍一下PCSS是什么。\n\nPercentage Closer Soft Shadows（PCSS）即百分比渐近软阴影，是计算机图形学中用于生成更逼真软阴影的一种技术，它是在 Percentage Closer Filtering（PCF）基础上发展而来的。\n\n在PCF的基础上，PCSS还额外考虑了光源、遮挡物和接收阴影的物体之间的几何关系，通过这些关系来调整用于计算阴影强度的采样区域大小。通过根据阴影的不同情况动态调整采样区域的大小，PCSS能生成更自然、更符合物理规律的软阴影。\n\n这里是提出PCSS的[论文](https://developer.download.nvidia.com/shaderlibrary/docs/shadow_PCSS.pdf)。\n\n其原理总结起来就是根据采样一个区域内处于阴影的比例，来动态的调节这个区域对应的阴影的采样范围。\n\n## PCSS的实现步骤\nPCSS的实现步骤如下：\n\n首先，计算平均的遮挡物距离。在阴影图中，以当前像素点为中心，在一个初始的较小采样区域内查找深度值小于当前像素点深度的采样点，这些采样点对应的物体即为遮挡物。通过计算这些遮挡物采样点深度的平均值，得到平均遮挡物距离**d_blocker**。\n\n![采样平均遮挡物距离1](get-d_blocker-1.png)\n\n对应代码为：\n```glsl\nfloat FindBlockerDepth(sampler2D shadowmap, vec2 uv, float d_receiver, float radius)\n{\n    float blocker_depth_sum = 0.0;\n    int blocker_count = 0;\n    \n    // 以当前像素为中心,半径为radius的范围采样\n    for (float y = -radius; y <= radius; y++) {\n        for (float x = -radius; x <= radius; x++) {\n            vec2 offset = vec2(x, y) * 1.0 / vec2(textureSize(shadowmap, 0));\n            float sampleDepth = TextureProjBilinear(shadowmap, uv + offset);\n            if (sampleDepth < d_receiver) {\n                blocker_depth_sum += sampleDepth;\n                blocker_count++;\n            }\n        }\n    }\n    return blocker_count > 0? blocker_depth_sum / float(blocker_count) : 0.0;\n}\n```\n其中**d_receiver**为当前像素点到光源的深度值，这个值可以将当前像素点位置变换到光源的投影下得到，在处理阴影贴图的时候就需要拿到了。TextureProjBilinear是获取shadowmap深度值的方法，里面采用了双线性插值的方法，不过对PCSS来说不一定需要使用这个方法。\n\n可以看到这个阶段，PCSS搜索了一个阴影贴图里面的区域（下图红色区域），记录下了这个区域的被阻挡范围的平均深度。\n\n![采样平均遮挡物距离2](get-d_blocker-2.png)\n\n\n然后根据这个范围，以及三角形相似原理，估算出半影半径。\n\n![计算半影半径](penumbra.png)\n其中d_receiver、d_blocker我们已知，W_light是光源的范围大小，可以根据实际情况来调整。用图上右方的公式，得出半影的采样范围W_penumbra。\n\n代码如下：\n```glsl\n// 计算遮挡物范围半径（基于相似三角形原理）\nfloat EstimateBlockerSearchRadius(vec2 uv, float d_receiver, float d_blocker, float light_size)\n{\n    if (d_blocker == 0.0) return 0.0;\n    return (d_receiver - d_blocker) * (light_size / d_blocker);\n}\n```\n\n最后，根据估算出的半影半径，扩大采样区域，然后在这个更大的区域内进行采样，并按照 PCF 的方式计算阴影强度。这样，离光源较近或遮挡物较近的地方，半影半径较小，阴影较实；离光源较远或遮挡物较远的地方，半影半径较大，阴影较虚，从而实现了更自然的软阴影效果。\n\n代码如下：\n```glsl\nfloat shadow_sum = 0.0f;\nfor(int pcss_i = 0; pcss_i < pcss_sample_count; pcss_i++)\n{\n    // 可以使用泊松采样盘等方法获取更自然的采样点位置，这里简单均匀采样\n    vec2 offset = vec2(cos(float(pcss_i) * 2.0 * 3.1415926 / float(pcss_sample_count)),\n    sin(float(pcss_i) * 2.0 * 3.1415926 / float(pcss_sample_count))) * blocker_radius;\n\n    vec4 sampleLightSpacePos = vec4(proj_coord.xy + offset, proj_coord.z, 1.0);\n    float sampleDepth = TextureProjBilinear(shadow_map, proj_coord.xy+offset);\n    shadow_sum += sampleDepth < d_recv? 1.0 : 0.0;\n}\nshadow = shadow_sum / pcss_sample_count;\n```\n\n## PCSS的效果\n下面是PCSS开启和关闭的效果对比，其中PCSS关闭下PCF的采样半径是3：\n![PCSS关闭](PCSS_OFF.png)\n\n![PCSS开启](PCSS_ON.png)\n\n可以看到开启了PCSS的效果后，遮挡物体的阴影区域，随着离遮挡物越来越远，出现了越来越明显的半影效果，效果更加自然和真实。\n\n# 总结\n## Percentage Closer Filtering（PCF）的作用\n1. 软阴影生成基础：PCF 是一种用于生成软阴影的基础技术。它基于阴影映射，在判断像素是否在阴影中时，不是只比较单个点的深度，而是在像素点周围一定区域内进行多次采样。\n\n2. 阴影强度计算：通过统计采样区域内处于阴影中的采样点比例来计算阴影强度。这种方式能有效避免硬阴影边缘的锯齿问题，使阴影边缘过渡更加自然，产生软阴影效果，提升了阴影的真实感。\n\n3. 平衡性能和效果：相对一些复杂的物理软阴影算法，PCF 较为简单，在性能和效果之间取得了较好的平衡，适用于实时渲染场景，如游戏。\n\n## Percentage Closer Soft Shadows（PCSS）的作用\n1. 动态软阴影生成：PCSS 在 PCF 基础上进一步改进。它能够根据光源、遮挡物和接收阴影物体之间的几何关系动态调整采样区域的大小。\n\n2. 更自然的阴影过渡：通过计算平均遮挡物距离和估算半影半径，根据半影半径调整采样区域进行采样计算阴影强度。这样生成的软阴影更加符合物理规律，阴影从完全阴影到完全光照的过渡更加自然、真实，在需要高逼真度渲染的场景中能显著提升视觉质量。","slug":"soft-shadow","published":1,"updated":"2024-12-08T04:20:27.459Z","comments":1,"layout":"post","photos":[],"_id":"cm5ht44vq002f34579f1tfmkj","content":"<h1 id=\"什么是软阴影\"><a href=\"#什么是软阴影\" class=\"headerlink\" title=\"什么是软阴影\"></a>什么是软阴影</h1><p>在3D中实现阴影最基础的方法是使用阴影贴图shadowmap，根据shadowmap中存储的信息来判定当前渲染的像素是否在阴影当中。</p>\n<p>阴影贴图的方法很好理解，但是仅仅基于阴影贴图的阴影效果，在阴影的边缘会有锯齿的情况出现。这往往是由于阴影贴图的分辨率不够导致的，然而一味的提升阴影贴图的分辨率也不是方法，毕竟实时渲染的性能也是需要考虑的一个方面。</p>\n<p>那么该如何在可接受的性能表现下实现软阴影的效果呢，下面详细介绍两种方法：Percentage Closer Filtering（PCF）以及Percentage Closer Soft Shadows（PCSS）。</p>\n<h1 id=\"柔和阴影边缘-PCF\"><a href=\"#柔和阴影边缘-PCF\" class=\"headerlink\" title=\"柔和阴影边缘-PCF\"></a>柔和阴影边缘-PCF</h1><p>下面是一个普通的阴影效果：<br><img src=\"/2024/12/08/soft-shadow/normal-shadow.png\" alt=\"普通的阴影效果\"><br>这个阴影贴图的分辨率是2048，这是在<a href=\"https://ruochenhua.github.io/2024/10/13/cascade-shadow-map/\">CSM</a>的最低一级的阴影效果。可以看到阴影边缘的锯齿感非常的强烈，同时由于采样精度的问题，模型的腿上也出现了不正确的阴影区域。最简单的方法就是通过提高阴影贴图的分辨率来缓解这个问题，但是显而易见这不是最好的解决方案，而Percentage Closer Filtering（后简称PCF）可以帮助我们解决这个问题。</p>\n<h2 id=\"什么是PCF\"><a href=\"#什么是PCF\" class=\"headerlink\" title=\"什么是PCF\"></a>什么是PCF</h2><p>Percentage Closer Filtering（PCF）是一种在计算机图形学中用于生成软阴影的技术。它主要用于解决硬阴影（如简单的阴影映射产生的锐利阴影边缘）不符合真实场景光照效果的问题。</p>\n<p>与简单的阴影映射不同，PCF 在判断像素是否在阴影中时，不是只比较单个点的深度。它会在像素点周围的一定区域内进行多次采样。例如，在一个以像素点为中心的小区域（通常是方形或圆形区域）内，对多个采样点进行深度比较。这些采样点的位置可以是均匀分布，也可以采用更复杂的分布方式，如泊松分布，以获得更自然的效果。</p>\n<p>对于每个采样点，比较其深度和阴影图中的深度来判断是否在阴影中。然后统计在阴影中的采样点的比例。设采样点总数为<strong>N</strong>，处于阴影中的采样点数量为<strong>n</strong>，则阴影强度可以通过公式计算<strong>shadow&#x3D;n&#x2F;N</strong>得到。这个阴影强度用于确定像素最终的阴影效果。如果阴影强度为1，表示像素完全处于阴影中；如果阴影强度为0，表示像素完全不在阴影中；介于两者之间的值表示不同程度的软阴影效果。</p>\n<h2 id=\"PCF的实现\"><a href=\"#PCF的实现\" class=\"headerlink\" title=\"PCF的实现\"></a>PCF的实现</h2><p>转换为代码如下：</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-type\">float</span> CalculatePCFShadow(<span class=\"hljs-type\">float</span> current_depth, <span class=\"hljs-type\">sampler2D</span> shadow_map,  <span class=\"hljs-type\">vec2</span> uv, <span class=\"hljs-type\">int</span> radius)<br>&#123;<br>    <span class=\"hljs-type\">float</span> shadow = <span class=\"hljs-number\">0.0</span>;<br>    <span class=\"hljs-type\">vec2</span> texel_size = <span class=\"hljs-number\">1.0</span> / <span class=\"hljs-type\">vec2</span>(<span class=\"hljs-built_in\">textureSize</span>(shadow_map, <span class=\"hljs-number\">0</span>));<br>    <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> x = -radius; x &lt;= radius; ++x)<br>    &#123;<br>        <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> y = -radius; y &lt;= radius; ++y)<br>        &#123;<br>            <span class=\"hljs-type\">float</span> pcf_depth = <span class=\"hljs-built_in\">texture</span>(shadow_map, <span class=\"hljs-type\">vec2</span>(uv + <span class=\"hljs-type\">vec2</span>(x, y) * texel_size)).r;<br>            shadow += current_depth &gt; pcf_depth ? <span class=\"hljs-number\">1.0</span> : <span class=\"hljs-number\">0.0</span>;<br>        &#125;<br>    &#125;<br>    shadow /= <span class=\"hljs-built_in\">pow</span>((<span class=\"hljs-number\">1</span>+radius*<span class=\"hljs-number\">2</span>),<span class=\"hljs-number\">2.0</span>);<br>    <span class=\"hljs-keyword\">return</span> shadow;<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<p>这里我使用了矩形的采样区域，可以看到其实当采样区域的半径为1的时候，采样点个数为9，阴影边缘的锯齿感已经得到了明显的改善。模型腿上也没有出现错误的阴影区域，效果大大提升。</p>\n<p><img src=\"/2024/12/08/soft-shadow/pcf-shadow-1.png\" alt=\"PCF阴影，采样半径1\"></p>\n<p>当采样半径提升为3，采样点个数为49时，阴影的边缘软化效果更明显了，不过付出了5倍的性能消耗，提升确并没有非常明显。</p>\n<p><img src=\"/2024/12/08/soft-shadow/pcf-shadow-3.png\" alt=\"PCF阴影，采样半径3\"> </p>\n<h1 id=\"半影的产生-PCSS\"><a href=\"#半影的产生-PCSS\" class=\"headerlink\" title=\"半影的产生-PCSS\"></a>半影的产生-PCSS</h1><h2 id=\"什么是本影和半影\"><a href=\"#什么是本影和半影\" class=\"headerlink\" title=\"什么是本影和半影\"></a>什么是本影和半影</h2><p>在实际的场景中，我们观察阴影，会发现下面这种情况：</p>\n<p><img src=\"/2024/12/08/soft-shadow/real_shadow.png\" alt=\"现实阴影\"><br>物体的深暗影子周围还有一片区域是浅浅的暗影。深暗影子的区域我们称之为“<strong>本影</strong>”，而浅暗影子的区域我们称之为“<strong>半影</strong>”。</p>\n<p>这种现象在体积光照（或者区域光照）的情况下很容易出现。其原因是，很多光源是有范围的，如下图假设有一个光源的大小用L1到L2，光源的右边有一个物体。</p>\n<p><img src=\"/2024/12/08/soft-shadow/umbra-principle.png\" alt=\"本影和半影的原理\"></p>\n<p>光源最上点位L1的位置，照向物体的时候，产生的阴影范围是<strong>A区域</strong>以及下方的<strong>B区域</strong>，上方的<strong>B区域</strong>会被L1照亮；L2点产生的阴影范围是<strong>A区域</strong>和上方的<strong>B区域</strong>，下方的<strong>B区域</strong>会被L2照亮。所以我们可以看到，<strong>区域A</strong>是光源完全的光都会被挡住的区域，所以他的阴影是最深的，是为<strong>本影</strong>。而两个<strong>区域B</strong>是挡住了光源的部分区域，同时被光源的另外一部分照亮的，是为<strong>半影</strong>。</p>\n<p><img src=\"/2024/12/08/soft-shadow/umbra-contrast.png\" alt=\"本影和半影的对照区域\"></p>\n<h2 id=\"什么是PCSS\"><a href=\"#什么是PCSS\" class=\"headerlink\" title=\"什么是PCSS\"></a>什么是PCSS</h2><p>在弄明白什么是本影和半影之后，我们来介绍一下PCSS是什么。</p>\n<p>Percentage Closer Soft Shadows（PCSS）即百分比渐近软阴影，是计算机图形学中用于生成更逼真软阴影的一种技术，它是在 Percentage Closer Filtering（PCF）基础上发展而来的。</p>\n<p>在PCF的基础上，PCSS还额外考虑了光源、遮挡物和接收阴影的物体之间的几何关系，通过这些关系来调整用于计算阴影强度的采样区域大小。通过根据阴影的不同情况动态调整采样区域的大小，PCSS能生成更自然、更符合物理规律的软阴影。</p>\n<p>这里是提出PCSS的<a href=\"https://developer.download.nvidia.com/shaderlibrary/docs/shadow_PCSS.pdf\">论文</a>。</p>\n<p>其原理总结起来就是根据采样一个区域内处于阴影的比例，来动态的调节这个区域对应的阴影的采样范围。</p>\n<h2 id=\"PCSS的实现步骤\"><a href=\"#PCSS的实现步骤\" class=\"headerlink\" title=\"PCSS的实现步骤\"></a>PCSS的实现步骤</h2><p>PCSS的实现步骤如下：</p>\n<p>首先，计算平均的遮挡物距离。在阴影图中，以当前像素点为中心，在一个初始的较小采样区域内查找深度值小于当前像素点深度的采样点，这些采样点对应的物体即为遮挡物。通过计算这些遮挡物采样点深度的平均值，得到平均遮挡物距离<strong>d_blocker</strong>。</p>\n<p><img src=\"/2024/12/08/soft-shadow/get-d_blocker-1.png\" alt=\"采样平均遮挡物距离1\"></p>\n<p>对应代码为：</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-type\">float</span> FindBlockerDepth(<span class=\"hljs-type\">sampler2D</span> shadowmap, <span class=\"hljs-type\">vec2</span> uv, <span class=\"hljs-type\">float</span> d_receiver, <span class=\"hljs-type\">float</span> radius)<br>&#123;<br>    <span class=\"hljs-type\">float</span> blocker_depth_sum = <span class=\"hljs-number\">0.0</span>;<br>    <span class=\"hljs-type\">int</span> blocker_count = <span class=\"hljs-number\">0</span>;<br>    <br>    <span class=\"hljs-comment\">// 以当前像素为中心,半径为radius的范围采样</span><br>    <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">float</span> y = -radius; y &lt;= radius; y++) &#123;<br>        <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">float</span> x = -radius; x &lt;= radius; x++) &#123;<br>            <span class=\"hljs-type\">vec2</span> <span class=\"hljs-keyword\">offset</span> = <span class=\"hljs-type\">vec2</span>(x, y) * <span class=\"hljs-number\">1.0</span> / <span class=\"hljs-type\">vec2</span>(<span class=\"hljs-built_in\">textureSize</span>(shadowmap, <span class=\"hljs-number\">0</span>));<br>            <span class=\"hljs-type\">float</span> sampleDepth = TextureProjBilinear(shadowmap, uv + <span class=\"hljs-keyword\">offset</span>);<br>            <span class=\"hljs-keyword\">if</span> (sampleDepth &lt; d_receiver) &#123;<br>                blocker_depth_sum += sampleDepth;<br>                blocker_count++;<br>            &#125;<br>        &#125;<br>    &#125;<br>    <span class=\"hljs-keyword\">return</span> blocker_count &gt; <span class=\"hljs-number\">0</span>? blocker_depth_sum / <span class=\"hljs-type\">float</span>(blocker_count) : <span class=\"hljs-number\">0.0</span>;<br>&#125;<br></code></pre></td></tr></table></figure>\n<p>其中<strong>d_receiver</strong>为当前像素点到光源的深度值，这个值可以将当前像素点位置变换到光源的投影下得到，在处理阴影贴图的时候就需要拿到了。TextureProjBilinear是获取shadowmap深度值的方法，里面采用了双线性插值的方法，不过对PCSS来说不一定需要使用这个方法。</p>\n<p>可以看到这个阶段，PCSS搜索了一个阴影贴图里面的区域（下图红色区域），记录下了这个区域的被阻挡范围的平均深度。</p>\n<p><img src=\"/2024/12/08/soft-shadow/get-d_blocker-2.png\" alt=\"采样平均遮挡物距离2\"></p>\n<p>然后根据这个范围，以及三角形相似原理，估算出半影半径。</p>\n<p><img src=\"/2024/12/08/soft-shadow/penumbra.png\" alt=\"计算半影半径\"><br>其中d_receiver、d_blocker我们已知，W_light是光源的范围大小，可以根据实际情况来调整。用图上右方的公式，得出半影的采样范围W_penumbra。</p>\n<p>代码如下：</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-comment\">// 计算遮挡物范围半径（基于相似三角形原理）</span><br><span class=\"hljs-type\">float</span> EstimateBlockerSearchRadius(<span class=\"hljs-type\">vec2</span> uv, <span class=\"hljs-type\">float</span> d_receiver, <span class=\"hljs-type\">float</span> d_blocker, <span class=\"hljs-type\">float</span> light_size)<br>&#123;<br>    <span class=\"hljs-keyword\">if</span> (d_blocker == <span class=\"hljs-number\">0.0</span>) <span class=\"hljs-keyword\">return</span> <span class=\"hljs-number\">0.0</span>;<br>    <span class=\"hljs-keyword\">return</span> (d_receiver - d_blocker) * (light_size / d_blocker);<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<p>最后，根据估算出的半影半径，扩大采样区域，然后在这个更大的区域内进行采样，并按照 PCF 的方式计算阴影强度。这样，离光源较近或遮挡物较近的地方，半影半径较小，阴影较实；离光源较远或遮挡物较远的地方，半影半径较大，阴影较虚，从而实现了更自然的软阴影效果。</p>\n<p>代码如下：</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-type\">float</span> shadow_sum = <span class=\"hljs-number\">0.0</span>f;<br><span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">int</span> pcss_i = <span class=\"hljs-number\">0</span>; pcss_i &lt; pcss_sample_count; pcss_i++)<br>&#123;<br>    <span class=\"hljs-comment\">// 可以使用泊松采样盘等方法获取更自然的采样点位置，这里简单均匀采样</span><br>    <span class=\"hljs-type\">vec2</span> <span class=\"hljs-keyword\">offset</span> = <span class=\"hljs-type\">vec2</span>(<span class=\"hljs-built_in\">cos</span>(<span class=\"hljs-type\">float</span>(pcss_i) * <span class=\"hljs-number\">2.0</span> * <span class=\"hljs-number\">3.1415926</span> / <span class=\"hljs-type\">float</span>(pcss_sample_count)),<br>    <span class=\"hljs-built_in\">sin</span>(<span class=\"hljs-type\">float</span>(pcss_i) * <span class=\"hljs-number\">2.0</span> * <span class=\"hljs-number\">3.1415926</span> / <span class=\"hljs-type\">float</span>(pcss_sample_count))) * blocker_radius;<br><br>    <span class=\"hljs-type\">vec4</span> sampleLightSpacePos = <span class=\"hljs-type\">vec4</span>(proj_coord.xy + <span class=\"hljs-keyword\">offset</span>, proj_coord.z, <span class=\"hljs-number\">1.0</span>);<br>    <span class=\"hljs-type\">float</span> sampleDepth = TextureProjBilinear(shadow_map, proj_coord.xy+<span class=\"hljs-keyword\">offset</span>);<br>    shadow_sum += sampleDepth &lt; d_recv? <span class=\"hljs-number\">1.0</span> : <span class=\"hljs-number\">0.0</span>;<br>&#125;<br>shadow = shadow_sum / pcss_sample_count;<br></code></pre></td></tr></table></figure>\n\n<h2 id=\"PCSS的效果\"><a href=\"#PCSS的效果\" class=\"headerlink\" title=\"PCSS的效果\"></a>PCSS的效果</h2><p>下面是PCSS开启和关闭的效果对比，其中PCSS关闭下PCF的采样半径是3：<br><img src=\"/2024/12/08/soft-shadow/PCSS_OFF.png\" alt=\"PCSS关闭\"></p>\n<p><img src=\"/2024/12/08/soft-shadow/PCSS_ON.png\" alt=\"PCSS开启\"></p>\n<p>可以看到开启了PCSS的效果后，遮挡物体的阴影区域，随着离遮挡物越来越远，出现了越来越明显的半影效果，效果更加自然和真实。</p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><h2 id=\"Percentage-Closer-Filtering（PCF）的作用\"><a href=\"#Percentage-Closer-Filtering（PCF）的作用\" class=\"headerlink\" title=\"Percentage Closer Filtering（PCF）的作用\"></a>Percentage Closer Filtering（PCF）的作用</h2><ol>\n<li><p>软阴影生成基础：PCF 是一种用于生成软阴影的基础技术。它基于阴影映射，在判断像素是否在阴影中时，不是只比较单个点的深度，而是在像素点周围一定区域内进行多次采样。</p>\n</li>\n<li><p>阴影强度计算：通过统计采样区域内处于阴影中的采样点比例来计算阴影强度。这种方式能有效避免硬阴影边缘的锯齿问题，使阴影边缘过渡更加自然，产生软阴影效果，提升了阴影的真实感。</p>\n</li>\n<li><p>平衡性能和效果：相对一些复杂的物理软阴影算法，PCF 较为简单，在性能和效果之间取得了较好的平衡，适用于实时渲染场景，如游戏。</p>\n</li>\n</ol>\n<h2 id=\"Percentage-Closer-Soft-Shadows（PCSS）的作用\"><a href=\"#Percentage-Closer-Soft-Shadows（PCSS）的作用\" class=\"headerlink\" title=\"Percentage Closer Soft Shadows（PCSS）的作用\"></a>Percentage Closer Soft Shadows（PCSS）的作用</h2><ol>\n<li><p>动态软阴影生成：PCSS 在 PCF 基础上进一步改进。它能够根据光源、遮挡物和接收阴影物体之间的几何关系动态调整采样区域的大小。</p>\n</li>\n<li><p>更自然的阴影过渡：通过计算平均遮挡物距离和估算半影半径，根据半影半径调整采样区域进行采样计算阴影强度。这样生成的软阴影更加符合物理规律，阴影从完全阴影到完全光照的过渡更加自然、真实，在需要高逼真度渲染的场景中能显著提升视觉质量。</p>\n</li>\n</ol>\n","excerpt":"","more":"<h1 id=\"什么是软阴影\"><a href=\"#什么是软阴影\" class=\"headerlink\" title=\"什么是软阴影\"></a>什么是软阴影</h1><p>在3D中实现阴影最基础的方法是使用阴影贴图shadowmap，根据shadowmap中存储的信息来判定当前渲染的像素是否在阴影当中。</p>\n<p>阴影贴图的方法很好理解，但是仅仅基于阴影贴图的阴影效果，在阴影的边缘会有锯齿的情况出现。这往往是由于阴影贴图的分辨率不够导致的，然而一味的提升阴影贴图的分辨率也不是方法，毕竟实时渲染的性能也是需要考虑的一个方面。</p>\n<p>那么该如何在可接受的性能表现下实现软阴影的效果呢，下面详细介绍两种方法：Percentage Closer Filtering（PCF）以及Percentage Closer Soft Shadows（PCSS）。</p>\n<h1 id=\"柔和阴影边缘-PCF\"><a href=\"#柔和阴影边缘-PCF\" class=\"headerlink\" title=\"柔和阴影边缘-PCF\"></a>柔和阴影边缘-PCF</h1><p>下面是一个普通的阴影效果：<br><img src=\"/2024/12/08/soft-shadow/normal-shadow.png\" alt=\"普通的阴影效果\"><br>这个阴影贴图的分辨率是2048，这是在<a href=\"https://ruochenhua.github.io/2024/10/13/cascade-shadow-map/\">CSM</a>的最低一级的阴影效果。可以看到阴影边缘的锯齿感非常的强烈，同时由于采样精度的问题，模型的腿上也出现了不正确的阴影区域。最简单的方法就是通过提高阴影贴图的分辨率来缓解这个问题，但是显而易见这不是最好的解决方案，而Percentage Closer Filtering（后简称PCF）可以帮助我们解决这个问题。</p>\n<h2 id=\"什么是PCF\"><a href=\"#什么是PCF\" class=\"headerlink\" title=\"什么是PCF\"></a>什么是PCF</h2><p>Percentage Closer Filtering（PCF）是一种在计算机图形学中用于生成软阴影的技术。它主要用于解决硬阴影（如简单的阴影映射产生的锐利阴影边缘）不符合真实场景光照效果的问题。</p>\n<p>与简单的阴影映射不同，PCF 在判断像素是否在阴影中时，不是只比较单个点的深度。它会在像素点周围的一定区域内进行多次采样。例如，在一个以像素点为中心的小区域（通常是方形或圆形区域）内，对多个采样点进行深度比较。这些采样点的位置可以是均匀分布，也可以采用更复杂的分布方式，如泊松分布，以获得更自然的效果。</p>\n<p>对于每个采样点，比较其深度和阴影图中的深度来判断是否在阴影中。然后统计在阴影中的采样点的比例。设采样点总数为<strong>N</strong>，处于阴影中的采样点数量为<strong>n</strong>，则阴影强度可以通过公式计算<strong>shadow&#x3D;n&#x2F;N</strong>得到。这个阴影强度用于确定像素最终的阴影效果。如果阴影强度为1，表示像素完全处于阴影中；如果阴影强度为0，表示像素完全不在阴影中；介于两者之间的值表示不同程度的软阴影效果。</p>\n<h2 id=\"PCF的实现\"><a href=\"#PCF的实现\" class=\"headerlink\" title=\"PCF的实现\"></a>PCF的实现</h2><p>转换为代码如下：</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-type\">float</span> CalculatePCFShadow(<span class=\"hljs-type\">float</span> current_depth, <span class=\"hljs-type\">sampler2D</span> shadow_map,  <span class=\"hljs-type\">vec2</span> uv, <span class=\"hljs-type\">int</span> radius)<br>&#123;<br>    <span class=\"hljs-type\">float</span> shadow = <span class=\"hljs-number\">0.0</span>;<br>    <span class=\"hljs-type\">vec2</span> texel_size = <span class=\"hljs-number\">1.0</span> / <span class=\"hljs-type\">vec2</span>(<span class=\"hljs-built_in\">textureSize</span>(shadow_map, <span class=\"hljs-number\">0</span>));<br>    <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> x = -radius; x &lt;= radius; ++x)<br>    &#123;<br>        <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> y = -radius; y &lt;= radius; ++y)<br>        &#123;<br>            <span class=\"hljs-type\">float</span> pcf_depth = <span class=\"hljs-built_in\">texture</span>(shadow_map, <span class=\"hljs-type\">vec2</span>(uv + <span class=\"hljs-type\">vec2</span>(x, y) * texel_size)).r;<br>            shadow += current_depth &gt; pcf_depth ? <span class=\"hljs-number\">1.0</span> : <span class=\"hljs-number\">0.0</span>;<br>        &#125;<br>    &#125;<br>    shadow /= <span class=\"hljs-built_in\">pow</span>((<span class=\"hljs-number\">1</span>+radius*<span class=\"hljs-number\">2</span>),<span class=\"hljs-number\">2.0</span>);<br>    <span class=\"hljs-keyword\">return</span> shadow;<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<p>这里我使用了矩形的采样区域，可以看到其实当采样区域的半径为1的时候，采样点个数为9，阴影边缘的锯齿感已经得到了明显的改善。模型腿上也没有出现错误的阴影区域，效果大大提升。</p>\n<p><img src=\"/2024/12/08/soft-shadow/pcf-shadow-1.png\" alt=\"PCF阴影，采样半径1\"></p>\n<p>当采样半径提升为3，采样点个数为49时，阴影的边缘软化效果更明显了，不过付出了5倍的性能消耗，提升确并没有非常明显。</p>\n<p><img src=\"/2024/12/08/soft-shadow/pcf-shadow-3.png\" alt=\"PCF阴影，采样半径3\"> </p>\n<h1 id=\"半影的产生-PCSS\"><a href=\"#半影的产生-PCSS\" class=\"headerlink\" title=\"半影的产生-PCSS\"></a>半影的产生-PCSS</h1><h2 id=\"什么是本影和半影\"><a href=\"#什么是本影和半影\" class=\"headerlink\" title=\"什么是本影和半影\"></a>什么是本影和半影</h2><p>在实际的场景中，我们观察阴影，会发现下面这种情况：</p>\n<p><img src=\"/2024/12/08/soft-shadow/real_shadow.png\" alt=\"现实阴影\"><br>物体的深暗影子周围还有一片区域是浅浅的暗影。深暗影子的区域我们称之为“<strong>本影</strong>”，而浅暗影子的区域我们称之为“<strong>半影</strong>”。</p>\n<p>这种现象在体积光照（或者区域光照）的情况下很容易出现。其原因是，很多光源是有范围的，如下图假设有一个光源的大小用L1到L2，光源的右边有一个物体。</p>\n<p><img src=\"/2024/12/08/soft-shadow/umbra-principle.png\" alt=\"本影和半影的原理\"></p>\n<p>光源最上点位L1的位置，照向物体的时候，产生的阴影范围是<strong>A区域</strong>以及下方的<strong>B区域</strong>，上方的<strong>B区域</strong>会被L1照亮；L2点产生的阴影范围是<strong>A区域</strong>和上方的<strong>B区域</strong>，下方的<strong>B区域</strong>会被L2照亮。所以我们可以看到，<strong>区域A</strong>是光源完全的光都会被挡住的区域，所以他的阴影是最深的，是为<strong>本影</strong>。而两个<strong>区域B</strong>是挡住了光源的部分区域，同时被光源的另外一部分照亮的，是为<strong>半影</strong>。</p>\n<p><img src=\"/2024/12/08/soft-shadow/umbra-contrast.png\" alt=\"本影和半影的对照区域\"></p>\n<h2 id=\"什么是PCSS\"><a href=\"#什么是PCSS\" class=\"headerlink\" title=\"什么是PCSS\"></a>什么是PCSS</h2><p>在弄明白什么是本影和半影之后，我们来介绍一下PCSS是什么。</p>\n<p>Percentage Closer Soft Shadows（PCSS）即百分比渐近软阴影，是计算机图形学中用于生成更逼真软阴影的一种技术，它是在 Percentage Closer Filtering（PCF）基础上发展而来的。</p>\n<p>在PCF的基础上，PCSS还额外考虑了光源、遮挡物和接收阴影的物体之间的几何关系，通过这些关系来调整用于计算阴影强度的采样区域大小。通过根据阴影的不同情况动态调整采样区域的大小，PCSS能生成更自然、更符合物理规律的软阴影。</p>\n<p>这里是提出PCSS的<a href=\"https://developer.download.nvidia.com/shaderlibrary/docs/shadow_PCSS.pdf\">论文</a>。</p>\n<p>其原理总结起来就是根据采样一个区域内处于阴影的比例，来动态的调节这个区域对应的阴影的采样范围。</p>\n<h2 id=\"PCSS的实现步骤\"><a href=\"#PCSS的实现步骤\" class=\"headerlink\" title=\"PCSS的实现步骤\"></a>PCSS的实现步骤</h2><p>PCSS的实现步骤如下：</p>\n<p>首先，计算平均的遮挡物距离。在阴影图中，以当前像素点为中心，在一个初始的较小采样区域内查找深度值小于当前像素点深度的采样点，这些采样点对应的物体即为遮挡物。通过计算这些遮挡物采样点深度的平均值，得到平均遮挡物距离<strong>d_blocker</strong>。</p>\n<p><img src=\"/2024/12/08/soft-shadow/get-d_blocker-1.png\" alt=\"采样平均遮挡物距离1\"></p>\n<p>对应代码为：</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-type\">float</span> FindBlockerDepth(<span class=\"hljs-type\">sampler2D</span> shadowmap, <span class=\"hljs-type\">vec2</span> uv, <span class=\"hljs-type\">float</span> d_receiver, <span class=\"hljs-type\">float</span> radius)<br>&#123;<br>    <span class=\"hljs-type\">float</span> blocker_depth_sum = <span class=\"hljs-number\">0.0</span>;<br>    <span class=\"hljs-type\">int</span> blocker_count = <span class=\"hljs-number\">0</span>;<br>    <br>    <span class=\"hljs-comment\">// 以当前像素为中心,半径为radius的范围采样</span><br>    <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">float</span> y = -radius; y &lt;= radius; y++) &#123;<br>        <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">float</span> x = -radius; x &lt;= radius; x++) &#123;<br>            <span class=\"hljs-type\">vec2</span> <span class=\"hljs-keyword\">offset</span> = <span class=\"hljs-type\">vec2</span>(x, y) * <span class=\"hljs-number\">1.0</span> / <span class=\"hljs-type\">vec2</span>(<span class=\"hljs-built_in\">textureSize</span>(shadowmap, <span class=\"hljs-number\">0</span>));<br>            <span class=\"hljs-type\">float</span> sampleDepth = TextureProjBilinear(shadowmap, uv + <span class=\"hljs-keyword\">offset</span>);<br>            <span class=\"hljs-keyword\">if</span> (sampleDepth &lt; d_receiver) &#123;<br>                blocker_depth_sum += sampleDepth;<br>                blocker_count++;<br>            &#125;<br>        &#125;<br>    &#125;<br>    <span class=\"hljs-keyword\">return</span> blocker_count &gt; <span class=\"hljs-number\">0</span>? blocker_depth_sum / <span class=\"hljs-type\">float</span>(blocker_count) : <span class=\"hljs-number\">0.0</span>;<br>&#125;<br></code></pre></td></tr></table></figure>\n<p>其中<strong>d_receiver</strong>为当前像素点到光源的深度值，这个值可以将当前像素点位置变换到光源的投影下得到，在处理阴影贴图的时候就需要拿到了。TextureProjBilinear是获取shadowmap深度值的方法，里面采用了双线性插值的方法，不过对PCSS来说不一定需要使用这个方法。</p>\n<p>可以看到这个阶段，PCSS搜索了一个阴影贴图里面的区域（下图红色区域），记录下了这个区域的被阻挡范围的平均深度。</p>\n<p><img src=\"/2024/12/08/soft-shadow/get-d_blocker-2.png\" alt=\"采样平均遮挡物距离2\"></p>\n<p>然后根据这个范围，以及三角形相似原理，估算出半影半径。</p>\n<p><img src=\"/2024/12/08/soft-shadow/penumbra.png\" alt=\"计算半影半径\"><br>其中d_receiver、d_blocker我们已知，W_light是光源的范围大小，可以根据实际情况来调整。用图上右方的公式，得出半影的采样范围W_penumbra。</p>\n<p>代码如下：</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-comment\">// 计算遮挡物范围半径（基于相似三角形原理）</span><br><span class=\"hljs-type\">float</span> EstimateBlockerSearchRadius(<span class=\"hljs-type\">vec2</span> uv, <span class=\"hljs-type\">float</span> d_receiver, <span class=\"hljs-type\">float</span> d_blocker, <span class=\"hljs-type\">float</span> light_size)<br>&#123;<br>    <span class=\"hljs-keyword\">if</span> (d_blocker == <span class=\"hljs-number\">0.0</span>) <span class=\"hljs-keyword\">return</span> <span class=\"hljs-number\">0.0</span>;<br>    <span class=\"hljs-keyword\">return</span> (d_receiver - d_blocker) * (light_size / d_blocker);<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<p>最后，根据估算出的半影半径，扩大采样区域，然后在这个更大的区域内进行采样，并按照 PCF 的方式计算阴影强度。这样，离光源较近或遮挡物较近的地方，半影半径较小，阴影较实；离光源较远或遮挡物较远的地方，半影半径较大，阴影较虚，从而实现了更自然的软阴影效果。</p>\n<p>代码如下：</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-type\">float</span> shadow_sum = <span class=\"hljs-number\">0.0</span>f;<br><span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">int</span> pcss_i = <span class=\"hljs-number\">0</span>; pcss_i &lt; pcss_sample_count; pcss_i++)<br>&#123;<br>    <span class=\"hljs-comment\">// 可以使用泊松采样盘等方法获取更自然的采样点位置，这里简单均匀采样</span><br>    <span class=\"hljs-type\">vec2</span> <span class=\"hljs-keyword\">offset</span> = <span class=\"hljs-type\">vec2</span>(<span class=\"hljs-built_in\">cos</span>(<span class=\"hljs-type\">float</span>(pcss_i) * <span class=\"hljs-number\">2.0</span> * <span class=\"hljs-number\">3.1415926</span> / <span class=\"hljs-type\">float</span>(pcss_sample_count)),<br>    <span class=\"hljs-built_in\">sin</span>(<span class=\"hljs-type\">float</span>(pcss_i) * <span class=\"hljs-number\">2.0</span> * <span class=\"hljs-number\">3.1415926</span> / <span class=\"hljs-type\">float</span>(pcss_sample_count))) * blocker_radius;<br><br>    <span class=\"hljs-type\">vec4</span> sampleLightSpacePos = <span class=\"hljs-type\">vec4</span>(proj_coord.xy + <span class=\"hljs-keyword\">offset</span>, proj_coord.z, <span class=\"hljs-number\">1.0</span>);<br>    <span class=\"hljs-type\">float</span> sampleDepth = TextureProjBilinear(shadow_map, proj_coord.xy+<span class=\"hljs-keyword\">offset</span>);<br>    shadow_sum += sampleDepth &lt; d_recv? <span class=\"hljs-number\">1.0</span> : <span class=\"hljs-number\">0.0</span>;<br>&#125;<br>shadow = shadow_sum / pcss_sample_count;<br></code></pre></td></tr></table></figure>\n\n<h2 id=\"PCSS的效果\"><a href=\"#PCSS的效果\" class=\"headerlink\" title=\"PCSS的效果\"></a>PCSS的效果</h2><p>下面是PCSS开启和关闭的效果对比，其中PCSS关闭下PCF的采样半径是3：<br><img src=\"/2024/12/08/soft-shadow/PCSS_OFF.png\" alt=\"PCSS关闭\"></p>\n<p><img src=\"/2024/12/08/soft-shadow/PCSS_ON.png\" alt=\"PCSS开启\"></p>\n<p>可以看到开启了PCSS的效果后，遮挡物体的阴影区域，随着离遮挡物越来越远，出现了越来越明显的半影效果，效果更加自然和真实。</p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><h2 id=\"Percentage-Closer-Filtering（PCF）的作用\"><a href=\"#Percentage-Closer-Filtering（PCF）的作用\" class=\"headerlink\" title=\"Percentage Closer Filtering（PCF）的作用\"></a>Percentage Closer Filtering（PCF）的作用</h2><ol>\n<li><p>软阴影生成基础：PCF 是一种用于生成软阴影的基础技术。它基于阴影映射，在判断像素是否在阴影中时，不是只比较单个点的深度，而是在像素点周围一定区域内进行多次采样。</p>\n</li>\n<li><p>阴影强度计算：通过统计采样区域内处于阴影中的采样点比例来计算阴影强度。这种方式能有效避免硬阴影边缘的锯齿问题，使阴影边缘过渡更加自然，产生软阴影效果，提升了阴影的真实感。</p>\n</li>\n<li><p>平衡性能和效果：相对一些复杂的物理软阴影算法，PCF 较为简单，在性能和效果之间取得了较好的平衡，适用于实时渲染场景，如游戏。</p>\n</li>\n</ol>\n<h2 id=\"Percentage-Closer-Soft-Shadows（PCSS）的作用\"><a href=\"#Percentage-Closer-Soft-Shadows（PCSS）的作用\" class=\"headerlink\" title=\"Percentage Closer Soft Shadows（PCSS）的作用\"></a>Percentage Closer Soft Shadows（PCSS）的作用</h2><ol>\n<li><p>动态软阴影生成：PCSS 在 PCF 基础上进一步改进。它能够根据光源、遮挡物和接收阴影物体之间的几何关系动态调整采样区域的大小。</p>\n</li>\n<li><p>更自然的阴影过渡：通过计算平均遮挡物距离和估算半影半径，根据半影半径调整采样区域进行采样计算阴影强度。这样生成的软阴影更加符合物理规律，阴影从完全阴影到完全光照的过渡更加自然、真实，在需要高逼真度渲染的场景中能显著提升视觉质量。</p>\n</li>\n</ol>\n"},{"title":"基于单次散射的天空大气渲染方法","date":"2024-10-15T14:03:02.000Z","index_img":"/2024/10/15/single-scatter-atmosphere/single-scatter-atmosphere.png","banner_img":"/2024/10/15/single-scatter-atmosphere/single-scatter-atmosphere.png","_content":"\n\n最近KongEngine实现了IBL(Image Based Lighting)，可以将HDR环境贴图映射作为3D场景的环境。\n\n![KongEngine的IBL效果](kong-screen-shot.png)\n\n在实现了IBL之后我又产生了一个想法，能否实现类似UE中的大气环境的渲染效果呢？我尝试去寻找答案，发现如果要完全复刻UE中的效果确实需要一定的功夫的，但是最基础的天空大气渲染并没有想象中那么复杂，于是我便花了点时间在KongEngine中实现了这个功能。\n\n![KongEngine天空大气效果](single-scatter-atmosphere.png)\n\n我打算将这个方法的基础思想和实现在此简单记录一下。\n\n# 单次散射模型\n星球的大气层是一种参与性介质，和在真空环境不同，光在大气层中传播的时候会因为大气中的微小颗粒（水、灰尘等等）发生散射（折射、反射）和吸收等情况。因此我们看向空中的一个点的时候，这个点的颜色是光经过多次散射得到的结果。\n\n光到达我们眼睛之前经过多少次反射和折射是不一定的，在实时渲染的需求下计算太多次光的变化显然也是不显示的。最简单的方法，是使用单次散射模型：我们假定光在进入我们眼睛之前，有且只发生了一次散射。光一般在第一次散射的时候，还会有最多的能量剩余，后面的多次散射能力相对少，对最后效果的呈现也影响不大，因此这种模型可以在保证性能的情况下，还有不错的效果。\n\n![](https://www.alanzucconi.com/wp-content/uploads/2017/09/scattering_02.png)\n\n除了散射，光在传播的时候会被大气吸收。一般来说，如果在一个均匀的介质中传播的话，光的被吸收的部分和介质的密度，以及传播的路径长度正相关。放到单散射模型的例子中，就是需要计算光在散射前和散射后的路径上，被吸收了多少能量。\n![](https://www.alanzucconi.com/wp-content/uploads/2017/09/scattering_07.png)\n\n按照上面的图，假设相机在A点，观察方向为AB，其中有一束光在P点发生散射后沿着PA进入相机。如果我们知道了光线损耗和距离相关的公式，那么似乎是只要计算出CP和PA的长度，在带入公式后就可以得到光该路径上传播后的最终能量了。\n\n![](https://www.alanzucconi.com/wp-content/uploads/2017/09/scattering_10a.png)\n\n但是对于大气层来说，它的密度并不是均匀的，而是和大气层距离地面的高度相关。所以在计算CP段光线损耗的时候，我们需要将这段距离分为多个小段，每个小段的损耗公式带入对应平均高度，在最后将所有结果相加才。小段划分的越密集，则结果越准确。\n\n如果我们还要考虑大气中的其他因素，比如说大气中的云层，在这些区域中光的损耗就不仅仅是和高度相关了。\n\n另外一点就是，当我们考虑AB方向的光线时，它的最终效果，是在大气层内的所有AB连线上面的点的散射结果的总和（P0、P1、P2...Pn），因此我们也同样的需要对AB线段进行分段采样并将结果叠加。AB线段上的每一个采样结果按照上面所描述的计算单一P点的方式。\n\n![](https://www.alanzucconi.com/wp-content/uploads/2017/09/scattering_08a.png)\n\n以上便是大气单次散射模型的基本思路。\n\n# 散射的计算\n之前我们提到了计算光的散射，这里我们介绍一下散射的相关计算。会提及一些公式，但是不会做过多的数学推导工作，对推导过程感兴趣的可以看一下文章中的参考资料。\n\n一般来说，天空大气的散射主要包括两种，分别是**瑞利散射**和**米氏散射**。\n\n - 瑞利散射是当光线通过介质中的微小颗粒或分子时发生的散射现象。由于颗粒或分子的尺寸远小于光的波长，不同波长的光线会以不同的角度散射，一个最经典的案例就是因为蓝色的波长是较短的，所以蓝色很容易发生瑞利散射，导致天空呈现蓝色。\n - 米氏散射则一般由空气中含有的较大颗粒的介质，如气溶胶、灰尘、水滴等引起。和瑞利散射不同的是，米氏散射和光的波长关系并不大。\n ![](https://www.alanzucconi.com/wp-content/uploads/2017/09/scattering_13.png)\n\n那么根据前面的模型思路，光在P点上发生了散射，其中一部分能量沿着PA方向进入相机的实现。获取这一部分能量的计算函数被称之为<strong>相位函数（Phase Function）</strong>。相位函数是用来描述，当光线发生散射的情况时，某个方向（一般是和原光线方向的夹角）占原光线的能量的比例。瑞利散射和米氏散射的相位函数是不同的，同一种散射的相位函数也会有不同的方法进行拟合，这里我不想去做过多的公式推导的工作，有兴趣的可以去查看[参考资料](https://patapom.com/topics/Revision2013/Revision%202013%20-%20Real-time%20Volumetric%20Rendering%20Course%20Notes.pdf)的推导过程。\n\n下方是瑞利散射的相位函数，theta是原光线方向和目标方向的夹角：\n<!-- wp:html -->\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n  <mtable displaystyle=\"true\" columnalign=\"right left right\" columnspacing=\"0em 2em\" rowspacing=\"3pt\">\n    <mtr>\n      <mtd>\n        <mi>P</mi>\n        <mo stretchy=\"false\">(</mo>\n        <mi>&#x3B8;</mi>\n        <mo stretchy=\"false\">)</mo>\n      </mtd>        \n      <mtd>\n        <mi></mi>\n        <mo>=</mo>\n        <mfrac>\n          <mn>3</mn>\n          <mrow>\n            <mn>16</mn>\n            <mi>&#x3C0;</mi>\n          </mrow>\n        </mfrac>\n        <mo stretchy=\"false\">(</mo>\n        <mn>1</mn>\n        <mo>+</mo>\n        <mi>c</mi>\n        <mi>o</mi>\n        <msup>\n          <mi>s</mi>\n          <mn>2</mn>\n        </msup>\n        <mi>&#x3B8;</mi>\n        <mo stretchy=\"false\">)</mo>\n      </mtd>\n    </mtr>\n  </mtable>\n</math>\n<!-- /wp:html -->\n\n<!-- wp:paragraph -->\n下方是米氏散射的相位函数，这里使用的是[Henyey-Greenstein函数](https://omlc.org/classroom/ece532/class3/hg.html)来近似。\n<!-- /wp:paragraph -->\n\n<!-- wp:html -->\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">  <mi>P</mi>  <mo stretchy=\"false\">(</mo>  <mi>&#x3B8;</mi>  <mo stretchy=\"false\">)</mo>  <mo>=</mo>  <mfrac>    <mrow>      <mn>1</mn>      <mo>&#x2212;</mo>      <msup>        <mi>g</mi>        <mrow>          <mn>2</mn>        </mrow>      </msup>    </mrow>    <mrow>      <mn>4</mn>      <mi>&#x3C0;</mi>      <mo stretchy=\"false\">(</mo>      <mn>1</mn>      <mo>+</mo>      <msup>        <mi>g</mi>        <mrow>          <mn>2</mn>        </mrow>      </msup>      <mo>&#x2212;</mo>      <mn>2</mn>      <mi>g</mi>      <mi>c</mi>      <mi>o</mi>      <mi>s</mi>      <mo stretchy=\"false\">(</mo>      <mi>&#x3B8;</mi>      <mo stretchy=\"false\">)</mo>      <msup>        <mo stretchy=\"false\">)</mo>        <mrow>          <mn>3</mn>          <mrow>            <mo>/</mo>          </mrow>          <mn>2</mn>        </mrow>      </msup>    </mrow>  </mfrac></math>\n<!-- /wp:html -->\n\n<!-- wp:paragraph -->\n<p>好了，现在我们知道了如何计算光线在P点的散射行为。剩下的工作就是需要计算光线在传播过程的损耗了。这一部分的计算方法称之为<strong>衰减系数</strong>，或者<strong>消光系数</strong>。消光系数中的一部分，需要对光线在传播的路线的长度和空气密度进行积分。在程序中的表示就是将光的传播路径分为小段，将每一小段的长度和平均密度（或者小段中点的介质密度）相乘而得到，这个结果我们称之为<strong>光学距离（Optical Depth）</strong>。</p>\n<!-- /wp:paragraph -->\n\n<!-- wp:paragraph -->\n<p>衰减系数的计算公式如下，红色部分代表的是在海拔为h的散射系数。他可以分解为在海平面上的散射系数乘以在海拔h的介质密度。</p>\n<!-- /wp:paragraph -->\n\n<!-- wp:html -->\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n  <mstyle mathcolor=\"#00AAFF\">\n    <mi>T</mi>\n    <mo stretchy=\"false\">(</mo>\n    <mi>P</mi>\n    <mi>A</mi>\n    <mo stretchy=\"false\">)</mo>\n  </mstyle>\n  <mo>=</mo>\n<mstyle mathcolor=\"#00AA00\">\n  <mi>exp</mi>\n</mstyle>\n  <mrow data-mjx-texclass=\"ORD\">\n    <mo>&#x2212;</mo>\n    <msubsup>\n      <mo data-mjx-texclass=\"OP\">&#x222B;</mo>\n      <mi>P</mi>\n      <mi>A</mi>\n    </msubsup>\n    <mrow data-mjx-texclass=\"ORD\">\n      <mstyle mathcolor=\"Red\">\n        <mi>&#x3B2;</mi>\n        <mo stretchy=\"false\">(</mo>\n        <mi>&#x3BB;</mi>\n        <mo>,</mo>\n        <mi>h</mi>\n        <mo stretchy=\"false\">)</mo>\n      </mstyle>\n    </mrow>\n    <mi>d</mi>\n    <mi>s</mi>\n  </mrow>\n</math>\n<!-- /wp:html -->\n\n<!-- wp:paragraph -->\n<p>分解出来后得到的结果如下，其中红色的部分代表的意思是在海平面的散射系数，是一个常量，黄色部分代表了在海拔为h的介质的密度。这个公式的积分便是针对AP路线上的介质密度，其结果也就是光学距离。</p>\n<!-- /wp:paragraph -->\n\n<!-- wp:html -->\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n  <mstyle mathcolor=\"#00AAFF\">\n    <mi>T</mi>\n    <mo stretchy=\"false\">(</mo>\n    <mi>P</mi>\n    <mi>A</mi>\n    <mo stretchy=\"false\">)</mo>\n  </mstyle>\n  <mo>=</mo>\n    <mstyle mathcolor=\"#00AA00\">\n      <mi>exp</mi>\n    </mstyle>\n  <mrow data-mjx-texclass=\"ORD\">\n    <mo>&#x2212;</mo>\n    <mstyle mathcolor=\"Red\">\n      <mi>&#x3B2;</mi>\n      <mo stretchy=\"false\">(</mo>\n      <mi>&#x3BB;</mi>\n      <mo stretchy=\"false\">)</mo>\n    </mstyle>\n    <msubsup>\n      <mo data-mjx-texclass=\"OP\">&#x222B;</mo>\n      <mi>P</mi>\n      <mi>A</mi>\n    </msubsup>\n    <mrow data-mjx-texclass=\"ORD\">\n      <mstyle mathcolor=\"Gold\">\n        <mi>&#x3C1;</mi>\n        <mo stretchy=\"false\">(</mo>\n        <mi>h</mi>\n        <mo stretchy=\"false\">)</mo>\n      </mstyle>\n    </mrow>\n    <mi>d</mi>\n    <mi>s</mi>\n  </mrow>\n</math>\n<!-- /wp:html -->\n\n<!-- wp:paragraph -->\n<p>介质密度的计算公式如下，其中H代表的是一个基准海拔，是一个常量。不同散射的基准海拔有所不同，瑞利散射的基准海拔是*8500*，而米氏散射的基准海拔是*1200*。</p>\n<!-- /wp:paragraph -->\n\n<!-- wp:html -->\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n  <mstyle mathcolor=\"Gold\">\n    <mi>&#x3C1;</mi>\n    <mo stretchy=\"false\">(</mo>\n    <mi>h</mi>\n    <mo stretchy=\"false\">)</mo>\n  </mstyle>\n  <mo>=</mo>\n  <mi>exp</mi>\n  <mo stretchy=\"false\">(</mo>\n  <mo>&#x2212;</mo>\n  <mfrac>\n    <mi>h</mi>\n    <mi>H</mi>\n  </mfrac>\n  <mo stretchy=\"false\">)</mo>\n</math>\n<!-- /wp:html -->\n\n自此，我们关于单次散射的天空大气渲染方法的基本预备知识已经了解的差不多了，接下来介绍shader的相关代码。\n\n\n# Shader代码\n首先，为了计算AB、PC等视线和光线在大气层内的传播距离，我们需要一个函数来计算射线和球体的相交情况。\n\n```glsl\nvec2 ray_sphere_intersection(vec3 ray_origin, vec3 ray_direction, vec3 sphere_center, float sphere_radius)\n{\n    // ray-sphere intersection that assumes\n    float a = dot(ray_direction, ray_direction);\n    vec3 oc = ray_origin - sphere_center;\n    float b = 2.0 * dot(ray_direction, oc);\n    float c = dot(oc, oc) - (sphere_radius * sphere_radius);\n    float d = (b*b) - 4.0*a*c;\n\n    // 返回击中结果，y小于x代表无结果\n    if (d < 0.0) return vec2(1e10,-1e10);\n    // 击中的话有两个相同或者不同的结果\n    return vec2(\n        (-b - sqrt(d))/(2.0*a),\n        (-b + sqrt(d))/(2.0*a)\n    );\n}\n```\n\n<!-- wp:paragraph -->\n<p>好了，那有了这个辅助公式，我们正式开始计算大气的颜色。首先，按照之前的理论，我们要计算视线向量在大气层内的长度，所以我们对射线方向和大气层，以及射线方向和星球表面做射线检测。得到视线在大气层内的长度后，根据我们预设的想要采样的步数（iSteps），计算出每次采样的长度ds。</p>\n<!-- /wp:paragraph -->\n\n```glsl\nray_dir = normalize(ray_dir);\n\n// 视线和大气层大小的尺寸的射线检测\n// x为大气入射点的距离、y为大气出射点的距离（x==y代表光线和大气球体相切，x>y代表光线不经过大气）\nvec2 atmos_hit = ray_sphere_intersection(ray_origin, ray_dir, rAtmos);\n// 未击中，返回0\nif (atmos_hit.x > atmos_hit.y) return vec3(0,0,0);\n\n    // 视线和星球做射线检测，取得近处的检测结果（远处的那个光被星球本体遮挡）\nvec2 planet_hit = ray_sphere_intersection(ray_origin, ray_dir, planet_radius);\nfloat light_distance = atmos_hit.y;\n\n// hit the planet\nif(planet_hit.x < planet_hit.y && planet_hit.x > 0.1)\n{\n    light_distance = planet_hit.x;\n}\n\n// light sample length\nfloat ds = light_distance / float(iSteps);\n```\n\n<!-- wp:paragraph -->\n<p>那么接下来，便是进入采样的循环。循环有两层，外面的循环是计算<strong>视线方向上的每个采样点的光能量</strong>，而计算这个点的光能量也需要一个循环，这个循环是用于采样该点和光源之间的连线在大气层内的线段（也就是之前图片的PC段）的光学距离。根据前面提到的公式，光学距离乘以海平面的散射系数便可以得到光的衰减系数，因此jSteps循环可以得到光线在进入大气层后，传播到达视线上的采样点（P0、P1...Pn点）的衰减所剩下的能量。</p>\n<!-- /wp:paragraph -->\n\n``` glsl\n// Initialize the primary ray time.\nfloat iTime = 0.0;\n\n// Initialize accumulators for Rayleigh and Mie scattering.\nvec3 total_scatter_rlh = vec3(0,0,0);\nvec3 total_scatter_mie = vec3(0,0,0);\n\n// Initialize optical depth accumulators for the primary ray.\nfloat total_od_rlh = 0.0;\nfloat total_od_mie = 0.0;\n\n// 对每个视线上的采样点循环\nfor (int i = 0; i < iSteps; i++) {\n    // 获取到采样点的位置\n    vec3 iPos = ray_origin + ray_dir * (iTime + ds * 0.5);\n\n    // 在当前点向太阳的位置做射线检测，以大气的半径为球体。.y是代表大气的出射点，j_steps代表采样数\n    float jStepSize = ray_sphere_intersection(iPos, pSun, rAtmos).y / float(jSteps);\n\n    float jTime = 0.0;\n    float jOdRlh = 0.0;\n    float jOdMie = 0.0;\n\n    // 在当前采样到大气入射点的距离上，采样计算\n    for (int j = 0; j < jSteps; j++) {\n        // 计算采样点到光源的衰减\n        vec3 jPos = iPos + pSun * (jTime + jStepSize * 0.5);\n\n        float jHeight = length(jPos-planet_center) - planet_radius;\n\n        // Accumulate the optical depth.\n        jOdRlh += get_atmos_density(jHeight, scale_height_rlh) * jStepSize;\n        jOdMie += get_atmos_density(jHeight, scale_height_mie) * jStepSize;\n\n        // Increment the secondary ray time.\n        jTime += jStepSize;\n    }\n\n    // 观察点和星球表面距离\n    float surface_height = length(iPos-planet_center) - planet_radius;\n\n    // 计算这一步的散射的光学深度结果\n    float od_step_rlh = get_atmos_density(surface_height, scale_height_rlh) * ds;\n    float od_step_mie = get_atmos_density(surface_height, scale_height_mie) * ds;\n    \n    total_od_rlh += od_step_rlh;\n    total_od_mie += od_step_mie;\n\n    // 计算衰减系数，光在经过一定距离后衰减剩下来的比例。\n    vec3 attn = exp(-(kMie * (total_od_mie + jOdMie) + kRlh * (total_od_rlh + jOdRlh)));\n\n    // Accumulate scattering.\n    total_scatter_rlh += od_step_rlh * attn;\n    total_scatter_mie += od_step_mie * attn;\n\n    // Increment the primary ray time.\n    iTime += ds;\n}\n```\n\n<!-- wp:paragraph -->\n<p>计算瑞利散射和米氏散射的大气密度的代码如下所示，其中scale_height在计算瑞利散射的时候带入的是8500，米氏散射则带入1200。</p>\n<!-- /wp:paragraph -->\n\n```glsl\n// 获取大气密度\n// 传入位置离海平面的高度，以及散射的相关基准高度\n// 大气中任意一点的散射系数的计算，简化拆解为散射在海平面的散射系数，乘以基于海平面高度的该散射的大气密度计算公式\nfloat get_atmos_density(float height_to_sea_level, float scale_height)\n{\n    return exp(-height_to_sea_level / scale_height);\n}\n```\n\n<!-- wp:paragraph -->\n<p>将iSteps循环的结果累加起来，我们便得到了视线上（AB）的每个采样点的光能量传播到达相机（A点）经过衰减的能量的和。那么还剩一个部分，就是光在P点进行散射的时候，光也损失掉了一部分能量，这个过程可以通过乘以米氏散射和瑞利散射的相位函数来计算。因为每个采样点P的结果都需要乘以相位函数，所以我们可以将它提取到最外面，乘以光的能量的总和即可。</p>\n<!-- /wp:paragraph -->\n\n<!-- wp:paragraph -->\n<p>不过需要注意的是，前面的公式提到了，相位函数是和光线入射方向和目标方向的夹角相关的，所以如果我们<strong>默认太阳光源是平行光</strong>的话，这个夹角对每个采样点来说是一样的，所以可以将相位函数提取到最外层。如果认为这个夹角对每个采样点不一样的话，那也许还是应该老老实实在计算每个采样点的光的时候单独去处理。</p>\n<!-- /wp:paragraph -->\n\n最终的结果除了乘以相位函数，还需要乘以海平面的散射系数，结果如下。</p>\n\n```glsl\n// 计算并返回最终颜色\n// iSun是光源（太阳）的颜色\nreturn iSun * (pRlh * kRlh * total_scatter_rlh + pMie * kMie * total_scatter_mie);\n```\n下面是得到的结果：\n\n<iframe src=\"single-scatter-atmosphere.mp4\" scrolling=\"no\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\"> </iframe>\n\n\n# 参考资料\n- https://www.alanzucconi.com/2017/10/10/atmospheric-scattering-1/\n- https://www.xianlongok.site/post/8e5d3b12/\n","source":"_posts/single-scatter-atmosphere.md","raw":"---\ntitle: 基于单次散射的天空大气渲染方法\ndate: 2024-10-15 22:03:02\ncategories: \n\t- 技术漫谈\ntags: [3D, render, 渲染, 编程]\n\nindex_img: /2024/10/15/single-scatter-atmosphere/single-scatter-atmosphere.png\nbanner_img: /2024/10/15/single-scatter-atmosphere/single-scatter-atmosphere.png\n---\n\n\n最近KongEngine实现了IBL(Image Based Lighting)，可以将HDR环境贴图映射作为3D场景的环境。\n\n![KongEngine的IBL效果](kong-screen-shot.png)\n\n在实现了IBL之后我又产生了一个想法，能否实现类似UE中的大气环境的渲染效果呢？我尝试去寻找答案，发现如果要完全复刻UE中的效果确实需要一定的功夫的，但是最基础的天空大气渲染并没有想象中那么复杂，于是我便花了点时间在KongEngine中实现了这个功能。\n\n![KongEngine天空大气效果](single-scatter-atmosphere.png)\n\n我打算将这个方法的基础思想和实现在此简单记录一下。\n\n# 单次散射模型\n星球的大气层是一种参与性介质，和在真空环境不同，光在大气层中传播的时候会因为大气中的微小颗粒（水、灰尘等等）发生散射（折射、反射）和吸收等情况。因此我们看向空中的一个点的时候，这个点的颜色是光经过多次散射得到的结果。\n\n光到达我们眼睛之前经过多少次反射和折射是不一定的，在实时渲染的需求下计算太多次光的变化显然也是不显示的。最简单的方法，是使用单次散射模型：我们假定光在进入我们眼睛之前，有且只发生了一次散射。光一般在第一次散射的时候，还会有最多的能量剩余，后面的多次散射能力相对少，对最后效果的呈现也影响不大，因此这种模型可以在保证性能的情况下，还有不错的效果。\n\n![](https://www.alanzucconi.com/wp-content/uploads/2017/09/scattering_02.png)\n\n除了散射，光在传播的时候会被大气吸收。一般来说，如果在一个均匀的介质中传播的话，光的被吸收的部分和介质的密度，以及传播的路径长度正相关。放到单散射模型的例子中，就是需要计算光在散射前和散射后的路径上，被吸收了多少能量。\n![](https://www.alanzucconi.com/wp-content/uploads/2017/09/scattering_07.png)\n\n按照上面的图，假设相机在A点，观察方向为AB，其中有一束光在P点发生散射后沿着PA进入相机。如果我们知道了光线损耗和距离相关的公式，那么似乎是只要计算出CP和PA的长度，在带入公式后就可以得到光该路径上传播后的最终能量了。\n\n![](https://www.alanzucconi.com/wp-content/uploads/2017/09/scattering_10a.png)\n\n但是对于大气层来说，它的密度并不是均匀的，而是和大气层距离地面的高度相关。所以在计算CP段光线损耗的时候，我们需要将这段距离分为多个小段，每个小段的损耗公式带入对应平均高度，在最后将所有结果相加才。小段划分的越密集，则结果越准确。\n\n如果我们还要考虑大气中的其他因素，比如说大气中的云层，在这些区域中光的损耗就不仅仅是和高度相关了。\n\n另外一点就是，当我们考虑AB方向的光线时，它的最终效果，是在大气层内的所有AB连线上面的点的散射结果的总和（P0、P1、P2...Pn），因此我们也同样的需要对AB线段进行分段采样并将结果叠加。AB线段上的每一个采样结果按照上面所描述的计算单一P点的方式。\n\n![](https://www.alanzucconi.com/wp-content/uploads/2017/09/scattering_08a.png)\n\n以上便是大气单次散射模型的基本思路。\n\n# 散射的计算\n之前我们提到了计算光的散射，这里我们介绍一下散射的相关计算。会提及一些公式，但是不会做过多的数学推导工作，对推导过程感兴趣的可以看一下文章中的参考资料。\n\n一般来说，天空大气的散射主要包括两种，分别是**瑞利散射**和**米氏散射**。\n\n - 瑞利散射是当光线通过介质中的微小颗粒或分子时发生的散射现象。由于颗粒或分子的尺寸远小于光的波长，不同波长的光线会以不同的角度散射，一个最经典的案例就是因为蓝色的波长是较短的，所以蓝色很容易发生瑞利散射，导致天空呈现蓝色。\n - 米氏散射则一般由空气中含有的较大颗粒的介质，如气溶胶、灰尘、水滴等引起。和瑞利散射不同的是，米氏散射和光的波长关系并不大。\n ![](https://www.alanzucconi.com/wp-content/uploads/2017/09/scattering_13.png)\n\n那么根据前面的模型思路，光在P点上发生了散射，其中一部分能量沿着PA方向进入相机的实现。获取这一部分能量的计算函数被称之为<strong>相位函数（Phase Function）</strong>。相位函数是用来描述，当光线发生散射的情况时，某个方向（一般是和原光线方向的夹角）占原光线的能量的比例。瑞利散射和米氏散射的相位函数是不同的，同一种散射的相位函数也会有不同的方法进行拟合，这里我不想去做过多的公式推导的工作，有兴趣的可以去查看[参考资料](https://patapom.com/topics/Revision2013/Revision%202013%20-%20Real-time%20Volumetric%20Rendering%20Course%20Notes.pdf)的推导过程。\n\n下方是瑞利散射的相位函数，theta是原光线方向和目标方向的夹角：\n<!-- wp:html -->\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n  <mtable displaystyle=\"true\" columnalign=\"right left right\" columnspacing=\"0em 2em\" rowspacing=\"3pt\">\n    <mtr>\n      <mtd>\n        <mi>P</mi>\n        <mo stretchy=\"false\">(</mo>\n        <mi>&#x3B8;</mi>\n        <mo stretchy=\"false\">)</mo>\n      </mtd>        \n      <mtd>\n        <mi></mi>\n        <mo>=</mo>\n        <mfrac>\n          <mn>3</mn>\n          <mrow>\n            <mn>16</mn>\n            <mi>&#x3C0;</mi>\n          </mrow>\n        </mfrac>\n        <mo stretchy=\"false\">(</mo>\n        <mn>1</mn>\n        <mo>+</mo>\n        <mi>c</mi>\n        <mi>o</mi>\n        <msup>\n          <mi>s</mi>\n          <mn>2</mn>\n        </msup>\n        <mi>&#x3B8;</mi>\n        <mo stretchy=\"false\">)</mo>\n      </mtd>\n    </mtr>\n  </mtable>\n</math>\n<!-- /wp:html -->\n\n<!-- wp:paragraph -->\n下方是米氏散射的相位函数，这里使用的是[Henyey-Greenstein函数](https://omlc.org/classroom/ece532/class3/hg.html)来近似。\n<!-- /wp:paragraph -->\n\n<!-- wp:html -->\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">  <mi>P</mi>  <mo stretchy=\"false\">(</mo>  <mi>&#x3B8;</mi>  <mo stretchy=\"false\">)</mo>  <mo>=</mo>  <mfrac>    <mrow>      <mn>1</mn>      <mo>&#x2212;</mo>      <msup>        <mi>g</mi>        <mrow>          <mn>2</mn>        </mrow>      </msup>    </mrow>    <mrow>      <mn>4</mn>      <mi>&#x3C0;</mi>      <mo stretchy=\"false\">(</mo>      <mn>1</mn>      <mo>+</mo>      <msup>        <mi>g</mi>        <mrow>          <mn>2</mn>        </mrow>      </msup>      <mo>&#x2212;</mo>      <mn>2</mn>      <mi>g</mi>      <mi>c</mi>      <mi>o</mi>      <mi>s</mi>      <mo stretchy=\"false\">(</mo>      <mi>&#x3B8;</mi>      <mo stretchy=\"false\">)</mo>      <msup>        <mo stretchy=\"false\">)</mo>        <mrow>          <mn>3</mn>          <mrow>            <mo>/</mo>          </mrow>          <mn>2</mn>        </mrow>      </msup>    </mrow>  </mfrac></math>\n<!-- /wp:html -->\n\n<!-- wp:paragraph -->\n<p>好了，现在我们知道了如何计算光线在P点的散射行为。剩下的工作就是需要计算光线在传播过程的损耗了。这一部分的计算方法称之为<strong>衰减系数</strong>，或者<strong>消光系数</strong>。消光系数中的一部分，需要对光线在传播的路线的长度和空气密度进行积分。在程序中的表示就是将光的传播路径分为小段，将每一小段的长度和平均密度（或者小段中点的介质密度）相乘而得到，这个结果我们称之为<strong>光学距离（Optical Depth）</strong>。</p>\n<!-- /wp:paragraph -->\n\n<!-- wp:paragraph -->\n<p>衰减系数的计算公式如下，红色部分代表的是在海拔为h的散射系数。他可以分解为在海平面上的散射系数乘以在海拔h的介质密度。</p>\n<!-- /wp:paragraph -->\n\n<!-- wp:html -->\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n  <mstyle mathcolor=\"#00AAFF\">\n    <mi>T</mi>\n    <mo stretchy=\"false\">(</mo>\n    <mi>P</mi>\n    <mi>A</mi>\n    <mo stretchy=\"false\">)</mo>\n  </mstyle>\n  <mo>=</mo>\n<mstyle mathcolor=\"#00AA00\">\n  <mi>exp</mi>\n</mstyle>\n  <mrow data-mjx-texclass=\"ORD\">\n    <mo>&#x2212;</mo>\n    <msubsup>\n      <mo data-mjx-texclass=\"OP\">&#x222B;</mo>\n      <mi>P</mi>\n      <mi>A</mi>\n    </msubsup>\n    <mrow data-mjx-texclass=\"ORD\">\n      <mstyle mathcolor=\"Red\">\n        <mi>&#x3B2;</mi>\n        <mo stretchy=\"false\">(</mo>\n        <mi>&#x3BB;</mi>\n        <mo>,</mo>\n        <mi>h</mi>\n        <mo stretchy=\"false\">)</mo>\n      </mstyle>\n    </mrow>\n    <mi>d</mi>\n    <mi>s</mi>\n  </mrow>\n</math>\n<!-- /wp:html -->\n\n<!-- wp:paragraph -->\n<p>分解出来后得到的结果如下，其中红色的部分代表的意思是在海平面的散射系数，是一个常量，黄色部分代表了在海拔为h的介质的密度。这个公式的积分便是针对AP路线上的介质密度，其结果也就是光学距离。</p>\n<!-- /wp:paragraph -->\n\n<!-- wp:html -->\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n  <mstyle mathcolor=\"#00AAFF\">\n    <mi>T</mi>\n    <mo stretchy=\"false\">(</mo>\n    <mi>P</mi>\n    <mi>A</mi>\n    <mo stretchy=\"false\">)</mo>\n  </mstyle>\n  <mo>=</mo>\n    <mstyle mathcolor=\"#00AA00\">\n      <mi>exp</mi>\n    </mstyle>\n  <mrow data-mjx-texclass=\"ORD\">\n    <mo>&#x2212;</mo>\n    <mstyle mathcolor=\"Red\">\n      <mi>&#x3B2;</mi>\n      <mo stretchy=\"false\">(</mo>\n      <mi>&#x3BB;</mi>\n      <mo stretchy=\"false\">)</mo>\n    </mstyle>\n    <msubsup>\n      <mo data-mjx-texclass=\"OP\">&#x222B;</mo>\n      <mi>P</mi>\n      <mi>A</mi>\n    </msubsup>\n    <mrow data-mjx-texclass=\"ORD\">\n      <mstyle mathcolor=\"Gold\">\n        <mi>&#x3C1;</mi>\n        <mo stretchy=\"false\">(</mo>\n        <mi>h</mi>\n        <mo stretchy=\"false\">)</mo>\n      </mstyle>\n    </mrow>\n    <mi>d</mi>\n    <mi>s</mi>\n  </mrow>\n</math>\n<!-- /wp:html -->\n\n<!-- wp:paragraph -->\n<p>介质密度的计算公式如下，其中H代表的是一个基准海拔，是一个常量。不同散射的基准海拔有所不同，瑞利散射的基准海拔是*8500*，而米氏散射的基准海拔是*1200*。</p>\n<!-- /wp:paragraph -->\n\n<!-- wp:html -->\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n  <mstyle mathcolor=\"Gold\">\n    <mi>&#x3C1;</mi>\n    <mo stretchy=\"false\">(</mo>\n    <mi>h</mi>\n    <mo stretchy=\"false\">)</mo>\n  </mstyle>\n  <mo>=</mo>\n  <mi>exp</mi>\n  <mo stretchy=\"false\">(</mo>\n  <mo>&#x2212;</mo>\n  <mfrac>\n    <mi>h</mi>\n    <mi>H</mi>\n  </mfrac>\n  <mo stretchy=\"false\">)</mo>\n</math>\n<!-- /wp:html -->\n\n自此，我们关于单次散射的天空大气渲染方法的基本预备知识已经了解的差不多了，接下来介绍shader的相关代码。\n\n\n# Shader代码\n首先，为了计算AB、PC等视线和光线在大气层内的传播距离，我们需要一个函数来计算射线和球体的相交情况。\n\n```glsl\nvec2 ray_sphere_intersection(vec3 ray_origin, vec3 ray_direction, vec3 sphere_center, float sphere_radius)\n{\n    // ray-sphere intersection that assumes\n    float a = dot(ray_direction, ray_direction);\n    vec3 oc = ray_origin - sphere_center;\n    float b = 2.0 * dot(ray_direction, oc);\n    float c = dot(oc, oc) - (sphere_radius * sphere_radius);\n    float d = (b*b) - 4.0*a*c;\n\n    // 返回击中结果，y小于x代表无结果\n    if (d < 0.0) return vec2(1e10,-1e10);\n    // 击中的话有两个相同或者不同的结果\n    return vec2(\n        (-b - sqrt(d))/(2.0*a),\n        (-b + sqrt(d))/(2.0*a)\n    );\n}\n```\n\n<!-- wp:paragraph -->\n<p>好了，那有了这个辅助公式，我们正式开始计算大气的颜色。首先，按照之前的理论，我们要计算视线向量在大气层内的长度，所以我们对射线方向和大气层，以及射线方向和星球表面做射线检测。得到视线在大气层内的长度后，根据我们预设的想要采样的步数（iSteps），计算出每次采样的长度ds。</p>\n<!-- /wp:paragraph -->\n\n```glsl\nray_dir = normalize(ray_dir);\n\n// 视线和大气层大小的尺寸的射线检测\n// x为大气入射点的距离、y为大气出射点的距离（x==y代表光线和大气球体相切，x>y代表光线不经过大气）\nvec2 atmos_hit = ray_sphere_intersection(ray_origin, ray_dir, rAtmos);\n// 未击中，返回0\nif (atmos_hit.x > atmos_hit.y) return vec3(0,0,0);\n\n    // 视线和星球做射线检测，取得近处的检测结果（远处的那个光被星球本体遮挡）\nvec2 planet_hit = ray_sphere_intersection(ray_origin, ray_dir, planet_radius);\nfloat light_distance = atmos_hit.y;\n\n// hit the planet\nif(planet_hit.x < planet_hit.y && planet_hit.x > 0.1)\n{\n    light_distance = planet_hit.x;\n}\n\n// light sample length\nfloat ds = light_distance / float(iSteps);\n```\n\n<!-- wp:paragraph -->\n<p>那么接下来，便是进入采样的循环。循环有两层，外面的循环是计算<strong>视线方向上的每个采样点的光能量</strong>，而计算这个点的光能量也需要一个循环，这个循环是用于采样该点和光源之间的连线在大气层内的线段（也就是之前图片的PC段）的光学距离。根据前面提到的公式，光学距离乘以海平面的散射系数便可以得到光的衰减系数，因此jSteps循环可以得到光线在进入大气层后，传播到达视线上的采样点（P0、P1...Pn点）的衰减所剩下的能量。</p>\n<!-- /wp:paragraph -->\n\n``` glsl\n// Initialize the primary ray time.\nfloat iTime = 0.0;\n\n// Initialize accumulators for Rayleigh and Mie scattering.\nvec3 total_scatter_rlh = vec3(0,0,0);\nvec3 total_scatter_mie = vec3(0,0,0);\n\n// Initialize optical depth accumulators for the primary ray.\nfloat total_od_rlh = 0.0;\nfloat total_od_mie = 0.0;\n\n// 对每个视线上的采样点循环\nfor (int i = 0; i < iSteps; i++) {\n    // 获取到采样点的位置\n    vec3 iPos = ray_origin + ray_dir * (iTime + ds * 0.5);\n\n    // 在当前点向太阳的位置做射线检测，以大气的半径为球体。.y是代表大气的出射点，j_steps代表采样数\n    float jStepSize = ray_sphere_intersection(iPos, pSun, rAtmos).y / float(jSteps);\n\n    float jTime = 0.0;\n    float jOdRlh = 0.0;\n    float jOdMie = 0.0;\n\n    // 在当前采样到大气入射点的距离上，采样计算\n    for (int j = 0; j < jSteps; j++) {\n        // 计算采样点到光源的衰减\n        vec3 jPos = iPos + pSun * (jTime + jStepSize * 0.5);\n\n        float jHeight = length(jPos-planet_center) - planet_radius;\n\n        // Accumulate the optical depth.\n        jOdRlh += get_atmos_density(jHeight, scale_height_rlh) * jStepSize;\n        jOdMie += get_atmos_density(jHeight, scale_height_mie) * jStepSize;\n\n        // Increment the secondary ray time.\n        jTime += jStepSize;\n    }\n\n    // 观察点和星球表面距离\n    float surface_height = length(iPos-planet_center) - planet_radius;\n\n    // 计算这一步的散射的光学深度结果\n    float od_step_rlh = get_atmos_density(surface_height, scale_height_rlh) * ds;\n    float od_step_mie = get_atmos_density(surface_height, scale_height_mie) * ds;\n    \n    total_od_rlh += od_step_rlh;\n    total_od_mie += od_step_mie;\n\n    // 计算衰减系数，光在经过一定距离后衰减剩下来的比例。\n    vec3 attn = exp(-(kMie * (total_od_mie + jOdMie) + kRlh * (total_od_rlh + jOdRlh)));\n\n    // Accumulate scattering.\n    total_scatter_rlh += od_step_rlh * attn;\n    total_scatter_mie += od_step_mie * attn;\n\n    // Increment the primary ray time.\n    iTime += ds;\n}\n```\n\n<!-- wp:paragraph -->\n<p>计算瑞利散射和米氏散射的大气密度的代码如下所示，其中scale_height在计算瑞利散射的时候带入的是8500，米氏散射则带入1200。</p>\n<!-- /wp:paragraph -->\n\n```glsl\n// 获取大气密度\n// 传入位置离海平面的高度，以及散射的相关基准高度\n// 大气中任意一点的散射系数的计算，简化拆解为散射在海平面的散射系数，乘以基于海平面高度的该散射的大气密度计算公式\nfloat get_atmos_density(float height_to_sea_level, float scale_height)\n{\n    return exp(-height_to_sea_level / scale_height);\n}\n```\n\n<!-- wp:paragraph -->\n<p>将iSteps循环的结果累加起来，我们便得到了视线上（AB）的每个采样点的光能量传播到达相机（A点）经过衰减的能量的和。那么还剩一个部分，就是光在P点进行散射的时候，光也损失掉了一部分能量，这个过程可以通过乘以米氏散射和瑞利散射的相位函数来计算。因为每个采样点P的结果都需要乘以相位函数，所以我们可以将它提取到最外面，乘以光的能量的总和即可。</p>\n<!-- /wp:paragraph -->\n\n<!-- wp:paragraph -->\n<p>不过需要注意的是，前面的公式提到了，相位函数是和光线入射方向和目标方向的夹角相关的，所以如果我们<strong>默认太阳光源是平行光</strong>的话，这个夹角对每个采样点来说是一样的，所以可以将相位函数提取到最外层。如果认为这个夹角对每个采样点不一样的话，那也许还是应该老老实实在计算每个采样点的光的时候单独去处理。</p>\n<!-- /wp:paragraph -->\n\n最终的结果除了乘以相位函数，还需要乘以海平面的散射系数，结果如下。</p>\n\n```glsl\n// 计算并返回最终颜色\n// iSun是光源（太阳）的颜色\nreturn iSun * (pRlh * kRlh * total_scatter_rlh + pMie * kMie * total_scatter_mie);\n```\n下面是得到的结果：\n\n<iframe src=\"single-scatter-atmosphere.mp4\" scrolling=\"no\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\"> </iframe>\n\n\n# 参考资料\n- https://www.alanzucconi.com/2017/10/10/atmospheric-scattering-1/\n- https://www.xianlongok.site/post/8e5d3b12/\n","slug":"single-scatter-atmosphere","published":1,"updated":"2024-10-15T14:31:41.794Z","comments":1,"layout":"post","photos":[],"_id":"cm5ht44vr002h34576ucy7xvt","content":"<p>最近KongEngine实现了IBL(Image Based Lighting)，可以将HDR环境贴图映射作为3D场景的环境。</p>\n<p><img src=\"/2024/10/15/single-scatter-atmosphere/kong-screen-shot.png\" alt=\"KongEngine的IBL效果\"></p>\n<p>在实现了IBL之后我又产生了一个想法，能否实现类似UE中的大气环境的渲染效果呢？我尝试去寻找答案，发现如果要完全复刻UE中的效果确实需要一定的功夫的，但是最基础的天空大气渲染并没有想象中那么复杂，于是我便花了点时间在KongEngine中实现了这个功能。</p>\n<p><img src=\"/2024/10/15/single-scatter-atmosphere/single-scatter-atmosphere.png\" alt=\"KongEngine天空大气效果\"></p>\n<p>我打算将这个方法的基础思想和实现在此简单记录一下。</p>\n<h1 id=\"单次散射模型\"><a href=\"#单次散射模型\" class=\"headerlink\" title=\"单次散射模型\"></a>单次散射模型</h1><p>星球的大气层是一种参与性介质，和在真空环境不同，光在大气层中传播的时候会因为大气中的微小颗粒（水、灰尘等等）发生散射（折射、反射）和吸收等情况。因此我们看向空中的一个点的时候，这个点的颜色是光经过多次散射得到的结果。</p>\n<p>光到达我们眼睛之前经过多少次反射和折射是不一定的，在实时渲染的需求下计算太多次光的变化显然也是不显示的。最简单的方法，是使用单次散射模型：我们假定光在进入我们眼睛之前，有且只发生了一次散射。光一般在第一次散射的时候，还会有最多的能量剩余，后面的多次散射能力相对少，对最后效果的呈现也影响不大，因此这种模型可以在保证性能的情况下，还有不错的效果。</p>\n<p><img src=\"https://www.alanzucconi.com/wp-content/uploads/2017/09/scattering_02.png\"></p>\n<p>除了散射，光在传播的时候会被大气吸收。一般来说，如果在一个均匀的介质中传播的话，光的被吸收的部分和介质的密度，以及传播的路径长度正相关。放到单散射模型的例子中，就是需要计算光在散射前和散射后的路径上，被吸收了多少能量。<br><img src=\"https://www.alanzucconi.com/wp-content/uploads/2017/09/scattering_07.png\"></p>\n<p>按照上面的图，假设相机在A点，观察方向为AB，其中有一束光在P点发生散射后沿着PA进入相机。如果我们知道了光线损耗和距离相关的公式，那么似乎是只要计算出CP和PA的长度，在带入公式后就可以得到光该路径上传播后的最终能量了。</p>\n<p><img src=\"https://www.alanzucconi.com/wp-content/uploads/2017/09/scattering_10a.png\"></p>\n<p>但是对于大气层来说，它的密度并不是均匀的，而是和大气层距离地面的高度相关。所以在计算CP段光线损耗的时候，我们需要将这段距离分为多个小段，每个小段的损耗公式带入对应平均高度，在最后将所有结果相加才。小段划分的越密集，则结果越准确。</p>\n<p>如果我们还要考虑大气中的其他因素，比如说大气中的云层，在这些区域中光的损耗就不仅仅是和高度相关了。</p>\n<p>另外一点就是，当我们考虑AB方向的光线时，它的最终效果，是在大气层内的所有AB连线上面的点的散射结果的总和（P0、P1、P2…Pn），因此我们也同样的需要对AB线段进行分段采样并将结果叠加。AB线段上的每一个采样结果按照上面所描述的计算单一P点的方式。</p>\n<p><img src=\"https://www.alanzucconi.com/wp-content/uploads/2017/09/scattering_08a.png\"></p>\n<p>以上便是大气单次散射模型的基本思路。</p>\n<h1 id=\"散射的计算\"><a href=\"#散射的计算\" class=\"headerlink\" title=\"散射的计算\"></a>散射的计算</h1><p>之前我们提到了计算光的散射，这里我们介绍一下散射的相关计算。会提及一些公式，但是不会做过多的数学推导工作，对推导过程感兴趣的可以看一下文章中的参考资料。</p>\n<p>一般来说，天空大气的散射主要包括两种，分别是<strong>瑞利散射</strong>和<strong>米氏散射</strong>。</p>\n<ul>\n<li>瑞利散射是当光线通过介质中的微小颗粒或分子时发生的散射现象。由于颗粒或分子的尺寸远小于光的波长，不同波长的光线会以不同的角度散射，一个最经典的案例就是因为蓝色的波长是较短的，所以蓝色很容易发生瑞利散射，导致天空呈现蓝色。</li>\n<li>米氏散射则一般由空气中含有的较大颗粒的介质，如气溶胶、灰尘、水滴等引起。和瑞利散射不同的是，米氏散射和光的波长关系并不大。<br> <img src=\"https://www.alanzucconi.com/wp-content/uploads/2017/09/scattering_13.png\"></li>\n</ul>\n<p>那么根据前面的模型思路，光在P点上发生了散射，其中一部分能量沿着PA方向进入相机的实现。获取这一部分能量的计算函数被称之为<strong>相位函数（Phase Function）</strong>。相位函数是用来描述，当光线发生散射的情况时，某个方向（一般是和原光线方向的夹角）占原光线的能量的比例。瑞利散射和米氏散射的相位函数是不同的，同一种散射的相位函数也会有不同的方法进行拟合，这里我不想去做过多的公式推导的工作，有兴趣的可以去查看<a href=\"https://patapom.com/topics/Revision2013/Revision%202013%20-%20Real-time%20Volumetric%20Rendering%20Course%20Notes.pdf\">参考资料</a>的推导过程。</p>\n<p>下方是瑞利散射的相位函数，theta是原光线方向和目标方向的夹角：</p>\n<!-- wp:html -->\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n  <mtable displaystyle=\"true\" columnalign=\"right left right\" columnspacing=\"0em 2em\" rowspacing=\"3pt\">\n    <mtr>\n      <mtd>\n        <mi>P</mi>\n        <mo stretchy=\"false\">(</mo>\n        <mi>&#x3B8;</mi>\n        <mo stretchy=\"false\">)</mo>\n      </mtd>        \n      <mtd>\n        <mi></mi>\n        <mo>=</mo>\n        <mfrac>\n          <mn>3</mn>\n          <mrow>\n            <mn>16</mn>\n            <mi>&#x3C0;</mi>\n          </mrow>\n        </mfrac>\n        <mo stretchy=\"false\">(</mo>\n        <mn>1</mn>\n        <mo>+</mo>\n        <mi>c</mi>\n        <mi>o</mi>\n        <msup>\n          <mi>s</mi>\n          <mn>2</mn>\n        </msup>\n        <mi>&#x3B8;</mi>\n        <mo stretchy=\"false\">)</mo>\n      </mtd>\n    </mtr>\n  </mtable>\n</math>\n<!-- /wp:html -->\n\n<!-- wp:paragraph -->\n<p>下方是米氏散射的相位函数，这里使用的是<a href=\"https://omlc.org/classroom/ece532/class3/hg.html\">Henyey-Greenstein函数</a>来近似。</p>\n<!-- /wp:paragraph -->\n\n<!-- wp:html -->\n<p><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">  <mi>P</mi>  <mo stretchy=\"false\">(</mo>  <mi>&#x3B8;</mi>  <mo stretchy=\"false\">)</mo>  <mo>&#x3D;</mo>  <mfrac>    <mrow>      <mn>1</mn>      <mo>&#x2212;</mo>      <msup>        <mi>g</mi>        <mrow>          <mn>2</mn>        </mrow>      </msup>    </mrow>    <mrow>      <mn>4</mn>      <mi>&#x3C0;</mi>      <mo stretchy=\"false\">(</mo>      <mn>1</mn>      <mo>+</mo>      <msup>        <mi>g</mi>        <mrow>          <mn>2</mn>        </mrow>      </msup>      <mo>&#x2212;</mo>      <mn>2</mn>      <mi>g</mi>      <mi>c</mi>      <mi>o</mi>      <mi>s</mi>      <mo stretchy=\"false\">(</mo>      <mi>&#x3B8;</mi>      <mo stretchy=\"false\">)</mo>      <msup>        <mo stretchy=\"false\">)</mo>        <mrow>          <mn>3</mn>          <mrow>            <mo>&#x2F;</mo>          </mrow>          <mn>2</mn>        </mrow>      </msup>    </mrow>  </mfrac></math></p>\n<!-- /wp:html -->\n\n<!-- wp:paragraph -->\n<p>好了，现在我们知道了如何计算光线在P点的散射行为。剩下的工作就是需要计算光线在传播过程的损耗了。这一部分的计算方法称之为<strong>衰减系数</strong>，或者<strong>消光系数</strong>。消光系数中的一部分，需要对光线在传播的路线的长度和空气密度进行积分。在程序中的表示就是将光的传播路径分为小段，将每一小段的长度和平均密度（或者小段中点的介质密度）相乘而得到，这个结果我们称之为<strong>光学距离（Optical Depth）</strong>。</p>\n<!-- /wp:paragraph -->\n\n<!-- wp:paragraph -->\n<p>衰减系数的计算公式如下，红色部分代表的是在海拔为h的散射系数。他可以分解为在海平面上的散射系数乘以在海拔h的介质密度。</p>\n<!-- /wp:paragraph -->\n\n<!-- wp:html -->\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n  <mstyle mathcolor=\"#00AAFF\">\n    <mi>T</mi>\n    <mo stretchy=\"false\">(</mo>\n    <mi>P</mi>\n    <mi>A</mi>\n    <mo stretchy=\"false\">)</mo>\n  </mstyle>\n  <mo>=</mo>\n<mstyle mathcolor=\"#00AA00\">\n  <mi>exp</mi>\n</mstyle>\n  <mrow data-mjx-texclass=\"ORD\">\n    <mo>&#x2212;</mo>\n    <msubsup>\n      <mo data-mjx-texclass=\"OP\">&#x222B;</mo>\n      <mi>P</mi>\n      <mi>A</mi>\n    </msubsup>\n    <mrow data-mjx-texclass=\"ORD\">\n      <mstyle mathcolor=\"Red\">\n        <mi>&#x3B2;</mi>\n        <mo stretchy=\"false\">(</mo>\n        <mi>&#x3BB;</mi>\n        <mo>,</mo>\n        <mi>h</mi>\n        <mo stretchy=\"false\">)</mo>\n      </mstyle>\n    </mrow>\n    <mi>d</mi>\n    <mi>s</mi>\n  </mrow>\n</math>\n<!-- /wp:html -->\n\n<!-- wp:paragraph -->\n<p>分解出来后得到的结果如下，其中红色的部分代表的意思是在海平面的散射系数，是一个常量，黄色部分代表了在海拔为h的介质的密度。这个公式的积分便是针对AP路线上的介质密度，其结果也就是光学距离。</p>\n<!-- /wp:paragraph -->\n\n<!-- wp:html -->\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n  <mstyle mathcolor=\"#00AAFF\">\n    <mi>T</mi>\n    <mo stretchy=\"false\">(</mo>\n    <mi>P</mi>\n    <mi>A</mi>\n    <mo stretchy=\"false\">)</mo>\n  </mstyle>\n  <mo>=</mo>\n    <mstyle mathcolor=\"#00AA00\">\n      <mi>exp</mi>\n    </mstyle>\n  <mrow data-mjx-texclass=\"ORD\">\n    <mo>&#x2212;</mo>\n    <mstyle mathcolor=\"Red\">\n      <mi>&#x3B2;</mi>\n      <mo stretchy=\"false\">(</mo>\n      <mi>&#x3BB;</mi>\n      <mo stretchy=\"false\">)</mo>\n    </mstyle>\n    <msubsup>\n      <mo data-mjx-texclass=\"OP\">&#x222B;</mo>\n      <mi>P</mi>\n      <mi>A</mi>\n    </msubsup>\n    <mrow data-mjx-texclass=\"ORD\">\n      <mstyle mathcolor=\"Gold\">\n        <mi>&#x3C1;</mi>\n        <mo stretchy=\"false\">(</mo>\n        <mi>h</mi>\n        <mo stretchy=\"false\">)</mo>\n      </mstyle>\n    </mrow>\n    <mi>d</mi>\n    <mi>s</mi>\n  </mrow>\n</math>\n<!-- /wp:html -->\n\n<!-- wp:paragraph -->\n<p>介质密度的计算公式如下，其中H代表的是一个基准海拔，是一个常量。不同散射的基准海拔有所不同，瑞利散射的基准海拔是*8500*，而米氏散射的基准海拔是*1200*。</p>\n<!-- /wp:paragraph -->\n\n<!-- wp:html -->\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n  <mstyle mathcolor=\"Gold\">\n    <mi>&#x3C1;</mi>\n    <mo stretchy=\"false\">(</mo>\n    <mi>h</mi>\n    <mo stretchy=\"false\">)</mo>\n  </mstyle>\n  <mo>=</mo>\n  <mi>exp</mi>\n  <mo stretchy=\"false\">(</mo>\n  <mo>&#x2212;</mo>\n  <mfrac>\n    <mi>h</mi>\n    <mi>H</mi>\n  </mfrac>\n  <mo stretchy=\"false\">)</mo>\n</math>\n<!-- /wp:html -->\n\n<p>自此，我们关于单次散射的天空大气渲染方法的基本预备知识已经了解的差不多了，接下来介绍shader的相关代码。</p>\n<h1 id=\"Shader代码\"><a href=\"#Shader代码\" class=\"headerlink\" title=\"Shader代码\"></a>Shader代码</h1><p>首先，为了计算AB、PC等视线和光线在大气层内的传播距离，我们需要一个函数来计算射线和球体的相交情况。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-type\">vec2</span> ray_sphere_intersection(<span class=\"hljs-type\">vec3</span> ray_origin, <span class=\"hljs-type\">vec3</span> ray_direction, <span class=\"hljs-type\">vec3</span> sphere_center, <span class=\"hljs-type\">float</span> sphere_radius)<br>&#123;<br>    <span class=\"hljs-comment\">// ray-sphere intersection that assumes</span><br>    <span class=\"hljs-type\">float</span> a = <span class=\"hljs-built_in\">dot</span>(ray_direction, ray_direction);<br>    <span class=\"hljs-type\">vec3</span> oc = ray_origin - sphere_center;<br>    <span class=\"hljs-type\">float</span> b = <span class=\"hljs-number\">2.0</span> * <span class=\"hljs-built_in\">dot</span>(ray_direction, oc);<br>    <span class=\"hljs-type\">float</span> c = <span class=\"hljs-built_in\">dot</span>(oc, oc) - (sphere_radius * sphere_radius);<br>    <span class=\"hljs-type\">float</span> d = (b*b) - <span class=\"hljs-number\">4.0</span>*a*c;<br><br>    <span class=\"hljs-comment\">// 返回击中结果，y小于x代表无结果</span><br>    <span class=\"hljs-keyword\">if</span> (d &lt; <span class=\"hljs-number\">0.0</span>) <span class=\"hljs-keyword\">return</span> <span class=\"hljs-type\">vec2</span>(<span class=\"hljs-number\">1e10</span>,<span class=\"hljs-number\">-1e10</span>);<br>    <span class=\"hljs-comment\">// 击中的话有两个相同或者不同的结果</span><br>    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-type\">vec2</span>(<br>        (-b - <span class=\"hljs-built_in\">sqrt</span>(d))/(<span class=\"hljs-number\">2.0</span>*a),<br>        (-b + <span class=\"hljs-built_in\">sqrt</span>(d))/(<span class=\"hljs-number\">2.0</span>*a)<br>    );<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<!-- wp:paragraph -->\n<p>好了，那有了这个辅助公式，我们正式开始计算大气的颜色。首先，按照之前的理论，我们要计算视线向量在大气层内的长度，所以我们对射线方向和大气层，以及射线方向和星球表面做射线检测。得到视线在大气层内的长度后，根据我们预设的想要采样的步数（iSteps），计算出每次采样的长度ds。</p>\n<!-- /wp:paragraph -->\n\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\">ray_dir = <span class=\"hljs-built_in\">normalize</span>(ray_dir);<br><br><span class=\"hljs-comment\">// 视线和大气层大小的尺寸的射线检测</span><br><span class=\"hljs-comment\">// x为大气入射点的距离、y为大气出射点的距离（x==y代表光线和大气球体相切，x&gt;y代表光线不经过大气）</span><br><span class=\"hljs-type\">vec2</span> atmos_hit = ray_sphere_intersection(ray_origin, ray_dir, rAtmos);<br><span class=\"hljs-comment\">// 未击中，返回0</span><br><span class=\"hljs-keyword\">if</span> (atmos_hit.x &gt; atmos_hit.y) <span class=\"hljs-keyword\">return</span> <span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">0</span>,<span class=\"hljs-number\">0</span>,<span class=\"hljs-number\">0</span>);<br><br>    <span class=\"hljs-comment\">// 视线和星球做射线检测，取得近处的检测结果（远处的那个光被星球本体遮挡）</span><br><span class=\"hljs-type\">vec2</span> planet_hit = ray_sphere_intersection(ray_origin, ray_dir, planet_radius);<br><span class=\"hljs-type\">float</span> light_distance = atmos_hit.y;<br><br><span class=\"hljs-comment\">// hit the planet</span><br><span class=\"hljs-keyword\">if</span>(planet_hit.x &lt; planet_hit.y &amp;&amp; planet_hit.x &gt; <span class=\"hljs-number\">0.1</span>)<br>&#123;<br>    light_distance = planet_hit.x;<br>&#125;<br><br><span class=\"hljs-comment\">// light sample length</span><br><span class=\"hljs-type\">float</span> ds = light_distance / <span class=\"hljs-type\">float</span>(iSteps);<br></code></pre></td></tr></table></figure>\n\n<!-- wp:paragraph -->\n<p>那么接下来，便是进入采样的循环。循环有两层，外面的循环是计算<strong>视线方向上的每个采样点的光能量</strong>，而计算这个点的光能量也需要一个循环，这个循环是用于采样该点和光源之间的连线在大气层内的线段（也就是之前图片的PC段）的光学距离。根据前面提到的公式，光学距离乘以海平面的散射系数便可以得到光的衰减系数，因此jSteps循环可以得到光线在进入大气层后，传播到达视线上的采样点（P0、P1...Pn点）的衰减所剩下的能量。</p>\n<!-- /wp:paragraph -->\n\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-comment\">// Initialize the primary ray time.</span><br><span class=\"hljs-type\">float</span> iTime = <span class=\"hljs-number\">0.0</span>;<br><br><span class=\"hljs-comment\">// Initialize accumulators for Rayleigh and Mie scattering.</span><br><span class=\"hljs-type\">vec3</span> total_scatter_rlh = <span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">0</span>,<span class=\"hljs-number\">0</span>,<span class=\"hljs-number\">0</span>);<br><span class=\"hljs-type\">vec3</span> total_scatter_mie = <span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">0</span>,<span class=\"hljs-number\">0</span>,<span class=\"hljs-number\">0</span>);<br><br><span class=\"hljs-comment\">// Initialize optical depth accumulators for the primary ray.</span><br><span class=\"hljs-type\">float</span> total_od_rlh = <span class=\"hljs-number\">0.0</span>;<br><span class=\"hljs-type\">float</span> total_od_mie = <span class=\"hljs-number\">0.0</span>;<br><br><span class=\"hljs-comment\">// 对每个视线上的采样点循环</span><br><span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> i = <span class=\"hljs-number\">0</span>; i &lt; iSteps; i++) &#123;<br>    <span class=\"hljs-comment\">// 获取到采样点的位置</span><br>    <span class=\"hljs-type\">vec3</span> iPos = ray_origin + ray_dir * (iTime + ds * <span class=\"hljs-number\">0.5</span>);<br><br>    <span class=\"hljs-comment\">// 在当前点向太阳的位置做射线检测，以大气的半径为球体。.y是代表大气的出射点，j_steps代表采样数</span><br>    <span class=\"hljs-type\">float</span> jStepSize = ray_sphere_intersection(iPos, pSun, rAtmos).y / <span class=\"hljs-type\">float</span>(jSteps);<br><br>    <span class=\"hljs-type\">float</span> jTime = <span class=\"hljs-number\">0.0</span>;<br>    <span class=\"hljs-type\">float</span> jOdRlh = <span class=\"hljs-number\">0.0</span>;<br>    <span class=\"hljs-type\">float</span> jOdMie = <span class=\"hljs-number\">0.0</span>;<br><br>    <span class=\"hljs-comment\">// 在当前采样到大气入射点的距离上，采样计算</span><br>    <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> j = <span class=\"hljs-number\">0</span>; j &lt; jSteps; j++) &#123;<br>        <span class=\"hljs-comment\">// 计算采样点到光源的衰减</span><br>        <span class=\"hljs-type\">vec3</span> jPos = iPos + pSun * (jTime + jStepSize * <span class=\"hljs-number\">0.5</span>);<br><br>        <span class=\"hljs-type\">float</span> jHeight = <span class=\"hljs-built_in\">length</span>(jPos-planet_center) - planet_radius;<br><br>        <span class=\"hljs-comment\">// Accumulate the optical depth.</span><br>        jOdRlh += get_atmos_density(jHeight, scale_height_rlh) * jStepSize;<br>        jOdMie += get_atmos_density(jHeight, scale_height_mie) * jStepSize;<br><br>        <span class=\"hljs-comment\">// Increment the secondary ray time.</span><br>        jTime += jStepSize;<br>    &#125;<br><br>    <span class=\"hljs-comment\">// 观察点和星球表面距离</span><br>    <span class=\"hljs-type\">float</span> surface_height = <span class=\"hljs-built_in\">length</span>(iPos-planet_center) - planet_radius;<br><br>    <span class=\"hljs-comment\">// 计算这一步的散射的光学深度结果</span><br>    <span class=\"hljs-type\">float</span> od_step_rlh = get_atmos_density(surface_height, scale_height_rlh) * ds;<br>    <span class=\"hljs-type\">float</span> od_step_mie = get_atmos_density(surface_height, scale_height_mie) * ds;<br>    <br>    total_od_rlh += od_step_rlh;<br>    total_od_mie += od_step_mie;<br><br>    <span class=\"hljs-comment\">// 计算衰减系数，光在经过一定距离后衰减剩下来的比例。</span><br>    <span class=\"hljs-type\">vec3</span> attn = <span class=\"hljs-built_in\">exp</span>(-(kMie * (total_od_mie + jOdMie) + kRlh * (total_od_rlh + jOdRlh)));<br><br>    <span class=\"hljs-comment\">// Accumulate scattering.</span><br>    total_scatter_rlh += od_step_rlh * attn;<br>    total_scatter_mie += od_step_mie * attn;<br><br>    <span class=\"hljs-comment\">// Increment the primary ray time.</span><br>    iTime += ds;<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<!-- wp:paragraph -->\n<p>计算瑞利散射和米氏散射的大气密度的代码如下所示，其中scale_height在计算瑞利散射的时候带入的是8500，米氏散射则带入1200。</p>\n<!-- /wp:paragraph -->\n\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-comment\">// 获取大气密度</span><br><span class=\"hljs-comment\">// 传入位置离海平面的高度，以及散射的相关基准高度</span><br><span class=\"hljs-comment\">// 大气中任意一点的散射系数的计算，简化拆解为散射在海平面的散射系数，乘以基于海平面高度的该散射的大气密度计算公式</span><br><span class=\"hljs-type\">float</span> get_atmos_density(<span class=\"hljs-type\">float</span> height_to_sea_level, <span class=\"hljs-type\">float</span> scale_height)<br>&#123;<br>    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-built_in\">exp</span>(-height_to_sea_level / scale_height);<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<!-- wp:paragraph -->\n<p>将iSteps循环的结果累加起来，我们便得到了视线上（AB）的每个采样点的光能量传播到达相机（A点）经过衰减的能量的和。那么还剩一个部分，就是光在P点进行散射的时候，光也损失掉了一部分能量，这个过程可以通过乘以米氏散射和瑞利散射的相位函数来计算。因为每个采样点P的结果都需要乘以相位函数，所以我们可以将它提取到最外面，乘以光的能量的总和即可。</p>\n<!-- /wp:paragraph -->\n\n<!-- wp:paragraph -->\n<p>不过需要注意的是，前面的公式提到了，相位函数是和光线入射方向和目标方向的夹角相关的，所以如果我们<strong>默认太阳光源是平行光</strong>的话，这个夹角对每个采样点来说是一样的，所以可以将相位函数提取到最外层。如果认为这个夹角对每个采样点不一样的话，那也许还是应该老老实实在计算每个采样点的光的时候单独去处理。</p>\n<!-- /wp:paragraph -->\n\n<p>最终的结果除了乘以相位函数，还需要乘以海平面的散射系数，结果如下。</p></p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-comment\">// 计算并返回最终颜色</span><br><span class=\"hljs-comment\">// iSun是光源（太阳）的颜色</span><br><span class=\"hljs-keyword\">return</span> iSun * (pRlh * kRlh * total_scatter_rlh + pMie * kMie * total_scatter_mie);<br></code></pre></td></tr></table></figure>\n<p>下面是得到的结果：</p>\n<iframe src=\"single-scatter-atmosphere.mp4\" scrolling=\"no\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\"> </iframe>\n\n\n<h1 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h1><ul>\n<li><a href=\"https://www.alanzucconi.com/2017/10/10/atmospheric-scattering-1/\">https://www.alanzucconi.com/2017/10/10/atmospheric-scattering-1/</a></li>\n<li><a href=\"https://www.xianlongok.site/post/8e5d3b12/\">https://www.xianlongok.site/post/8e5d3b12/</a></li>\n</ul>\n","excerpt":"","more":"<p>最近KongEngine实现了IBL(Image Based Lighting)，可以将HDR环境贴图映射作为3D场景的环境。</p>\n<p><img src=\"/2024/10/15/single-scatter-atmosphere/kong-screen-shot.png\" alt=\"KongEngine的IBL效果\"></p>\n<p>在实现了IBL之后我又产生了一个想法，能否实现类似UE中的大气环境的渲染效果呢？我尝试去寻找答案，发现如果要完全复刻UE中的效果确实需要一定的功夫的，但是最基础的天空大气渲染并没有想象中那么复杂，于是我便花了点时间在KongEngine中实现了这个功能。</p>\n<p><img src=\"/2024/10/15/single-scatter-atmosphere/single-scatter-atmosphere.png\" alt=\"KongEngine天空大气效果\"></p>\n<p>我打算将这个方法的基础思想和实现在此简单记录一下。</p>\n<h1 id=\"单次散射模型\"><a href=\"#单次散射模型\" class=\"headerlink\" title=\"单次散射模型\"></a>单次散射模型</h1><p>星球的大气层是一种参与性介质，和在真空环境不同，光在大气层中传播的时候会因为大气中的微小颗粒（水、灰尘等等）发生散射（折射、反射）和吸收等情况。因此我们看向空中的一个点的时候，这个点的颜色是光经过多次散射得到的结果。</p>\n<p>光到达我们眼睛之前经过多少次反射和折射是不一定的，在实时渲染的需求下计算太多次光的变化显然也是不显示的。最简单的方法，是使用单次散射模型：我们假定光在进入我们眼睛之前，有且只发生了一次散射。光一般在第一次散射的时候，还会有最多的能量剩余，后面的多次散射能力相对少，对最后效果的呈现也影响不大，因此这种模型可以在保证性能的情况下，还有不错的效果。</p>\n<p><img src=\"https://www.alanzucconi.com/wp-content/uploads/2017/09/scattering_02.png\"></p>\n<p>除了散射，光在传播的时候会被大气吸收。一般来说，如果在一个均匀的介质中传播的话，光的被吸收的部分和介质的密度，以及传播的路径长度正相关。放到单散射模型的例子中，就是需要计算光在散射前和散射后的路径上，被吸收了多少能量。<br><img src=\"https://www.alanzucconi.com/wp-content/uploads/2017/09/scattering_07.png\"></p>\n<p>按照上面的图，假设相机在A点，观察方向为AB，其中有一束光在P点发生散射后沿着PA进入相机。如果我们知道了光线损耗和距离相关的公式，那么似乎是只要计算出CP和PA的长度，在带入公式后就可以得到光该路径上传播后的最终能量了。</p>\n<p><img src=\"https://www.alanzucconi.com/wp-content/uploads/2017/09/scattering_10a.png\"></p>\n<p>但是对于大气层来说，它的密度并不是均匀的，而是和大气层距离地面的高度相关。所以在计算CP段光线损耗的时候，我们需要将这段距离分为多个小段，每个小段的损耗公式带入对应平均高度，在最后将所有结果相加才。小段划分的越密集，则结果越准确。</p>\n<p>如果我们还要考虑大气中的其他因素，比如说大气中的云层，在这些区域中光的损耗就不仅仅是和高度相关了。</p>\n<p>另外一点就是，当我们考虑AB方向的光线时，它的最终效果，是在大气层内的所有AB连线上面的点的散射结果的总和（P0、P1、P2…Pn），因此我们也同样的需要对AB线段进行分段采样并将结果叠加。AB线段上的每一个采样结果按照上面所描述的计算单一P点的方式。</p>\n<p><img src=\"https://www.alanzucconi.com/wp-content/uploads/2017/09/scattering_08a.png\"></p>\n<p>以上便是大气单次散射模型的基本思路。</p>\n<h1 id=\"散射的计算\"><a href=\"#散射的计算\" class=\"headerlink\" title=\"散射的计算\"></a>散射的计算</h1><p>之前我们提到了计算光的散射，这里我们介绍一下散射的相关计算。会提及一些公式，但是不会做过多的数学推导工作，对推导过程感兴趣的可以看一下文章中的参考资料。</p>\n<p>一般来说，天空大气的散射主要包括两种，分别是<strong>瑞利散射</strong>和<strong>米氏散射</strong>。</p>\n<ul>\n<li>瑞利散射是当光线通过介质中的微小颗粒或分子时发生的散射现象。由于颗粒或分子的尺寸远小于光的波长，不同波长的光线会以不同的角度散射，一个最经典的案例就是因为蓝色的波长是较短的，所以蓝色很容易发生瑞利散射，导致天空呈现蓝色。</li>\n<li>米氏散射则一般由空气中含有的较大颗粒的介质，如气溶胶、灰尘、水滴等引起。和瑞利散射不同的是，米氏散射和光的波长关系并不大。<br> <img src=\"https://www.alanzucconi.com/wp-content/uploads/2017/09/scattering_13.png\"></li>\n</ul>\n<p>那么根据前面的模型思路，光在P点上发生了散射，其中一部分能量沿着PA方向进入相机的实现。获取这一部分能量的计算函数被称之为<strong>相位函数（Phase Function）</strong>。相位函数是用来描述，当光线发生散射的情况时，某个方向（一般是和原光线方向的夹角）占原光线的能量的比例。瑞利散射和米氏散射的相位函数是不同的，同一种散射的相位函数也会有不同的方法进行拟合，这里我不想去做过多的公式推导的工作，有兴趣的可以去查看<a href=\"https://patapom.com/topics/Revision2013/Revision%202013%20-%20Real-time%20Volumetric%20Rendering%20Course%20Notes.pdf\">参考资料</a>的推导过程。</p>\n<p>下方是瑞利散射的相位函数，theta是原光线方向和目标方向的夹角：</p>\n<!-- wp:html -->\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n  <mtable displaystyle=\"true\" columnalign=\"right left right\" columnspacing=\"0em 2em\" rowspacing=\"3pt\">\n    <mtr>\n      <mtd>\n        <mi>P</mi>\n        <mo stretchy=\"false\">(</mo>\n        <mi>&#x3B8;</mi>\n        <mo stretchy=\"false\">)</mo>\n      </mtd>        \n      <mtd>\n        <mi></mi>\n        <mo>=</mo>\n        <mfrac>\n          <mn>3</mn>\n          <mrow>\n            <mn>16</mn>\n            <mi>&#x3C0;</mi>\n          </mrow>\n        </mfrac>\n        <mo stretchy=\"false\">(</mo>\n        <mn>1</mn>\n        <mo>+</mo>\n        <mi>c</mi>\n        <mi>o</mi>\n        <msup>\n          <mi>s</mi>\n          <mn>2</mn>\n        </msup>\n        <mi>&#x3B8;</mi>\n        <mo stretchy=\"false\">)</mo>\n      </mtd>\n    </mtr>\n  </mtable>\n</math>\n<!-- /wp:html -->\n\n<!-- wp:paragraph -->\n<p>下方是米氏散射的相位函数，这里使用的是<a href=\"https://omlc.org/classroom/ece532/class3/hg.html\">Henyey-Greenstein函数</a>来近似。</p>\n<!-- /wp:paragraph -->\n\n<!-- wp:html -->\n<p><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">  <mi>P</mi>  <mo stretchy=\"false\">(</mo>  <mi>&#x3B8;</mi>  <mo stretchy=\"false\">)</mo>  <mo>&#x3D;</mo>  <mfrac>    <mrow>      <mn>1</mn>      <mo>&#x2212;</mo>      <msup>        <mi>g</mi>        <mrow>          <mn>2</mn>        </mrow>      </msup>    </mrow>    <mrow>      <mn>4</mn>      <mi>&#x3C0;</mi>      <mo stretchy=\"false\">(</mo>      <mn>1</mn>      <mo>+</mo>      <msup>        <mi>g</mi>        <mrow>          <mn>2</mn>        </mrow>      </msup>      <mo>&#x2212;</mo>      <mn>2</mn>      <mi>g</mi>      <mi>c</mi>      <mi>o</mi>      <mi>s</mi>      <mo stretchy=\"false\">(</mo>      <mi>&#x3B8;</mi>      <mo stretchy=\"false\">)</mo>      <msup>        <mo stretchy=\"false\">)</mo>        <mrow>          <mn>3</mn>          <mrow>            <mo>&#x2F;</mo>          </mrow>          <mn>2</mn>        </mrow>      </msup>    </mrow>  </mfrac></math></p>\n<!-- /wp:html -->\n\n<!-- wp:paragraph -->\n<p>好了，现在我们知道了如何计算光线在P点的散射行为。剩下的工作就是需要计算光线在传播过程的损耗了。这一部分的计算方法称之为<strong>衰减系数</strong>，或者<strong>消光系数</strong>。消光系数中的一部分，需要对光线在传播的路线的长度和空气密度进行积分。在程序中的表示就是将光的传播路径分为小段，将每一小段的长度和平均密度（或者小段中点的介质密度）相乘而得到，这个结果我们称之为<strong>光学距离（Optical Depth）</strong>。</p>\n<!-- /wp:paragraph -->\n\n<!-- wp:paragraph -->\n<p>衰减系数的计算公式如下，红色部分代表的是在海拔为h的散射系数。他可以分解为在海平面上的散射系数乘以在海拔h的介质密度。</p>\n<!-- /wp:paragraph -->\n\n<!-- wp:html -->\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n  <mstyle mathcolor=\"#00AAFF\">\n    <mi>T</mi>\n    <mo stretchy=\"false\">(</mo>\n    <mi>P</mi>\n    <mi>A</mi>\n    <mo stretchy=\"false\">)</mo>\n  </mstyle>\n  <mo>=</mo>\n<mstyle mathcolor=\"#00AA00\">\n  <mi>exp</mi>\n</mstyle>\n  <mrow data-mjx-texclass=\"ORD\">\n    <mo>&#x2212;</mo>\n    <msubsup>\n      <mo data-mjx-texclass=\"OP\">&#x222B;</mo>\n      <mi>P</mi>\n      <mi>A</mi>\n    </msubsup>\n    <mrow data-mjx-texclass=\"ORD\">\n      <mstyle mathcolor=\"Red\">\n        <mi>&#x3B2;</mi>\n        <mo stretchy=\"false\">(</mo>\n        <mi>&#x3BB;</mi>\n        <mo>,</mo>\n        <mi>h</mi>\n        <mo stretchy=\"false\">)</mo>\n      </mstyle>\n    </mrow>\n    <mi>d</mi>\n    <mi>s</mi>\n  </mrow>\n</math>\n<!-- /wp:html -->\n\n<!-- wp:paragraph -->\n<p>分解出来后得到的结果如下，其中红色的部分代表的意思是在海平面的散射系数，是一个常量，黄色部分代表了在海拔为h的介质的密度。这个公式的积分便是针对AP路线上的介质密度，其结果也就是光学距离。</p>\n<!-- /wp:paragraph -->\n\n<!-- wp:html -->\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n  <mstyle mathcolor=\"#00AAFF\">\n    <mi>T</mi>\n    <mo stretchy=\"false\">(</mo>\n    <mi>P</mi>\n    <mi>A</mi>\n    <mo stretchy=\"false\">)</mo>\n  </mstyle>\n  <mo>=</mo>\n    <mstyle mathcolor=\"#00AA00\">\n      <mi>exp</mi>\n    </mstyle>\n  <mrow data-mjx-texclass=\"ORD\">\n    <mo>&#x2212;</mo>\n    <mstyle mathcolor=\"Red\">\n      <mi>&#x3B2;</mi>\n      <mo stretchy=\"false\">(</mo>\n      <mi>&#x3BB;</mi>\n      <mo stretchy=\"false\">)</mo>\n    </mstyle>\n    <msubsup>\n      <mo data-mjx-texclass=\"OP\">&#x222B;</mo>\n      <mi>P</mi>\n      <mi>A</mi>\n    </msubsup>\n    <mrow data-mjx-texclass=\"ORD\">\n      <mstyle mathcolor=\"Gold\">\n        <mi>&#x3C1;</mi>\n        <mo stretchy=\"false\">(</mo>\n        <mi>h</mi>\n        <mo stretchy=\"false\">)</mo>\n      </mstyle>\n    </mrow>\n    <mi>d</mi>\n    <mi>s</mi>\n  </mrow>\n</math>\n<!-- /wp:html -->\n\n<!-- wp:paragraph -->\n<p>介质密度的计算公式如下，其中H代表的是一个基准海拔，是一个常量。不同散射的基准海拔有所不同，瑞利散射的基准海拔是*8500*，而米氏散射的基准海拔是*1200*。</p>\n<!-- /wp:paragraph -->\n\n<!-- wp:html -->\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n  <mstyle mathcolor=\"Gold\">\n    <mi>&#x3C1;</mi>\n    <mo stretchy=\"false\">(</mo>\n    <mi>h</mi>\n    <mo stretchy=\"false\">)</mo>\n  </mstyle>\n  <mo>=</mo>\n  <mi>exp</mi>\n  <mo stretchy=\"false\">(</mo>\n  <mo>&#x2212;</mo>\n  <mfrac>\n    <mi>h</mi>\n    <mi>H</mi>\n  </mfrac>\n  <mo stretchy=\"false\">)</mo>\n</math>\n<!-- /wp:html -->\n\n<p>自此，我们关于单次散射的天空大气渲染方法的基本预备知识已经了解的差不多了，接下来介绍shader的相关代码。</p>\n<h1 id=\"Shader代码\"><a href=\"#Shader代码\" class=\"headerlink\" title=\"Shader代码\"></a>Shader代码</h1><p>首先，为了计算AB、PC等视线和光线在大气层内的传播距离，我们需要一个函数来计算射线和球体的相交情况。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-type\">vec2</span> ray_sphere_intersection(<span class=\"hljs-type\">vec3</span> ray_origin, <span class=\"hljs-type\">vec3</span> ray_direction, <span class=\"hljs-type\">vec3</span> sphere_center, <span class=\"hljs-type\">float</span> sphere_radius)<br>&#123;<br>    <span class=\"hljs-comment\">// ray-sphere intersection that assumes</span><br>    <span class=\"hljs-type\">float</span> a = <span class=\"hljs-built_in\">dot</span>(ray_direction, ray_direction);<br>    <span class=\"hljs-type\">vec3</span> oc = ray_origin - sphere_center;<br>    <span class=\"hljs-type\">float</span> b = <span class=\"hljs-number\">2.0</span> * <span class=\"hljs-built_in\">dot</span>(ray_direction, oc);<br>    <span class=\"hljs-type\">float</span> c = <span class=\"hljs-built_in\">dot</span>(oc, oc) - (sphere_radius * sphere_radius);<br>    <span class=\"hljs-type\">float</span> d = (b*b) - <span class=\"hljs-number\">4.0</span>*a*c;<br><br>    <span class=\"hljs-comment\">// 返回击中结果，y小于x代表无结果</span><br>    <span class=\"hljs-keyword\">if</span> (d &lt; <span class=\"hljs-number\">0.0</span>) <span class=\"hljs-keyword\">return</span> <span class=\"hljs-type\">vec2</span>(<span class=\"hljs-number\">1e10</span>,<span class=\"hljs-number\">-1e10</span>);<br>    <span class=\"hljs-comment\">// 击中的话有两个相同或者不同的结果</span><br>    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-type\">vec2</span>(<br>        (-b - <span class=\"hljs-built_in\">sqrt</span>(d))/(<span class=\"hljs-number\">2.0</span>*a),<br>        (-b + <span class=\"hljs-built_in\">sqrt</span>(d))/(<span class=\"hljs-number\">2.0</span>*a)<br>    );<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<!-- wp:paragraph -->\n<p>好了，那有了这个辅助公式，我们正式开始计算大气的颜色。首先，按照之前的理论，我们要计算视线向量在大气层内的长度，所以我们对射线方向和大气层，以及射线方向和星球表面做射线检测。得到视线在大气层内的长度后，根据我们预设的想要采样的步数（iSteps），计算出每次采样的长度ds。</p>\n<!-- /wp:paragraph -->\n\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\">ray_dir = <span class=\"hljs-built_in\">normalize</span>(ray_dir);<br><br><span class=\"hljs-comment\">// 视线和大气层大小的尺寸的射线检测</span><br><span class=\"hljs-comment\">// x为大气入射点的距离、y为大气出射点的距离（x==y代表光线和大气球体相切，x&gt;y代表光线不经过大气）</span><br><span class=\"hljs-type\">vec2</span> atmos_hit = ray_sphere_intersection(ray_origin, ray_dir, rAtmos);<br><span class=\"hljs-comment\">// 未击中，返回0</span><br><span class=\"hljs-keyword\">if</span> (atmos_hit.x &gt; atmos_hit.y) <span class=\"hljs-keyword\">return</span> <span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">0</span>,<span class=\"hljs-number\">0</span>,<span class=\"hljs-number\">0</span>);<br><br>    <span class=\"hljs-comment\">// 视线和星球做射线检测，取得近处的检测结果（远处的那个光被星球本体遮挡）</span><br><span class=\"hljs-type\">vec2</span> planet_hit = ray_sphere_intersection(ray_origin, ray_dir, planet_radius);<br><span class=\"hljs-type\">float</span> light_distance = atmos_hit.y;<br><br><span class=\"hljs-comment\">// hit the planet</span><br><span class=\"hljs-keyword\">if</span>(planet_hit.x &lt; planet_hit.y &amp;&amp; planet_hit.x &gt; <span class=\"hljs-number\">0.1</span>)<br>&#123;<br>    light_distance = planet_hit.x;<br>&#125;<br><br><span class=\"hljs-comment\">// light sample length</span><br><span class=\"hljs-type\">float</span> ds = light_distance / <span class=\"hljs-type\">float</span>(iSteps);<br></code></pre></td></tr></table></figure>\n\n<!-- wp:paragraph -->\n<p>那么接下来，便是进入采样的循环。循环有两层，外面的循环是计算<strong>视线方向上的每个采样点的光能量</strong>，而计算这个点的光能量也需要一个循环，这个循环是用于采样该点和光源之间的连线在大气层内的线段（也就是之前图片的PC段）的光学距离。根据前面提到的公式，光学距离乘以海平面的散射系数便可以得到光的衰减系数，因此jSteps循环可以得到光线在进入大气层后，传播到达视线上的采样点（P0、P1...Pn点）的衰减所剩下的能量。</p>\n<!-- /wp:paragraph -->\n\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-comment\">// Initialize the primary ray time.</span><br><span class=\"hljs-type\">float</span> iTime = <span class=\"hljs-number\">0.0</span>;<br><br><span class=\"hljs-comment\">// Initialize accumulators for Rayleigh and Mie scattering.</span><br><span class=\"hljs-type\">vec3</span> total_scatter_rlh = <span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">0</span>,<span class=\"hljs-number\">0</span>,<span class=\"hljs-number\">0</span>);<br><span class=\"hljs-type\">vec3</span> total_scatter_mie = <span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">0</span>,<span class=\"hljs-number\">0</span>,<span class=\"hljs-number\">0</span>);<br><br><span class=\"hljs-comment\">// Initialize optical depth accumulators for the primary ray.</span><br><span class=\"hljs-type\">float</span> total_od_rlh = <span class=\"hljs-number\">0.0</span>;<br><span class=\"hljs-type\">float</span> total_od_mie = <span class=\"hljs-number\">0.0</span>;<br><br><span class=\"hljs-comment\">// 对每个视线上的采样点循环</span><br><span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> i = <span class=\"hljs-number\">0</span>; i &lt; iSteps; i++) &#123;<br>    <span class=\"hljs-comment\">// 获取到采样点的位置</span><br>    <span class=\"hljs-type\">vec3</span> iPos = ray_origin + ray_dir * (iTime + ds * <span class=\"hljs-number\">0.5</span>);<br><br>    <span class=\"hljs-comment\">// 在当前点向太阳的位置做射线检测，以大气的半径为球体。.y是代表大气的出射点，j_steps代表采样数</span><br>    <span class=\"hljs-type\">float</span> jStepSize = ray_sphere_intersection(iPos, pSun, rAtmos).y / <span class=\"hljs-type\">float</span>(jSteps);<br><br>    <span class=\"hljs-type\">float</span> jTime = <span class=\"hljs-number\">0.0</span>;<br>    <span class=\"hljs-type\">float</span> jOdRlh = <span class=\"hljs-number\">0.0</span>;<br>    <span class=\"hljs-type\">float</span> jOdMie = <span class=\"hljs-number\">0.0</span>;<br><br>    <span class=\"hljs-comment\">// 在当前采样到大气入射点的距离上，采样计算</span><br>    <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> j = <span class=\"hljs-number\">0</span>; j &lt; jSteps; j++) &#123;<br>        <span class=\"hljs-comment\">// 计算采样点到光源的衰减</span><br>        <span class=\"hljs-type\">vec3</span> jPos = iPos + pSun * (jTime + jStepSize * <span class=\"hljs-number\">0.5</span>);<br><br>        <span class=\"hljs-type\">float</span> jHeight = <span class=\"hljs-built_in\">length</span>(jPos-planet_center) - planet_radius;<br><br>        <span class=\"hljs-comment\">// Accumulate the optical depth.</span><br>        jOdRlh += get_atmos_density(jHeight, scale_height_rlh) * jStepSize;<br>        jOdMie += get_atmos_density(jHeight, scale_height_mie) * jStepSize;<br><br>        <span class=\"hljs-comment\">// Increment the secondary ray time.</span><br>        jTime += jStepSize;<br>    &#125;<br><br>    <span class=\"hljs-comment\">// 观察点和星球表面距离</span><br>    <span class=\"hljs-type\">float</span> surface_height = <span class=\"hljs-built_in\">length</span>(iPos-planet_center) - planet_radius;<br><br>    <span class=\"hljs-comment\">// 计算这一步的散射的光学深度结果</span><br>    <span class=\"hljs-type\">float</span> od_step_rlh = get_atmos_density(surface_height, scale_height_rlh) * ds;<br>    <span class=\"hljs-type\">float</span> od_step_mie = get_atmos_density(surface_height, scale_height_mie) * ds;<br>    <br>    total_od_rlh += od_step_rlh;<br>    total_od_mie += od_step_mie;<br><br>    <span class=\"hljs-comment\">// 计算衰减系数，光在经过一定距离后衰减剩下来的比例。</span><br>    <span class=\"hljs-type\">vec3</span> attn = <span class=\"hljs-built_in\">exp</span>(-(kMie * (total_od_mie + jOdMie) + kRlh * (total_od_rlh + jOdRlh)));<br><br>    <span class=\"hljs-comment\">// Accumulate scattering.</span><br>    total_scatter_rlh += od_step_rlh * attn;<br>    total_scatter_mie += od_step_mie * attn;<br><br>    <span class=\"hljs-comment\">// Increment the primary ray time.</span><br>    iTime += ds;<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<!-- wp:paragraph -->\n<p>计算瑞利散射和米氏散射的大气密度的代码如下所示，其中scale_height在计算瑞利散射的时候带入的是8500，米氏散射则带入1200。</p>\n<!-- /wp:paragraph -->\n\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-comment\">// 获取大气密度</span><br><span class=\"hljs-comment\">// 传入位置离海平面的高度，以及散射的相关基准高度</span><br><span class=\"hljs-comment\">// 大气中任意一点的散射系数的计算，简化拆解为散射在海平面的散射系数，乘以基于海平面高度的该散射的大气密度计算公式</span><br><span class=\"hljs-type\">float</span> get_atmos_density(<span class=\"hljs-type\">float</span> height_to_sea_level, <span class=\"hljs-type\">float</span> scale_height)<br>&#123;<br>    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-built_in\">exp</span>(-height_to_sea_level / scale_height);<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<!-- wp:paragraph -->\n<p>将iSteps循环的结果累加起来，我们便得到了视线上（AB）的每个采样点的光能量传播到达相机（A点）经过衰减的能量的和。那么还剩一个部分，就是光在P点进行散射的时候，光也损失掉了一部分能量，这个过程可以通过乘以米氏散射和瑞利散射的相位函数来计算。因为每个采样点P的结果都需要乘以相位函数，所以我们可以将它提取到最外面，乘以光的能量的总和即可。</p>\n<!-- /wp:paragraph -->\n\n<!-- wp:paragraph -->\n<p>不过需要注意的是，前面的公式提到了，相位函数是和光线入射方向和目标方向的夹角相关的，所以如果我们<strong>默认太阳光源是平行光</strong>的话，这个夹角对每个采样点来说是一样的，所以可以将相位函数提取到最外层。如果认为这个夹角对每个采样点不一样的话，那也许还是应该老老实实在计算每个采样点的光的时候单独去处理。</p>\n<!-- /wp:paragraph -->\n\n<p>最终的结果除了乘以相位函数，还需要乘以海平面的散射系数，结果如下。</p></p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-comment\">// 计算并返回最终颜色</span><br><span class=\"hljs-comment\">// iSun是光源（太阳）的颜色</span><br><span class=\"hljs-keyword\">return</span> iSun * (pRlh * kRlh * total_scatter_rlh + pMie * kMie * total_scatter_mie);<br></code></pre></td></tr></table></figure>\n<p>下面是得到的结果：</p>\n<iframe src=\"single-scatter-atmosphere.mp4\" scrolling=\"no\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\"> </iframe>\n\n\n<h1 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h1><ul>\n<li><a href=\"https://www.alanzucconi.com/2017/10/10/atmospheric-scattering-1/\">https://www.alanzucconi.com/2017/10/10/atmospheric-scattering-1/</a></li>\n<li><a href=\"https://www.xianlongok.site/post/8e5d3b12/\">https://www.xianlongok.site/post/8e5d3b12/</a></li>\n</ul>\n"},{"title":"屏幕空间反射","date":"2024-12-10T13:58:52.000Z","index_img":"/2024/12/10/screen-space-reflection/ssr_mugshot.png","banner_img":"/2024/12/10/screen-space-reflection/ssr_mugshot.png","_content":"\n# 什么是屏幕空间反射\n\n在前面的文章的一些配图中，其实已经揭露了之前在[KongEngine](https://github.com/ruochenhua/KongEngine)中实现的一个不小的功能点，就是**屏幕空间反射（screen space reflection）**。加入了屏幕空间反射能力之后，在一些光滑和带有反射材质的表面上，能够实现不错的反射效果。\n\n![KongEngine中的屏幕空间反射效果](screen-space-reflection-in-kong.png)\n\n屏幕空间反射（后简称**SSR**）是一种在实时渲染中用于模拟物体表面反射的成熟技术。SSR 的核心原理是在**屏幕空间**中进行光线追踪，以此计算反射效果，而无需像传统方法那样在世界空间或物体空间中进行复杂的光线与场景求交计算。它主要利用屏幕上已有的**深度图**和**法线图**等信息，通过对这些信息的分析和处理，确定反射光线的方向和位置，进而得到反射颜色。\n\n因为SSR不错的效果表现和相对来说比较低的性能开销，使其被广泛的应用在各个实时渲染领域，包括游戏、虚拟现实、建筑可视化等等。当然SSR的效果其实还不够完美，有很多无法解决的问题，这个在后面也会提到。但是在大多数情况下它的效果都是足够的，属于一个很**高性价比**的方法。\n\n# 如何实现屏幕空间反射\n\n## 屏幕空间反射的实现方法\n\n简单概括一下SSR的实现方法：\n\n1. 对于屏幕上的每个像素，先获取其**深度值**和**法线向量**。\n2. 结合相机参数和屏幕坐标计算出**观察向量**，进而得到**反射向量**。    \n3. 沿着反射向量在屏幕空间进行光线追踪，查找反射光线与场景中其他物体的相交点，以获取反射光线的颜色，最终将反射颜色与场景的原始颜色进行合成，得到带有反射效果的最终渲染结果。\n\n我们对于上面1、2两步应该已经不陌生了，毕竟我们在前面的文章就介绍了KongEngine接入[延迟渲染](https://ruochenhua.github.io/2024/10/19/defer-render/)的能力，在G-Buffer中我们已经存储了屏幕空间的各种相关数据，包括深度值和法线向量。有了这些数据，按照第2点计算反射向量也是很顺理成章的事情。\n\n对应的代码如下：\n```c++\n// 将延迟渲染保存的数据传给SSR shader\nvoid CRender::SSReflectionRender() const\n{\t\n\t// scene normal：defer_buffer_.g_normal_\n\t// scene reflection mask: defer_buffer_.g_orm_\n\t// scene position: defer_buffer_.g_position_\n\t// scene depth存在于normal贴图的w分量上\n\tssreflection_shader->Use();\n\tglActiveTexture(GL_TEXTURE0);\n\tglBindTexture(GL_TEXTURE_2D, defer_buffer_.g_position_);\n\tglActiveTexture(GL_TEXTURE0 + 1);\n\tglBindTexture(GL_TEXTURE_2D, defer_buffer_.g_normal_);\n\tglActiveTexture(GL_TEXTURE0 + 2);\n\t// 用给后处理的texture作为scene color\n\tglBindTexture(GL_TEXTURE_2D, post_process.screen_quad_texture[0]);\n\tglActiveTexture(GL_TEXTURE0 + 3);\n\tglBindTexture(GL_TEXTURE_2D, defer_buffer_.g_orm_);\n\t\n\tquad_shape->Draw();\n}\n```\n\n\n那么SSR的关键步骤，其实就在第三步，也是需要理解的重点部分。\n\n## 获得反射的颜色\n\n![SSR计算反射向量](ssr-step3.gif)\n\n上面这张图大致描述了第3步的状态。图片中**蓝色**的向量代表了从相机向场景中的每个像素发射的观察向量，**绿色**的向量代表了场景中的法线向量，根据观察向量和法线向量，我们能够计算出反射向量，也就是图片中的**红色**向量。\n\n我们需要得到的反射结果的颜色，基于反射向量和渲染场景中的其他物体的相交结果，这个是通过在*屏幕空间进行步近，判断步近后的坐标深度和深度缓存中存储的物体深度是否相交*来得到的。如果有相交结果，则该像素的反射颜色就是相交处的场景颜色，若超出步近范围（会预先设置一个步近长度或者步数的范围），则改点没有反射需要处理。\n\n![反射向量步近](ssr-step4.gif)\n\n这个原理是非常简单易懂的，下面是这段逻辑的大致代码：\n```glsl\nvoid main()\n{\n    vec2 tex_size = textureSize(scene_position, 0).xy;\n    vec2 tex_uv = gl_FragCoord.xy / tex_size;\n\n    // 材质相关的参数\n    vec4 orm = texture(orm_texture, TexCoords);\n    float roughness = orm.y;\n    float metallic = orm.z;\n    // 颜色信息\n    vec4 s_color = texture(scene_color, TexCoords);\n    FragColor = s_color;\n\n    // 深度和法线\n    vec4 normal_depth = texture(scene_normal, TexCoords);\n    vec3 world_normal = normalize(normal_depth.xyz + randVec3(fract(TexCoords.x*12.345)*sin(TexCoords.y)*9876.31)*0.2*roughness);\n\n    ...\n\n}\n```\n\n上面这段代码是将gbuffer中的信息读出来，包括前面讲到的几个部分。其中法线信息world_normal和材质的粗糙度做了一个随机方向的叠加，可以稍微增加反射效果的粗糙感。\n\n```glsl\n{\n    ...\n\n    // 远近平面\n    vec2 near_far = matrix_ubo.near_far.xy;\n    vec3 world_pos = texture(scene_position, TexCoords).xyz;\n    vec3 cam_pos = matrix_ubo.cam_pos.xyz;\n\n    mat4 projection = matrix_ubo.projection;\n    mat4 view = matrix_ubo.view;\n    mat4 vp = projection * view;    // 世界坐标到裁切坐标的转换矩阵\n\n    vec3 view_dir = normalize(world_pos-cam_pos);\n    vec3 rd = normalize(reflect(view_dir, world_normal));\n    \n    float resolution = 0.5;\n\n    float max_step_dist = 5.0;        \n    vec3 start_pos_world = world_pos  + rd*0.1;\n    vec3 end_pos_world = world_pos + max_step_dist*rd;\n\n    // 在屏幕空间上的从起始点到结束点的坐标\n    vec4 start_clip = vp * vec4(start_pos_world, 1.0);\n    vec4 end_clip   = vp * vec4(end_pos_world, 1.0);\n\n    ...\n}\n\n```\n\n在上面的代码，我们计算出了反射向量rd，同时也为步进设定了一个范围max_step_dist，得到了反射的步进区间，接下来就是进行步进的操作了。\n\n\n```glsl\n{\n    ...\n    \n    // 步进的步数\n    int step_count = 32;\n    int sample_count = step_count;\n    float delta = 1.0 / sample_count;   // 如果sample count为10，则delta采样为总共的1/10\n\n    vec4 reflect_color = vec4(0.0);\n    vec3 cam_pos = matrix_ubo.cam_pos.xyz;\n    for(int i = 0; i < sample_count; i++)\n    {\n        float sample_t = i*delta;\n        \n        // 步进到达处的屏幕空间uv\n        vec2 uv = vec2(0);\n\n        if(campareDepth(start_clip, end_clip, start_pos_world, end_pos_world, sample_t, uv))\n        {\n            reflect_color = texture(scene_color, uv);        \n            break;\n        }\n    }\n\n    FragColor = reflect_color*metallic;\n}\n\n```\n\n上面的步进代码中，根据设定好的步进步数迭代，campareDepth函数中将当前位置的深度和深度缓存中的数据作对比，若当前深度大于缓存中的值，则代表击中并返回对应的屏幕空间贴图对应的uv值。\n\n最后反射的颜色和金属度相乘，金属度越高的材质反射也是越高的。在场景渲染的最后，将反射颜色和场景实际的颜色结合，就得到了基本的反射效果了。\n\n```glsl\nFragColor = texture(scene_texture, TexCoords);\nvec4 reflection_color = texture(reflection_texture, TexCoords);\n\nFragColor.rgb += reflection_color.rgb * reflection_color.a;\n```\n\n![SSR效果:sample数32](ssr_normal_s32.png)\n上面是采样步数为32步时，得到的反射效果。可以看到反射效果确实出来了，但是条纹效果太过于明显。我们可以提高采样的精度，将sample的数量改为128后可以得到明显改善的结果，如下图。\n\n![SSR效果：sample数128](ssr_normal_s128.png)\n\n\n# 屏幕空间反射的优化\n现在我们已经有了基础的反射效果了，但是我们还是不满足不是吗。单纯提升采样精度确实能得到不错的效果，但是始终还是要考虑实际的性能的。那么有什么方法可以优化SSR的表现呢，下面会做一部分简单的介绍。\n\n## 粗晒和精筛\n在上面的采样处理中，我们通过步进迭代获取到了深度超过gbuffer中的深度的位置。为了弥补采样步数不足，我们可以将采样过程分为两部分：首先是粗筛，用较低的采样精度获取到大致的区间；然后再利用二分法或者其他方法在大致区间内进行二次筛选。\n\n```glsl\n{\n    ...\n\n    float sample_t = i*delta;\n    // 线性插值找到当前采样的屏幕空间的点\n    vec2 uv = vec2(0);\n\n    if(campareDepth(start_clip, end_clip, start_pos_world, end_pos_world, sample_t, uv))\n    {\n        reflect_color = texture(scene_color, uv);\n        int split_count = 10;\n        float i_divide_pos = 0.5;\n        while(split_count > 0)\n        {\n            if(campareDepth(start_clip, end_clip, start_pos_world, end_pos_world, (float(i)-i_divide_pos)*delta, uv))\n            {\n                i_divide_pos += i_divide_pos*0.5;\n            }\n            else\n            {\n                i_divide_pos -= i_divide_pos*0.5;\n            }\n            split_count--;\n        }\n\n        reflect_color = texture(scene_color, uv);\n        break;\n    }\n\n    ...\n}\n```\n\n下面是这种方法的结果，可以看到效果是稍微好了些，不过如果需要再进一步的话，还是避免不了要提升采样精度。\n![SSR二次采样](ssr_sample_twice.png)\n\n## 屏幕空间步进\n\n目前比较常用的优化方法，是把三维空间做光线步近替换为在屏幕空间做光线步近。\n传统的在三维空间做光线步近，很难避免采样不均的问题，如果我们是以三维空间的的步近长度作为采样依据的话，会出现下面的问题。其中蓝色小格子代表的是像素，红色的点对应的是每个采样点对应的像素位置。\n\n![SSR过采样](ssr_over_sample.png)\n当反射角度相对来说比较大，很容易出现非常多采样点对应同一个像素，进行了大量的重复运算。\n\n![SSR欠采样](ssr_under_sample.png)\n\n反射角度过小的时候，有很容易出现跳过中间某些像素的情况，出现了欠采样的情况。这也是我们上面的反射效果出现了带状的原因。\n\n在[Efficient GPU Screen-Space Ray Tracing](https://jcgt.org/published/0003/04/04/)这篇文章提出了在屏幕空间采样的观点。通过将采样点的选择放在屏幕空间，实现采样点连续且分布均匀的效果。每个采样点不会进行重复计算，也保证了性能的最优。\n![SSR屏幕空间采样方法](ssr_ss_sample.png)\n\n为了实现屏幕看见步近，代码需要做一些修改：\n```glsl\n{\n    ...\n\n    vec3 start_pos_world = world_pos  + rd*0.1;\n    vec3 end_pos_world = world_pos + max_step_dist*rd;\n    // 在屏幕空间上的从起始点到结束点的坐标[0, resolution]\n    vec4 start_clip = vp * vec4(start_pos_world, 1.0);\n    vec4 end_clip   = vp * vec4(end_pos_world, 1.0);\n    // 在屏幕空间进行光线步进\n    // 起始点和结束点\n    vec3 start_ndc  = start_clip.xyz / start_clip.w;\n    vec3 end_ndc    = end_clip.xyz / end_clip.w;\n    vec3 ndc_diff = end_ndc - start_ndc;\n\n    // ndc->屏幕坐标 [0, resolution.xy]\n    vec3 start_screen  = vec3(0);\n    start_screen.xy = (start_ndc.xy + 1) / 2 * tex_size;\n    start_screen.z = (near_far.y - near_far.x) * 0.5 * start_ndc.z + (near_far.x + near_far.y) * 0.5;\n\n    vec3 end_screen    = vec3(0);    \n    end_screen.xy = (end_ndc.xy + 1) / 2 * tex_size;\n    end_screen.z = (near_far.y - near_far.x) * 0.5 * end_ndc.z + (near_far.x + near_far.y) * 0.5;\n\n    int step_count = 32;\n\n    vec3 screen_diff = end_screen - start_screen;\n    int sample_count = int(max(abs(screen_diff.x), abs(screen_diff.y)) * resolution) ; // 大于1\n\n    sample_count = min(sample_count, 64);\n    vec3 delta_screen = screen_diff / float(sample_count);\n\n    // 如果sample count为10，则每次采样的前进的长度为总长度的1/10\n    float percentage_delta = 1.0 / float(sample_count);\n    vec3 current_screen = start_screen;\n    vec3 last_screen = current_screen;\n    float current_percentage = 0.0;\n    float last_percentage = 0.0;\n\n    ...\n\n```\n使用屏幕空间步近，前面和原来的差不多，在获取步近的起始点和结束点的时候，需要将坐标转换为屏幕空间的坐标，也就是其中的current_screen和last_screen。\n\n屏幕空间采样点数和**采样的起始和结束位置的像素差值**有关，所以和渲染输出的分辨率也是相关的。如果渲染分辨率越高，其对应所需要的采样点数可能也会增加，这里我们控制在64以内。当然如果起始点和结束点的像素差值较小，对应的采样点数也会变小，也就是对于距离相机很远的位置的采样会减少，在怎么不影响效果的情况下提升性能表现。\n\n\n``` glsl\n    ...\n\n    vec4 reflect_color = vec4(0.0);\n    vec3 cam_pos = matrix_ubo.cam_pos.xyz;\n    for(int i = 0; i < sample_count; i++)\n    {\n        // 采样当前屏幕上的点对应场景世界空间坐标的位置\n        vec2 uv = current_screen.xy / tex_size;\n\n        // 转换为贴图坐标，检查越界\n        if(uv.x < 0.0 || uv.y < 0.0 || uv.x > 1.0 || uv.y > 1.0)\n        {\n            continue;\n        }\n        else\n        {\n            // 延迟渲染存储的屏幕对应的世界位置\n            vec3 sample_world = texture(scene_position, uv).xyz;\n            \n            vec4 sample_ndc = vec4(mix(start_ndc, end_ndc, current_percentage), 1.0);\n            if(compareDepth(sample_ndc, uv))\n            {\n                // 初筛后再二分法检查\n                int split_count = 5;\n                while(split_count > 0)\n                {\n                    vec3 mid_screen = (last_screen + current_screen) * 0.5;\n                    float mid_percentage = (last_percentage + current_percentage) * 0.5;\n                    vec4 mid_ndc = vec4(mix(start_ndc, end_ndc, mid_percentage), 1.0);\n                    uv = mid_screen.xy / tex_size;\n                    if(compareDepth(mid_ndc, uv))\n                    {\n                        current_screen = mid_screen;\n                    }\n                    else\n                    {\n                        last_screen = mid_screen;\n                    }\n                    split_count--;\n                }\n\n                reflect_color = texture(scene_color, uv);\n                break;\n            }\n\n            last_screen = current_screen;\n            last_percentage = current_percentage;\n            current_screen += delta_screen;\n            current_percentage += percentage_delta;\n        }\n    }\n\n    FragColor = reflect_color*metallic;\n\n}\n\n```\n这里在屏幕空间采样还配合了之前的粗筛和精筛的方法，下面是使用屏幕空间采样的表现。可以看到条纹的状况被极大的缓解了。\n\n![SSR屏幕空间采样结果](ssr_result.png)\n\n应用在实际场景中，SSR的效果能比较明显的提升渲染质感。\n![SSR实际应用](ssr_mugshot.png)\n\n# 总结\n\nSSR是一种计算场景的反射效果的算法，它基于屏幕空间已有的深度图和法线图等信息，通过计算反射向量，在屏幕空间中进行光线追踪，查找反射光线与场景中其他物体的相交点，获取相交点的颜色作为反射颜色，并与原始颜色合成得到最终渲染结果。\n\nSSR的优点是计算效率相对较高，能实时反映场景中物体的变化，适用于复杂几何形状和不规则表面，适合大规模的动态场景，无需额外的镜头或几何体。\n\n当然，SSR也有局限性，它只能反射屏幕上可见的物体，超出屏幕边界的内容无法被反射；反射的物体可能存在失真或错误，尤其是边缘区域；依赖屏幕分辨率，高分辨率下可能对性能有较大影响\n\n## SSR的优化改进算法：\n### SSSR（Spatially Separated Screen Space Reflection）\n原理：SSSR 是对传统 SSR 技术的一种改进。它主要是基于空间分离的思想来处理屏幕空间反射。传统 SSR 在处理反射时可能会受到屏幕空间限制和采样不足等问题的影响。SSSR 通过将屏幕空间划分为不同的区域，在这些区域内分别进行更精细的反射处理。\n\n例如，它可以根据场景中物体的距离、重要性或者反射特性等因素，对空间进行划分。对于反射效果比较复杂或者重要的区域，分配更多的资源进行反射计算，而对于相对简单或者不重要的区域，则采用较为简略的计算方式。\n\n优点：\n- 提高反射精度：通过对特定区域的精细处理，能够有效提高反射的精度。比如在处理具有高反射率的物体表面或者复杂的光照反射场景时，可以得到更真实、细腻的反射效果。\n- 优化性能：与传统 SSR 相比，SSSR 能够更合理地分配计算资源。它避免了在整个屏幕空间进行统一标准的反射计算，从而在一定程度上减轻了计算负担，特别是在大规模复杂场景中，可以更好地平衡反射效果和性能。\n\n局限性：\n- 空间划分的复杂性：如何合理地划分空间是一个具有挑战性的问题。如果空间划分不合理，可能会导致反射效果出现不自然的边界或者遗漏重要的反射区域。\n- 增加算法复杂度：空间划分和不同区域的分别处理增加了算法的复杂度。这可能会导致开发和调试的难度增加，并且在某些情况下，可能会引入新的错误或者视觉瑕疵。\n\n### Hi-z SSR（Hierarchical - z Screen Space Reflection）\n原理：Hi - z SSR 是利用层次化的深度信息（Hierarchical-z）来改进 SSR。它构建了一个层次化的深度缓冲区，这个缓冲区可以更有效地存储和检索深度信息。在计算反射时，通过这个层次化的结构，可以快速地在不同层次的深度信息中进行搜索和采样。\n例如，在较高层次的深度信息中，可以快速定位反射光线可能相交的大致区域，然后在较低层次的深度信息中进行更精细的搜索，就像在地图的不同比例尺中查找目标位置一样。这种层次化的搜索方式能够更高效地利用深度信息来计算反射。\n\n优点：\n- 高效的深度搜索：层次化的深度搜索大大提高了反射光线与场景相交点的查找效率。尤其是在处理具有深度层次丰富的复杂场景时，能够快速定位反射位置，减少计算时间。\n- 增强的反射范围：由于能够更好地利用深度信息，Hi-z SSR 可以在一定程度上缓解传统 SSR 中屏幕外反射难以处理的问题。它可以通过层次化的深度结构，对屏幕外部分场景的深度信息进行合理推测和利用，从而扩展反射的有效范围。\n\n局限性\n- 深度缓冲区的构建成本：构建层次化的深度缓冲区需要额外的存储空间和计算资源来生成和维护。这可能会在一些资源受限的场景或者硬件平台上带来一定的负担。\n\n- 精度与性能的平衡：尽管 Hi-z SSR 提高了搜索效率，但在平衡反射精度和性能方面仍然是一个挑战。在某些情况下，过于追求效率可能会导致反射精度下降，而过度强调精度又可能会使性能开销过大。\n\n\n# 参考资料\n\nhttps://lettier.github.io/3d-game-shaders-for-beginners/screen-space-reflection.html\n\nhttps://jcgt.org/published/0003/04/04/\n\nhttps://blog.csdn.net/qjh5606/article/details/120102582?ops_request_misc=%257B%2522request%255Fid%2522%253A%25225a1434f7df5d388dc4166f4877eb172b%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=5a1434f7df5d388dc4166f4877eb172b&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-120102582-null-null.142^v100^control&utm_term=Efficient%20GPU%20Screen-Space%20Ray%20Tracing&spm=1018.2226.3001.4187","source":"_posts/screen-space-reflection.md","raw":"---\ntitle: 屏幕空间反射\ndate: 2024-12-10 21:58:52\ncategories: \n\t- 技术漫谈\ntags: [3D, 渲染, 编程]\nindex_img: /2024/12/10/screen-space-reflection/ssr_mugshot.png\nbanner_img: /2024/12/10/screen-space-reflection/ssr_mugshot.png\n---\n\n# 什么是屏幕空间反射\n\n在前面的文章的一些配图中，其实已经揭露了之前在[KongEngine](https://github.com/ruochenhua/KongEngine)中实现的一个不小的功能点，就是**屏幕空间反射（screen space reflection）**。加入了屏幕空间反射能力之后，在一些光滑和带有反射材质的表面上，能够实现不错的反射效果。\n\n![KongEngine中的屏幕空间反射效果](screen-space-reflection-in-kong.png)\n\n屏幕空间反射（后简称**SSR**）是一种在实时渲染中用于模拟物体表面反射的成熟技术。SSR 的核心原理是在**屏幕空间**中进行光线追踪，以此计算反射效果，而无需像传统方法那样在世界空间或物体空间中进行复杂的光线与场景求交计算。它主要利用屏幕上已有的**深度图**和**法线图**等信息，通过对这些信息的分析和处理，确定反射光线的方向和位置，进而得到反射颜色。\n\n因为SSR不错的效果表现和相对来说比较低的性能开销，使其被广泛的应用在各个实时渲染领域，包括游戏、虚拟现实、建筑可视化等等。当然SSR的效果其实还不够完美，有很多无法解决的问题，这个在后面也会提到。但是在大多数情况下它的效果都是足够的，属于一个很**高性价比**的方法。\n\n# 如何实现屏幕空间反射\n\n## 屏幕空间反射的实现方法\n\n简单概括一下SSR的实现方法：\n\n1. 对于屏幕上的每个像素，先获取其**深度值**和**法线向量**。\n2. 结合相机参数和屏幕坐标计算出**观察向量**，进而得到**反射向量**。    \n3. 沿着反射向量在屏幕空间进行光线追踪，查找反射光线与场景中其他物体的相交点，以获取反射光线的颜色，最终将反射颜色与场景的原始颜色进行合成，得到带有反射效果的最终渲染结果。\n\n我们对于上面1、2两步应该已经不陌生了，毕竟我们在前面的文章就介绍了KongEngine接入[延迟渲染](https://ruochenhua.github.io/2024/10/19/defer-render/)的能力，在G-Buffer中我们已经存储了屏幕空间的各种相关数据，包括深度值和法线向量。有了这些数据，按照第2点计算反射向量也是很顺理成章的事情。\n\n对应的代码如下：\n```c++\n// 将延迟渲染保存的数据传给SSR shader\nvoid CRender::SSReflectionRender() const\n{\t\n\t// scene normal：defer_buffer_.g_normal_\n\t// scene reflection mask: defer_buffer_.g_orm_\n\t// scene position: defer_buffer_.g_position_\n\t// scene depth存在于normal贴图的w分量上\n\tssreflection_shader->Use();\n\tglActiveTexture(GL_TEXTURE0);\n\tglBindTexture(GL_TEXTURE_2D, defer_buffer_.g_position_);\n\tglActiveTexture(GL_TEXTURE0 + 1);\n\tglBindTexture(GL_TEXTURE_2D, defer_buffer_.g_normal_);\n\tglActiveTexture(GL_TEXTURE0 + 2);\n\t// 用给后处理的texture作为scene color\n\tglBindTexture(GL_TEXTURE_2D, post_process.screen_quad_texture[0]);\n\tglActiveTexture(GL_TEXTURE0 + 3);\n\tglBindTexture(GL_TEXTURE_2D, defer_buffer_.g_orm_);\n\t\n\tquad_shape->Draw();\n}\n```\n\n\n那么SSR的关键步骤，其实就在第三步，也是需要理解的重点部分。\n\n## 获得反射的颜色\n\n![SSR计算反射向量](ssr-step3.gif)\n\n上面这张图大致描述了第3步的状态。图片中**蓝色**的向量代表了从相机向场景中的每个像素发射的观察向量，**绿色**的向量代表了场景中的法线向量，根据观察向量和法线向量，我们能够计算出反射向量，也就是图片中的**红色**向量。\n\n我们需要得到的反射结果的颜色，基于反射向量和渲染场景中的其他物体的相交结果，这个是通过在*屏幕空间进行步近，判断步近后的坐标深度和深度缓存中存储的物体深度是否相交*来得到的。如果有相交结果，则该像素的反射颜色就是相交处的场景颜色，若超出步近范围（会预先设置一个步近长度或者步数的范围），则改点没有反射需要处理。\n\n![反射向量步近](ssr-step4.gif)\n\n这个原理是非常简单易懂的，下面是这段逻辑的大致代码：\n```glsl\nvoid main()\n{\n    vec2 tex_size = textureSize(scene_position, 0).xy;\n    vec2 tex_uv = gl_FragCoord.xy / tex_size;\n\n    // 材质相关的参数\n    vec4 orm = texture(orm_texture, TexCoords);\n    float roughness = orm.y;\n    float metallic = orm.z;\n    // 颜色信息\n    vec4 s_color = texture(scene_color, TexCoords);\n    FragColor = s_color;\n\n    // 深度和法线\n    vec4 normal_depth = texture(scene_normal, TexCoords);\n    vec3 world_normal = normalize(normal_depth.xyz + randVec3(fract(TexCoords.x*12.345)*sin(TexCoords.y)*9876.31)*0.2*roughness);\n\n    ...\n\n}\n```\n\n上面这段代码是将gbuffer中的信息读出来，包括前面讲到的几个部分。其中法线信息world_normal和材质的粗糙度做了一个随机方向的叠加，可以稍微增加反射效果的粗糙感。\n\n```glsl\n{\n    ...\n\n    // 远近平面\n    vec2 near_far = matrix_ubo.near_far.xy;\n    vec3 world_pos = texture(scene_position, TexCoords).xyz;\n    vec3 cam_pos = matrix_ubo.cam_pos.xyz;\n\n    mat4 projection = matrix_ubo.projection;\n    mat4 view = matrix_ubo.view;\n    mat4 vp = projection * view;    // 世界坐标到裁切坐标的转换矩阵\n\n    vec3 view_dir = normalize(world_pos-cam_pos);\n    vec3 rd = normalize(reflect(view_dir, world_normal));\n    \n    float resolution = 0.5;\n\n    float max_step_dist = 5.0;        \n    vec3 start_pos_world = world_pos  + rd*0.1;\n    vec3 end_pos_world = world_pos + max_step_dist*rd;\n\n    // 在屏幕空间上的从起始点到结束点的坐标\n    vec4 start_clip = vp * vec4(start_pos_world, 1.0);\n    vec4 end_clip   = vp * vec4(end_pos_world, 1.0);\n\n    ...\n}\n\n```\n\n在上面的代码，我们计算出了反射向量rd，同时也为步进设定了一个范围max_step_dist，得到了反射的步进区间，接下来就是进行步进的操作了。\n\n\n```glsl\n{\n    ...\n    \n    // 步进的步数\n    int step_count = 32;\n    int sample_count = step_count;\n    float delta = 1.0 / sample_count;   // 如果sample count为10，则delta采样为总共的1/10\n\n    vec4 reflect_color = vec4(0.0);\n    vec3 cam_pos = matrix_ubo.cam_pos.xyz;\n    for(int i = 0; i < sample_count; i++)\n    {\n        float sample_t = i*delta;\n        \n        // 步进到达处的屏幕空间uv\n        vec2 uv = vec2(0);\n\n        if(campareDepth(start_clip, end_clip, start_pos_world, end_pos_world, sample_t, uv))\n        {\n            reflect_color = texture(scene_color, uv);        \n            break;\n        }\n    }\n\n    FragColor = reflect_color*metallic;\n}\n\n```\n\n上面的步进代码中，根据设定好的步进步数迭代，campareDepth函数中将当前位置的深度和深度缓存中的数据作对比，若当前深度大于缓存中的值，则代表击中并返回对应的屏幕空间贴图对应的uv值。\n\n最后反射的颜色和金属度相乘，金属度越高的材质反射也是越高的。在场景渲染的最后，将反射颜色和场景实际的颜色结合，就得到了基本的反射效果了。\n\n```glsl\nFragColor = texture(scene_texture, TexCoords);\nvec4 reflection_color = texture(reflection_texture, TexCoords);\n\nFragColor.rgb += reflection_color.rgb * reflection_color.a;\n```\n\n![SSR效果:sample数32](ssr_normal_s32.png)\n上面是采样步数为32步时，得到的反射效果。可以看到反射效果确实出来了，但是条纹效果太过于明显。我们可以提高采样的精度，将sample的数量改为128后可以得到明显改善的结果，如下图。\n\n![SSR效果：sample数128](ssr_normal_s128.png)\n\n\n# 屏幕空间反射的优化\n现在我们已经有了基础的反射效果了，但是我们还是不满足不是吗。单纯提升采样精度确实能得到不错的效果，但是始终还是要考虑实际的性能的。那么有什么方法可以优化SSR的表现呢，下面会做一部分简单的介绍。\n\n## 粗晒和精筛\n在上面的采样处理中，我们通过步进迭代获取到了深度超过gbuffer中的深度的位置。为了弥补采样步数不足，我们可以将采样过程分为两部分：首先是粗筛，用较低的采样精度获取到大致的区间；然后再利用二分法或者其他方法在大致区间内进行二次筛选。\n\n```glsl\n{\n    ...\n\n    float sample_t = i*delta;\n    // 线性插值找到当前采样的屏幕空间的点\n    vec2 uv = vec2(0);\n\n    if(campareDepth(start_clip, end_clip, start_pos_world, end_pos_world, sample_t, uv))\n    {\n        reflect_color = texture(scene_color, uv);\n        int split_count = 10;\n        float i_divide_pos = 0.5;\n        while(split_count > 0)\n        {\n            if(campareDepth(start_clip, end_clip, start_pos_world, end_pos_world, (float(i)-i_divide_pos)*delta, uv))\n            {\n                i_divide_pos += i_divide_pos*0.5;\n            }\n            else\n            {\n                i_divide_pos -= i_divide_pos*0.5;\n            }\n            split_count--;\n        }\n\n        reflect_color = texture(scene_color, uv);\n        break;\n    }\n\n    ...\n}\n```\n\n下面是这种方法的结果，可以看到效果是稍微好了些，不过如果需要再进一步的话，还是避免不了要提升采样精度。\n![SSR二次采样](ssr_sample_twice.png)\n\n## 屏幕空间步进\n\n目前比较常用的优化方法，是把三维空间做光线步近替换为在屏幕空间做光线步近。\n传统的在三维空间做光线步近，很难避免采样不均的问题，如果我们是以三维空间的的步近长度作为采样依据的话，会出现下面的问题。其中蓝色小格子代表的是像素，红色的点对应的是每个采样点对应的像素位置。\n\n![SSR过采样](ssr_over_sample.png)\n当反射角度相对来说比较大，很容易出现非常多采样点对应同一个像素，进行了大量的重复运算。\n\n![SSR欠采样](ssr_under_sample.png)\n\n反射角度过小的时候，有很容易出现跳过中间某些像素的情况，出现了欠采样的情况。这也是我们上面的反射效果出现了带状的原因。\n\n在[Efficient GPU Screen-Space Ray Tracing](https://jcgt.org/published/0003/04/04/)这篇文章提出了在屏幕空间采样的观点。通过将采样点的选择放在屏幕空间，实现采样点连续且分布均匀的效果。每个采样点不会进行重复计算，也保证了性能的最优。\n![SSR屏幕空间采样方法](ssr_ss_sample.png)\n\n为了实现屏幕看见步近，代码需要做一些修改：\n```glsl\n{\n    ...\n\n    vec3 start_pos_world = world_pos  + rd*0.1;\n    vec3 end_pos_world = world_pos + max_step_dist*rd;\n    // 在屏幕空间上的从起始点到结束点的坐标[0, resolution]\n    vec4 start_clip = vp * vec4(start_pos_world, 1.0);\n    vec4 end_clip   = vp * vec4(end_pos_world, 1.0);\n    // 在屏幕空间进行光线步进\n    // 起始点和结束点\n    vec3 start_ndc  = start_clip.xyz / start_clip.w;\n    vec3 end_ndc    = end_clip.xyz / end_clip.w;\n    vec3 ndc_diff = end_ndc - start_ndc;\n\n    // ndc->屏幕坐标 [0, resolution.xy]\n    vec3 start_screen  = vec3(0);\n    start_screen.xy = (start_ndc.xy + 1) / 2 * tex_size;\n    start_screen.z = (near_far.y - near_far.x) * 0.5 * start_ndc.z + (near_far.x + near_far.y) * 0.5;\n\n    vec3 end_screen    = vec3(0);    \n    end_screen.xy = (end_ndc.xy + 1) / 2 * tex_size;\n    end_screen.z = (near_far.y - near_far.x) * 0.5 * end_ndc.z + (near_far.x + near_far.y) * 0.5;\n\n    int step_count = 32;\n\n    vec3 screen_diff = end_screen - start_screen;\n    int sample_count = int(max(abs(screen_diff.x), abs(screen_diff.y)) * resolution) ; // 大于1\n\n    sample_count = min(sample_count, 64);\n    vec3 delta_screen = screen_diff / float(sample_count);\n\n    // 如果sample count为10，则每次采样的前进的长度为总长度的1/10\n    float percentage_delta = 1.0 / float(sample_count);\n    vec3 current_screen = start_screen;\n    vec3 last_screen = current_screen;\n    float current_percentage = 0.0;\n    float last_percentage = 0.0;\n\n    ...\n\n```\n使用屏幕空间步近，前面和原来的差不多，在获取步近的起始点和结束点的时候，需要将坐标转换为屏幕空间的坐标，也就是其中的current_screen和last_screen。\n\n屏幕空间采样点数和**采样的起始和结束位置的像素差值**有关，所以和渲染输出的分辨率也是相关的。如果渲染分辨率越高，其对应所需要的采样点数可能也会增加，这里我们控制在64以内。当然如果起始点和结束点的像素差值较小，对应的采样点数也会变小，也就是对于距离相机很远的位置的采样会减少，在怎么不影响效果的情况下提升性能表现。\n\n\n``` glsl\n    ...\n\n    vec4 reflect_color = vec4(0.0);\n    vec3 cam_pos = matrix_ubo.cam_pos.xyz;\n    for(int i = 0; i < sample_count; i++)\n    {\n        // 采样当前屏幕上的点对应场景世界空间坐标的位置\n        vec2 uv = current_screen.xy / tex_size;\n\n        // 转换为贴图坐标，检查越界\n        if(uv.x < 0.0 || uv.y < 0.0 || uv.x > 1.0 || uv.y > 1.0)\n        {\n            continue;\n        }\n        else\n        {\n            // 延迟渲染存储的屏幕对应的世界位置\n            vec3 sample_world = texture(scene_position, uv).xyz;\n            \n            vec4 sample_ndc = vec4(mix(start_ndc, end_ndc, current_percentage), 1.0);\n            if(compareDepth(sample_ndc, uv))\n            {\n                // 初筛后再二分法检查\n                int split_count = 5;\n                while(split_count > 0)\n                {\n                    vec3 mid_screen = (last_screen + current_screen) * 0.5;\n                    float mid_percentage = (last_percentage + current_percentage) * 0.5;\n                    vec4 mid_ndc = vec4(mix(start_ndc, end_ndc, mid_percentage), 1.0);\n                    uv = mid_screen.xy / tex_size;\n                    if(compareDepth(mid_ndc, uv))\n                    {\n                        current_screen = mid_screen;\n                    }\n                    else\n                    {\n                        last_screen = mid_screen;\n                    }\n                    split_count--;\n                }\n\n                reflect_color = texture(scene_color, uv);\n                break;\n            }\n\n            last_screen = current_screen;\n            last_percentage = current_percentage;\n            current_screen += delta_screen;\n            current_percentage += percentage_delta;\n        }\n    }\n\n    FragColor = reflect_color*metallic;\n\n}\n\n```\n这里在屏幕空间采样还配合了之前的粗筛和精筛的方法，下面是使用屏幕空间采样的表现。可以看到条纹的状况被极大的缓解了。\n\n![SSR屏幕空间采样结果](ssr_result.png)\n\n应用在实际场景中，SSR的效果能比较明显的提升渲染质感。\n![SSR实际应用](ssr_mugshot.png)\n\n# 总结\n\nSSR是一种计算场景的反射效果的算法，它基于屏幕空间已有的深度图和法线图等信息，通过计算反射向量，在屏幕空间中进行光线追踪，查找反射光线与场景中其他物体的相交点，获取相交点的颜色作为反射颜色，并与原始颜色合成得到最终渲染结果。\n\nSSR的优点是计算效率相对较高，能实时反映场景中物体的变化，适用于复杂几何形状和不规则表面，适合大规模的动态场景，无需额外的镜头或几何体。\n\n当然，SSR也有局限性，它只能反射屏幕上可见的物体，超出屏幕边界的内容无法被反射；反射的物体可能存在失真或错误，尤其是边缘区域；依赖屏幕分辨率，高分辨率下可能对性能有较大影响\n\n## SSR的优化改进算法：\n### SSSR（Spatially Separated Screen Space Reflection）\n原理：SSSR 是对传统 SSR 技术的一种改进。它主要是基于空间分离的思想来处理屏幕空间反射。传统 SSR 在处理反射时可能会受到屏幕空间限制和采样不足等问题的影响。SSSR 通过将屏幕空间划分为不同的区域，在这些区域内分别进行更精细的反射处理。\n\n例如，它可以根据场景中物体的距离、重要性或者反射特性等因素，对空间进行划分。对于反射效果比较复杂或者重要的区域，分配更多的资源进行反射计算，而对于相对简单或者不重要的区域，则采用较为简略的计算方式。\n\n优点：\n- 提高反射精度：通过对特定区域的精细处理，能够有效提高反射的精度。比如在处理具有高反射率的物体表面或者复杂的光照反射场景时，可以得到更真实、细腻的反射效果。\n- 优化性能：与传统 SSR 相比，SSSR 能够更合理地分配计算资源。它避免了在整个屏幕空间进行统一标准的反射计算，从而在一定程度上减轻了计算负担，特别是在大规模复杂场景中，可以更好地平衡反射效果和性能。\n\n局限性：\n- 空间划分的复杂性：如何合理地划分空间是一个具有挑战性的问题。如果空间划分不合理，可能会导致反射效果出现不自然的边界或者遗漏重要的反射区域。\n- 增加算法复杂度：空间划分和不同区域的分别处理增加了算法的复杂度。这可能会导致开发和调试的难度增加，并且在某些情况下，可能会引入新的错误或者视觉瑕疵。\n\n### Hi-z SSR（Hierarchical - z Screen Space Reflection）\n原理：Hi - z SSR 是利用层次化的深度信息（Hierarchical-z）来改进 SSR。它构建了一个层次化的深度缓冲区，这个缓冲区可以更有效地存储和检索深度信息。在计算反射时，通过这个层次化的结构，可以快速地在不同层次的深度信息中进行搜索和采样。\n例如，在较高层次的深度信息中，可以快速定位反射光线可能相交的大致区域，然后在较低层次的深度信息中进行更精细的搜索，就像在地图的不同比例尺中查找目标位置一样。这种层次化的搜索方式能够更高效地利用深度信息来计算反射。\n\n优点：\n- 高效的深度搜索：层次化的深度搜索大大提高了反射光线与场景相交点的查找效率。尤其是在处理具有深度层次丰富的复杂场景时，能够快速定位反射位置，减少计算时间。\n- 增强的反射范围：由于能够更好地利用深度信息，Hi-z SSR 可以在一定程度上缓解传统 SSR 中屏幕外反射难以处理的问题。它可以通过层次化的深度结构，对屏幕外部分场景的深度信息进行合理推测和利用，从而扩展反射的有效范围。\n\n局限性\n- 深度缓冲区的构建成本：构建层次化的深度缓冲区需要额外的存储空间和计算资源来生成和维护。这可能会在一些资源受限的场景或者硬件平台上带来一定的负担。\n\n- 精度与性能的平衡：尽管 Hi-z SSR 提高了搜索效率，但在平衡反射精度和性能方面仍然是一个挑战。在某些情况下，过于追求效率可能会导致反射精度下降，而过度强调精度又可能会使性能开销过大。\n\n\n# 参考资料\n\nhttps://lettier.github.io/3d-game-shaders-for-beginners/screen-space-reflection.html\n\nhttps://jcgt.org/published/0003/04/04/\n\nhttps://blog.csdn.net/qjh5606/article/details/120102582?ops_request_misc=%257B%2522request%255Fid%2522%253A%25225a1434f7df5d388dc4166f4877eb172b%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=5a1434f7df5d388dc4166f4877eb172b&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-120102582-null-null.142^v100^control&utm_term=Efficient%20GPU%20Screen-Space%20Ray%20Tracing&spm=1018.2226.3001.4187","slug":"screen-space-reflection","published":1,"updated":"2024-12-14T02:35:39.996Z","comments":1,"layout":"post","photos":[],"_id":"cm5ht44vr002i3457f0ld1hf6","content":"<h1 id=\"什么是屏幕空间反射\"><a href=\"#什么是屏幕空间反射\" class=\"headerlink\" title=\"什么是屏幕空间反射\"></a>什么是屏幕空间反射</h1><p>在前面的文章的一些配图中，其实已经揭露了之前在<a href=\"https://github.com/ruochenhua/KongEngine\">KongEngine</a>中实现的一个不小的功能点，就是<strong>屏幕空间反射（screen space reflection）</strong>。加入了屏幕空间反射能力之后，在一些光滑和带有反射材质的表面上，能够实现不错的反射效果。</p>\n<p><img src=\"/2024/12/10/screen-space-reflection/screen-space-reflection-in-kong.png\" alt=\"KongEngine中的屏幕空间反射效果\"></p>\n<p>屏幕空间反射（后简称<strong>SSR</strong>）是一种在实时渲染中用于模拟物体表面反射的成熟技术。SSR 的核心原理是在<strong>屏幕空间</strong>中进行光线追踪，以此计算反射效果，而无需像传统方法那样在世界空间或物体空间中进行复杂的光线与场景求交计算。它主要利用屏幕上已有的<strong>深度图</strong>和<strong>法线图</strong>等信息，通过对这些信息的分析和处理，确定反射光线的方向和位置，进而得到反射颜色。</p>\n<p>因为SSR不错的效果表现和相对来说比较低的性能开销，使其被广泛的应用在各个实时渲染领域，包括游戏、虚拟现实、建筑可视化等等。当然SSR的效果其实还不够完美，有很多无法解决的问题，这个在后面也会提到。但是在大多数情况下它的效果都是足够的，属于一个很<strong>高性价比</strong>的方法。</p>\n<h1 id=\"如何实现屏幕空间反射\"><a href=\"#如何实现屏幕空间反射\" class=\"headerlink\" title=\"如何实现屏幕空间反射\"></a>如何实现屏幕空间反射</h1><h2 id=\"屏幕空间反射的实现方法\"><a href=\"#屏幕空间反射的实现方法\" class=\"headerlink\" title=\"屏幕空间反射的实现方法\"></a>屏幕空间反射的实现方法</h2><p>简单概括一下SSR的实现方法：</p>\n<ol>\n<li>对于屏幕上的每个像素，先获取其<strong>深度值</strong>和<strong>法线向量</strong>。</li>\n<li>结合相机参数和屏幕坐标计算出<strong>观察向量</strong>，进而得到<strong>反射向量</strong>。    </li>\n<li>沿着反射向量在屏幕空间进行光线追踪，查找反射光线与场景中其他物体的相交点，以获取反射光线的颜色，最终将反射颜色与场景的原始颜色进行合成，得到带有反射效果的最终渲染结果。</li>\n</ol>\n<p>我们对于上面1、2两步应该已经不陌生了，毕竟我们在前面的文章就介绍了KongEngine接入<a href=\"https://ruochenhua.github.io/2024/10/19/defer-render/\">延迟渲染</a>的能力，在G-Buffer中我们已经存储了屏幕空间的各种相关数据，包括深度值和法线向量。有了这些数据，按照第2点计算反射向量也是很顺理成章的事情。</p>\n<p>对应的代码如下：</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c++\"><span class=\"hljs-comment\">// 将延迟渲染保存的数据传给SSR shader</span><br><span class=\"hljs-function\"><span class=\"hljs-type\">void</span> <span class=\"hljs-title\">CRender::SSReflectionRender</span><span class=\"hljs-params\">()</span> <span class=\"hljs-type\">const</span></span><br><span class=\"hljs-function\"></span>&#123;\t<br>\t<span class=\"hljs-comment\">// scene normal：defer_buffer_.g_normal_</span><br>\t<span class=\"hljs-comment\">// scene reflection mask: defer_buffer_.g_orm_</span><br>\t<span class=\"hljs-comment\">// scene position: defer_buffer_.g_position_</span><br>\t<span class=\"hljs-comment\">// scene depth存在于normal贴图的w分量上</span><br>\tssreflection_shader-&gt;<span class=\"hljs-built_in\">Use</span>();<br>\t<span class=\"hljs-built_in\">glActiveTexture</span>(GL_TEXTURE0);<br>\t<span class=\"hljs-built_in\">glBindTexture</span>(GL_TEXTURE_2D, defer_buffer_.g_position_);<br>\t<span class=\"hljs-built_in\">glActiveTexture</span>(GL_TEXTURE0 + <span class=\"hljs-number\">1</span>);<br>\t<span class=\"hljs-built_in\">glBindTexture</span>(GL_TEXTURE_2D, defer_buffer_.g_normal_);<br>\t<span class=\"hljs-built_in\">glActiveTexture</span>(GL_TEXTURE0 + <span class=\"hljs-number\">2</span>);<br>\t<span class=\"hljs-comment\">// 用给后处理的texture作为scene color</span><br>\t<span class=\"hljs-built_in\">glBindTexture</span>(GL_TEXTURE_2D, post_process.screen_quad_texture[<span class=\"hljs-number\">0</span>]);<br>\t<span class=\"hljs-built_in\">glActiveTexture</span>(GL_TEXTURE0 + <span class=\"hljs-number\">3</span>);<br>\t<span class=\"hljs-built_in\">glBindTexture</span>(GL_TEXTURE_2D, defer_buffer_.g_orm_);<br>\t<br>\tquad_shape-&gt;<span class=\"hljs-built_in\">Draw</span>();<br>&#125;<br></code></pre></td></tr></table></figure>\n\n\n<p>那么SSR的关键步骤，其实就在第三步，也是需要理解的重点部分。</p>\n<h2 id=\"获得反射的颜色\"><a href=\"#获得反射的颜色\" class=\"headerlink\" title=\"获得反射的颜色\"></a>获得反射的颜色</h2><p><img src=\"/2024/12/10/screen-space-reflection/ssr-step3.gif\" alt=\"SSR计算反射向量\"></p>\n<p>上面这张图大致描述了第3步的状态。图片中<strong>蓝色</strong>的向量代表了从相机向场景中的每个像素发射的观察向量，<strong>绿色</strong>的向量代表了场景中的法线向量，根据观察向量和法线向量，我们能够计算出反射向量，也就是图片中的<strong>红色</strong>向量。</p>\n<p>我们需要得到的反射结果的颜色，基于反射向量和渲染场景中的其他物体的相交结果，这个是通过在<em>屏幕空间进行步近，判断步近后的坐标深度和深度缓存中存储的物体深度是否相交</em>来得到的。如果有相交结果，则该像素的反射颜色就是相交处的场景颜色，若超出步近范围（会预先设置一个步近长度或者步数的范围），则改点没有反射需要处理。</p>\n<p><img src=\"/2024/12/10/screen-space-reflection/ssr-step4.gif\" alt=\"反射向量步近\"></p>\n<p>这个原理是非常简单易懂的，下面是这段逻辑的大致代码：</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-type\">void</span> main()<br>&#123;<br>    <span class=\"hljs-type\">vec2</span> tex_size = <span class=\"hljs-built_in\">textureSize</span>(scene_position, <span class=\"hljs-number\">0</span>).xy;<br>    <span class=\"hljs-type\">vec2</span> tex_uv = <span class=\"hljs-built_in\">gl_FragCoord</span>.xy / tex_size;<br><br>    <span class=\"hljs-comment\">// 材质相关的参数</span><br>    <span class=\"hljs-type\">vec4</span> orm = <span class=\"hljs-built_in\">texture</span>(orm_texture, TexCoords);<br>    <span class=\"hljs-type\">float</span> roughness = orm.y;<br>    <span class=\"hljs-type\">float</span> metallic = orm.z;<br>    <span class=\"hljs-comment\">// 颜色信息</span><br>    <span class=\"hljs-type\">vec4</span> s_color = <span class=\"hljs-built_in\">texture</span>(scene_color, TexCoords);<br>    FragColor = s_color;<br><br>    <span class=\"hljs-comment\">// 深度和法线</span><br>    <span class=\"hljs-type\">vec4</span> normal_depth = <span class=\"hljs-built_in\">texture</span>(scene_normal, TexCoords);<br>    <span class=\"hljs-type\">vec3</span> world_normal = <span class=\"hljs-built_in\">normalize</span>(normal_depth.xyz + randVec3(<span class=\"hljs-built_in\">fract</span>(TexCoords.x*<span class=\"hljs-number\">12.345</span>)*<span class=\"hljs-built_in\">sin</span>(TexCoords.y)*<span class=\"hljs-number\">9876.31</span>)*<span class=\"hljs-number\">0.2</span>*roughness);<br><br>    ...<br><br>&#125;<br></code></pre></td></tr></table></figure>\n\n<p>上面这段代码是将gbuffer中的信息读出来，包括前面讲到的几个部分。其中法线信息world_normal和材质的粗糙度做了一个随机方向的叠加，可以稍微增加反射效果的粗糙感。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\">&#123;<br>    ...<br><br>    <span class=\"hljs-comment\">// 远近平面</span><br>    <span class=\"hljs-type\">vec2</span> near_far = matrix_ubo.near_far.xy;<br>    <span class=\"hljs-type\">vec3</span> world_pos = <span class=\"hljs-built_in\">texture</span>(scene_position, TexCoords).xyz;<br>    <span class=\"hljs-type\">vec3</span> cam_pos = matrix_ubo.cam_pos.xyz;<br><br>    <span class=\"hljs-type\">mat4</span> projection = matrix_ubo.projection;<br>    <span class=\"hljs-type\">mat4</span> view = matrix_ubo.view;<br>    <span class=\"hljs-type\">mat4</span> vp = projection * view;    <span class=\"hljs-comment\">// 世界坐标到裁切坐标的转换矩阵</span><br><br>    <span class=\"hljs-type\">vec3</span> view_dir = <span class=\"hljs-built_in\">normalize</span>(world_pos-cam_pos);<br>    <span class=\"hljs-type\">vec3</span> rd = <span class=\"hljs-built_in\">normalize</span>(<span class=\"hljs-built_in\">reflect</span>(view_dir, world_normal));<br>    <br>    <span class=\"hljs-type\">float</span> resolution = <span class=\"hljs-number\">0.5</span>;<br><br>    <span class=\"hljs-type\">float</span> max_step_dist = <span class=\"hljs-number\">5.0</span>;        <br>    <span class=\"hljs-type\">vec3</span> start_pos_world = world_pos  + rd*<span class=\"hljs-number\">0.1</span>;<br>    <span class=\"hljs-type\">vec3</span> end_pos_world = world_pos + max_step_dist*rd;<br><br>    <span class=\"hljs-comment\">// 在屏幕空间上的从起始点到结束点的坐标</span><br>    <span class=\"hljs-type\">vec4</span> start_clip = vp * <span class=\"hljs-type\">vec4</span>(start_pos_world, <span class=\"hljs-number\">1.0</span>);<br>    <span class=\"hljs-type\">vec4</span> end_clip   = vp * <span class=\"hljs-type\">vec4</span>(end_pos_world, <span class=\"hljs-number\">1.0</span>);<br><br>    ...<br>&#125;<br><br></code></pre></td></tr></table></figure>\n\n<p>在上面的代码，我们计算出了反射向量rd，同时也为步进设定了一个范围max_step_dist，得到了反射的步进区间，接下来就是进行步进的操作了。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\">&#123;<br>    ...<br>    <br>    <span class=\"hljs-comment\">// 步进的步数</span><br>    <span class=\"hljs-type\">int</span> step_count = <span class=\"hljs-number\">32</span>;<br>    <span class=\"hljs-type\">int</span> sample_count = step_count;<br>    <span class=\"hljs-type\">float</span> delta = <span class=\"hljs-number\">1.0</span> / sample_count;   <span class=\"hljs-comment\">// 如果sample count为10，则delta采样为总共的1/10</span><br><br>    <span class=\"hljs-type\">vec4</span> reflect_color = <span class=\"hljs-type\">vec4</span>(<span class=\"hljs-number\">0.0</span>);<br>    <span class=\"hljs-type\">vec3</span> cam_pos = matrix_ubo.cam_pos.xyz;<br>    <span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">int</span> i = <span class=\"hljs-number\">0</span>; i &lt; sample_count; i++)<br>    &#123;<br>        <span class=\"hljs-type\">float</span> sample_t = i*delta;<br>        <br>        <span class=\"hljs-comment\">// 步进到达处的屏幕空间uv</span><br>        <span class=\"hljs-type\">vec2</span> uv = <span class=\"hljs-type\">vec2</span>(<span class=\"hljs-number\">0</span>);<br><br>        <span class=\"hljs-keyword\">if</span>(campareDepth(start_clip, end_clip, start_pos_world, end_pos_world, sample_t, uv))<br>        &#123;<br>            reflect_color = <span class=\"hljs-built_in\">texture</span>(scene_color, uv);        <br>            <span class=\"hljs-keyword\">break</span>;<br>        &#125;<br>    &#125;<br><br>    FragColor = reflect_color*metallic;<br>&#125;<br><br></code></pre></td></tr></table></figure>\n\n<p>上面的步进代码中，根据设定好的步进步数迭代，campareDepth函数中将当前位置的深度和深度缓存中的数据作对比，若当前深度大于缓存中的值，则代表击中并返回对应的屏幕空间贴图对应的uv值。</p>\n<p>最后反射的颜色和金属度相乘，金属度越高的材质反射也是越高的。在场景渲染的最后，将反射颜色和场景实际的颜色结合，就得到了基本的反射效果了。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\">FragColor = <span class=\"hljs-built_in\">texture</span>(scene_texture, TexCoords);<br><span class=\"hljs-type\">vec4</span> reflection_color = <span class=\"hljs-built_in\">texture</span>(reflection_texture, TexCoords);<br><br>FragColor.rgb += reflection_color.rgb * reflection_color.a;<br></code></pre></td></tr></table></figure>\n\n<p><img src=\"/2024/12/10/screen-space-reflection/ssr_normal_s32.png\" alt=\"SSR效果:sample数32\"><br>上面是采样步数为32步时，得到的反射效果。可以看到反射效果确实出来了，但是条纹效果太过于明显。我们可以提高采样的精度，将sample的数量改为128后可以得到明显改善的结果，如下图。</p>\n<p><img src=\"/2024/12/10/screen-space-reflection/ssr_normal_s128.png\" alt=\"SSR效果：sample数128\"></p>\n<h1 id=\"屏幕空间反射的优化\"><a href=\"#屏幕空间反射的优化\" class=\"headerlink\" title=\"屏幕空间反射的优化\"></a>屏幕空间反射的优化</h1><p>现在我们已经有了基础的反射效果了，但是我们还是不满足不是吗。单纯提升采样精度确实能得到不错的效果，但是始终还是要考虑实际的性能的。那么有什么方法可以优化SSR的表现呢，下面会做一部分简单的介绍。</p>\n<h2 id=\"粗晒和精筛\"><a href=\"#粗晒和精筛\" class=\"headerlink\" title=\"粗晒和精筛\"></a>粗晒和精筛</h2><p>在上面的采样处理中，我们通过步进迭代获取到了深度超过gbuffer中的深度的位置。为了弥补采样步数不足，我们可以将采样过程分为两部分：首先是粗筛，用较低的采样精度获取到大致的区间；然后再利用二分法或者其他方法在大致区间内进行二次筛选。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\">&#123;<br>    ...<br><br>    <span class=\"hljs-type\">float</span> sample_t = i*delta;<br>    <span class=\"hljs-comment\">// 线性插值找到当前采样的屏幕空间的点</span><br>    <span class=\"hljs-type\">vec2</span> uv = <span class=\"hljs-type\">vec2</span>(<span class=\"hljs-number\">0</span>);<br><br>    <span class=\"hljs-keyword\">if</span>(campareDepth(start_clip, end_clip, start_pos_world, end_pos_world, sample_t, uv))<br>    &#123;<br>        reflect_color = <span class=\"hljs-built_in\">texture</span>(scene_color, uv);<br>        <span class=\"hljs-type\">int</span> split_count = <span class=\"hljs-number\">10</span>;<br>        <span class=\"hljs-type\">float</span> i_divide_pos = <span class=\"hljs-number\">0.5</span>;<br>        <span class=\"hljs-keyword\">while</span>(split_count &gt; <span class=\"hljs-number\">0</span>)<br>        &#123;<br>            <span class=\"hljs-keyword\">if</span>(campareDepth(start_clip, end_clip, start_pos_world, end_pos_world, (<span class=\"hljs-type\">float</span>(i)-i_divide_pos)*delta, uv))<br>            &#123;<br>                i_divide_pos += i_divide_pos*<span class=\"hljs-number\">0.5</span>;<br>            &#125;<br>            <span class=\"hljs-keyword\">else</span><br>            &#123;<br>                i_divide_pos -= i_divide_pos*<span class=\"hljs-number\">0.5</span>;<br>            &#125;<br>            split_count--;<br>        &#125;<br><br>        reflect_color = <span class=\"hljs-built_in\">texture</span>(scene_color, uv);<br>        <span class=\"hljs-keyword\">break</span>;<br>    &#125;<br><br>    ...<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<p>下面是这种方法的结果，可以看到效果是稍微好了些，不过如果需要再进一步的话，还是避免不了要提升采样精度。<br><img src=\"/2024/12/10/screen-space-reflection/ssr_sample_twice.png\" alt=\"SSR二次采样\"></p>\n<h2 id=\"屏幕空间步进\"><a href=\"#屏幕空间步进\" class=\"headerlink\" title=\"屏幕空间步进\"></a>屏幕空间步进</h2><p>目前比较常用的优化方法，是把三维空间做光线步近替换为在屏幕空间做光线步近。<br>传统的在三维空间做光线步近，很难避免采样不均的问题，如果我们是以三维空间的的步近长度作为采样依据的话，会出现下面的问题。其中蓝色小格子代表的是像素，红色的点对应的是每个采样点对应的像素位置。</p>\n<p><img src=\"/2024/12/10/screen-space-reflection/ssr_over_sample.png\" alt=\"SSR过采样\"><br>当反射角度相对来说比较大，很容易出现非常多采样点对应同一个像素，进行了大量的重复运算。</p>\n<p><img src=\"/2024/12/10/screen-space-reflection/ssr_under_sample.png\" alt=\"SSR欠采样\"></p>\n<p>反射角度过小的时候，有很容易出现跳过中间某些像素的情况，出现了欠采样的情况。这也是我们上面的反射效果出现了带状的原因。</p>\n<p>在<a href=\"https://jcgt.org/published/0003/04/04/\">Efficient GPU Screen-Space Ray Tracing</a>这篇文章提出了在屏幕空间采样的观点。通过将采样点的选择放在屏幕空间，实现采样点连续且分布均匀的效果。每个采样点不会进行重复计算，也保证了性能的最优。<br><img src=\"/2024/12/10/screen-space-reflection/ssr_ss_sample.png\" alt=\"SSR屏幕空间采样方法\"></p>\n<p>为了实现屏幕看见步近，代码需要做一些修改：</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\">&#123;<br>    ...<br><br>    <span class=\"hljs-type\">vec3</span> start_pos_world = world_pos  + rd*<span class=\"hljs-number\">0.1</span>;<br>    <span class=\"hljs-type\">vec3</span> end_pos_world = world_pos + max_step_dist*rd;<br>    <span class=\"hljs-comment\">// 在屏幕空间上的从起始点到结束点的坐标[0, resolution]</span><br>    <span class=\"hljs-type\">vec4</span> start_clip = vp * <span class=\"hljs-type\">vec4</span>(start_pos_world, <span class=\"hljs-number\">1.0</span>);<br>    <span class=\"hljs-type\">vec4</span> end_clip   = vp * <span class=\"hljs-type\">vec4</span>(end_pos_world, <span class=\"hljs-number\">1.0</span>);<br>    <span class=\"hljs-comment\">// 在屏幕空间进行光线步进</span><br>    <span class=\"hljs-comment\">// 起始点和结束点</span><br>    <span class=\"hljs-type\">vec3</span> start_ndc  = start_clip.xyz / start_clip.w;<br>    <span class=\"hljs-type\">vec3</span> end_ndc    = end_clip.xyz / end_clip.w;<br>    <span class=\"hljs-type\">vec3</span> ndc_diff = end_ndc - start_ndc;<br><br>    <span class=\"hljs-comment\">// ndc-&gt;屏幕坐标 [0, resolution.xy]</span><br>    <span class=\"hljs-type\">vec3</span> start_screen  = <span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">0</span>);<br>    start_screen.xy = (start_ndc.xy + <span class=\"hljs-number\">1</span>) / <span class=\"hljs-number\">2</span> * tex_size;<br>    start_screen.z = (near_far.y - near_far.x) * <span class=\"hljs-number\">0.5</span> * start_ndc.z + (near_far.x + near_far.y) * <span class=\"hljs-number\">0.5</span>;<br><br>    <span class=\"hljs-type\">vec3</span> end_screen    = <span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">0</span>);    <br>    end_screen.xy = (end_ndc.xy + <span class=\"hljs-number\">1</span>) / <span class=\"hljs-number\">2</span> * tex_size;<br>    end_screen.z = (near_far.y - near_far.x) * <span class=\"hljs-number\">0.5</span> * end_ndc.z + (near_far.x + near_far.y) * <span class=\"hljs-number\">0.5</span>;<br><br>    <span class=\"hljs-type\">int</span> step_count = <span class=\"hljs-number\">32</span>;<br><br>    <span class=\"hljs-type\">vec3</span> screen_diff = end_screen - start_screen;<br>    <span class=\"hljs-type\">int</span> sample_count = <span class=\"hljs-type\">int</span>(<span class=\"hljs-built_in\">max</span>(<span class=\"hljs-built_in\">abs</span>(screen_diff.x), <span class=\"hljs-built_in\">abs</span>(screen_diff.y)) * resolution) ; <span class=\"hljs-comment\">// 大于1</span><br><br>    sample_count = <span class=\"hljs-built_in\">min</span>(sample_count, <span class=\"hljs-number\">64</span>);<br>    <span class=\"hljs-type\">vec3</span> delta_screen = screen_diff / <span class=\"hljs-type\">float</span>(sample_count);<br><br>    <span class=\"hljs-comment\">// 如果sample count为10，则每次采样的前进的长度为总长度的1/10</span><br>    <span class=\"hljs-type\">float</span> percentage_delta = <span class=\"hljs-number\">1.0</span> / <span class=\"hljs-type\">float</span>(sample_count);<br>    <span class=\"hljs-type\">vec3</span> current_screen = start_screen;<br>    <span class=\"hljs-type\">vec3</span> last_screen = current_screen;<br>    <span class=\"hljs-type\">float</span> current_percentage = <span class=\"hljs-number\">0.0</span>;<br>    <span class=\"hljs-type\">float</span> last_percentage = <span class=\"hljs-number\">0.0</span>;<br><br>    ...<br><br></code></pre></td></tr></table></figure>\n<p>使用屏幕空间步近，前面和原来的差不多，在获取步近的起始点和结束点的时候，需要将坐标转换为屏幕空间的坐标，也就是其中的current_screen和last_screen。</p>\n<p>屏幕空间采样点数和<strong>采样的起始和结束位置的像素差值</strong>有关，所以和渲染输出的分辨率也是相关的。如果渲染分辨率越高，其对应所需要的采样点数可能也会增加，这里我们控制在64以内。当然如果起始点和结束点的像素差值较小，对应的采样点数也会变小，也就是对于距离相机很远的位置的采样会减少，在怎么不影响效果的情况下提升性能表现。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\">    ...<br><br>    <span class=\"hljs-type\">vec4</span> reflect_color = <span class=\"hljs-type\">vec4</span>(<span class=\"hljs-number\">0.0</span>);<br>    <span class=\"hljs-type\">vec3</span> cam_pos = matrix_ubo.cam_pos.xyz;<br>    <span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">int</span> i = <span class=\"hljs-number\">0</span>; i &lt; sample_count; i++)<br>    &#123;<br>        <span class=\"hljs-comment\">// 采样当前屏幕上的点对应场景世界空间坐标的位置</span><br>        <span class=\"hljs-type\">vec2</span> uv = current_screen.xy / tex_size;<br><br>        <span class=\"hljs-comment\">// 转换为贴图坐标，检查越界</span><br>        <span class=\"hljs-keyword\">if</span>(uv.x &lt; <span class=\"hljs-number\">0.0</span> || uv.y &lt; <span class=\"hljs-number\">0.0</span> || uv.x &gt; <span class=\"hljs-number\">1.0</span> || uv.y &gt; <span class=\"hljs-number\">1.0</span>)<br>        &#123;<br>            <span class=\"hljs-keyword\">continue</span>;<br>        &#125;<br>        <span class=\"hljs-keyword\">else</span><br>        &#123;<br>            <span class=\"hljs-comment\">// 延迟渲染存储的屏幕对应的世界位置</span><br>            <span class=\"hljs-type\">vec3</span> sample_world = <span class=\"hljs-built_in\">texture</span>(scene_position, uv).xyz;<br>            <br>            <span class=\"hljs-type\">vec4</span> sample_ndc = <span class=\"hljs-type\">vec4</span>(<span class=\"hljs-built_in\">mix</span>(start_ndc, end_ndc, current_percentage), <span class=\"hljs-number\">1.0</span>);<br>            <span class=\"hljs-keyword\">if</span>(compareDepth(sample_ndc, uv))<br>            &#123;<br>                <span class=\"hljs-comment\">// 初筛后再二分法检查</span><br>                <span class=\"hljs-type\">int</span> split_count = <span class=\"hljs-number\">5</span>;<br>                <span class=\"hljs-keyword\">while</span>(split_count &gt; <span class=\"hljs-number\">0</span>)<br>                &#123;<br>                    <span class=\"hljs-type\">vec3</span> mid_screen = (last_screen + current_screen) * <span class=\"hljs-number\">0.5</span>;<br>                    <span class=\"hljs-type\">float</span> mid_percentage = (last_percentage + current_percentage) * <span class=\"hljs-number\">0.5</span>;<br>                    <span class=\"hljs-type\">vec4</span> mid_ndc = <span class=\"hljs-type\">vec4</span>(<span class=\"hljs-built_in\">mix</span>(start_ndc, end_ndc, mid_percentage), <span class=\"hljs-number\">1.0</span>);<br>                    uv = mid_screen.xy / tex_size;<br>                    <span class=\"hljs-keyword\">if</span>(compareDepth(mid_ndc, uv))<br>                    &#123;<br>                        current_screen = mid_screen;<br>                    &#125;<br>                    <span class=\"hljs-keyword\">else</span><br>                    &#123;<br>                        last_screen = mid_screen;<br>                    &#125;<br>                    split_count--;<br>                &#125;<br><br>                reflect_color = <span class=\"hljs-built_in\">texture</span>(scene_color, uv);<br>                <span class=\"hljs-keyword\">break</span>;<br>            &#125;<br><br>            last_screen = current_screen;<br>            last_percentage = current_percentage;<br>            current_screen += delta_screen;<br>            current_percentage += percentage_delta;<br>        &#125;<br>    &#125;<br><br>    FragColor = reflect_color*metallic;<br><br>&#125;<br><br></code></pre></td></tr></table></figure>\n<p>这里在屏幕空间采样还配合了之前的粗筛和精筛的方法，下面是使用屏幕空间采样的表现。可以看到条纹的状况被极大的缓解了。</p>\n<p><img src=\"/2024/12/10/screen-space-reflection/ssr_result.png\" alt=\"SSR屏幕空间采样结果\"></p>\n<p>应用在实际场景中，SSR的效果能比较明显的提升渲染质感。<br><img src=\"/2024/12/10/screen-space-reflection/ssr_mugshot.png\" alt=\"SSR实际应用\"></p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>SSR是一种计算场景的反射效果的算法，它基于屏幕空间已有的深度图和法线图等信息，通过计算反射向量，在屏幕空间中进行光线追踪，查找反射光线与场景中其他物体的相交点，获取相交点的颜色作为反射颜色，并与原始颜色合成得到最终渲染结果。</p>\n<p>SSR的优点是计算效率相对较高，能实时反映场景中物体的变化，适用于复杂几何形状和不规则表面，适合大规模的动态场景，无需额外的镜头或几何体。</p>\n<p>当然，SSR也有局限性，它只能反射屏幕上可见的物体，超出屏幕边界的内容无法被反射；反射的物体可能存在失真或错误，尤其是边缘区域；依赖屏幕分辨率，高分辨率下可能对性能有较大影响</p>\n<h2 id=\"SSR的优化改进算法：\"><a href=\"#SSR的优化改进算法：\" class=\"headerlink\" title=\"SSR的优化改进算法：\"></a>SSR的优化改进算法：</h2><h3 id=\"SSSR（Spatially-Separated-Screen-Space-Reflection）\"><a href=\"#SSSR（Spatially-Separated-Screen-Space-Reflection）\" class=\"headerlink\" title=\"SSSR（Spatially Separated Screen Space Reflection）\"></a>SSSR（Spatially Separated Screen Space Reflection）</h3><p>原理：SSSR 是对传统 SSR 技术的一种改进。它主要是基于空间分离的思想来处理屏幕空间反射。传统 SSR 在处理反射时可能会受到屏幕空间限制和采样不足等问题的影响。SSSR 通过将屏幕空间划分为不同的区域，在这些区域内分别进行更精细的反射处理。</p>\n<p>例如，它可以根据场景中物体的距离、重要性或者反射特性等因素，对空间进行划分。对于反射效果比较复杂或者重要的区域，分配更多的资源进行反射计算，而对于相对简单或者不重要的区域，则采用较为简略的计算方式。</p>\n<p>优点：</p>\n<ul>\n<li>提高反射精度：通过对特定区域的精细处理，能够有效提高反射的精度。比如在处理具有高反射率的物体表面或者复杂的光照反射场景时，可以得到更真实、细腻的反射效果。</li>\n<li>优化性能：与传统 SSR 相比，SSSR 能够更合理地分配计算资源。它避免了在整个屏幕空间进行统一标准的反射计算，从而在一定程度上减轻了计算负担，特别是在大规模复杂场景中，可以更好地平衡反射效果和性能。</li>\n</ul>\n<p>局限性：</p>\n<ul>\n<li>空间划分的复杂性：如何合理地划分空间是一个具有挑战性的问题。如果空间划分不合理，可能会导致反射效果出现不自然的边界或者遗漏重要的反射区域。</li>\n<li>增加算法复杂度：空间划分和不同区域的分别处理增加了算法的复杂度。这可能会导致开发和调试的难度增加，并且在某些情况下，可能会引入新的错误或者视觉瑕疵。</li>\n</ul>\n<h3 id=\"Hi-z-SSR（Hierarchical-z-Screen-Space-Reflection）\"><a href=\"#Hi-z-SSR（Hierarchical-z-Screen-Space-Reflection）\" class=\"headerlink\" title=\"Hi-z SSR（Hierarchical - z Screen Space Reflection）\"></a>Hi-z SSR（Hierarchical - z Screen Space Reflection）</h3><p>原理：Hi - z SSR 是利用层次化的深度信息（Hierarchical-z）来改进 SSR。它构建了一个层次化的深度缓冲区，这个缓冲区可以更有效地存储和检索深度信息。在计算反射时，通过这个层次化的结构，可以快速地在不同层次的深度信息中进行搜索和采样。<br>例如，在较高层次的深度信息中，可以快速定位反射光线可能相交的大致区域，然后在较低层次的深度信息中进行更精细的搜索，就像在地图的不同比例尺中查找目标位置一样。这种层次化的搜索方式能够更高效地利用深度信息来计算反射。</p>\n<p>优点：</p>\n<ul>\n<li>高效的深度搜索：层次化的深度搜索大大提高了反射光线与场景相交点的查找效率。尤其是在处理具有深度层次丰富的复杂场景时，能够快速定位反射位置，减少计算时间。</li>\n<li>增强的反射范围：由于能够更好地利用深度信息，Hi-z SSR 可以在一定程度上缓解传统 SSR 中屏幕外反射难以处理的问题。它可以通过层次化的深度结构，对屏幕外部分场景的深度信息进行合理推测和利用，从而扩展反射的有效范围。</li>\n</ul>\n<p>局限性</p>\n<ul>\n<li><p>深度缓冲区的构建成本：构建层次化的深度缓冲区需要额外的存储空间和计算资源来生成和维护。这可能会在一些资源受限的场景或者硬件平台上带来一定的负担。</p>\n</li>\n<li><p>精度与性能的平衡：尽管 Hi-z SSR 提高了搜索效率，但在平衡反射精度和性能方面仍然是一个挑战。在某些情况下，过于追求效率可能会导致反射精度下降，而过度强调精度又可能会使性能开销过大。</p>\n</li>\n</ul>\n<h1 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h1><p><a href=\"https://lettier.github.io/3d-game-shaders-for-beginners/screen-space-reflection.html\">https://lettier.github.io/3d-game-shaders-for-beginners/screen-space-reflection.html</a></p>\n<p><a href=\"https://jcgt.org/published/0003/04/04/\">https://jcgt.org/published/0003/04/04/</a></p>\n<p><a href=\"https://blog.csdn.net/qjh5606/article/details/120102582?ops_request_misc=%257B%2522request%255Fid%2522%253A%25225a1434f7df5d388dc4166f4877eb172b%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=5a1434f7df5d388dc4166f4877eb172b&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-120102582-null-null.142%5Ev100%5Econtrol&utm_term=Efficient%20GPU%20Screen-Space%20Ray%20Tracing&spm=1018.2226.3001.4187\">https://blog.csdn.net/qjh5606/article/details/120102582?ops_request_misc=%257B%2522request%255Fid%2522%253A%25225a1434f7df5d388dc4166f4877eb172b%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=5a1434f7df5d388dc4166f4877eb172b&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-120102582-null-null.142^v100^control&amp;utm_term=Efficient%20GPU%20Screen-Space%20Ray%20Tracing&amp;spm=1018.2226.3001.4187</a></p>\n","excerpt":"","more":"<h1 id=\"什么是屏幕空间反射\"><a href=\"#什么是屏幕空间反射\" class=\"headerlink\" title=\"什么是屏幕空间反射\"></a>什么是屏幕空间反射</h1><p>在前面的文章的一些配图中，其实已经揭露了之前在<a href=\"https://github.com/ruochenhua/KongEngine\">KongEngine</a>中实现的一个不小的功能点，就是<strong>屏幕空间反射（screen space reflection）</strong>。加入了屏幕空间反射能力之后，在一些光滑和带有反射材质的表面上，能够实现不错的反射效果。</p>\n<p><img src=\"/2024/12/10/screen-space-reflection/screen-space-reflection-in-kong.png\" alt=\"KongEngine中的屏幕空间反射效果\"></p>\n<p>屏幕空间反射（后简称<strong>SSR</strong>）是一种在实时渲染中用于模拟物体表面反射的成熟技术。SSR 的核心原理是在<strong>屏幕空间</strong>中进行光线追踪，以此计算反射效果，而无需像传统方法那样在世界空间或物体空间中进行复杂的光线与场景求交计算。它主要利用屏幕上已有的<strong>深度图</strong>和<strong>法线图</strong>等信息，通过对这些信息的分析和处理，确定反射光线的方向和位置，进而得到反射颜色。</p>\n<p>因为SSR不错的效果表现和相对来说比较低的性能开销，使其被广泛的应用在各个实时渲染领域，包括游戏、虚拟现实、建筑可视化等等。当然SSR的效果其实还不够完美，有很多无法解决的问题，这个在后面也会提到。但是在大多数情况下它的效果都是足够的，属于一个很<strong>高性价比</strong>的方法。</p>\n<h1 id=\"如何实现屏幕空间反射\"><a href=\"#如何实现屏幕空间反射\" class=\"headerlink\" title=\"如何实现屏幕空间反射\"></a>如何实现屏幕空间反射</h1><h2 id=\"屏幕空间反射的实现方法\"><a href=\"#屏幕空间反射的实现方法\" class=\"headerlink\" title=\"屏幕空间反射的实现方法\"></a>屏幕空间反射的实现方法</h2><p>简单概括一下SSR的实现方法：</p>\n<ol>\n<li>对于屏幕上的每个像素，先获取其<strong>深度值</strong>和<strong>法线向量</strong>。</li>\n<li>结合相机参数和屏幕坐标计算出<strong>观察向量</strong>，进而得到<strong>反射向量</strong>。    </li>\n<li>沿着反射向量在屏幕空间进行光线追踪，查找反射光线与场景中其他物体的相交点，以获取反射光线的颜色，最终将反射颜色与场景的原始颜色进行合成，得到带有反射效果的最终渲染结果。</li>\n</ol>\n<p>我们对于上面1、2两步应该已经不陌生了，毕竟我们在前面的文章就介绍了KongEngine接入<a href=\"https://ruochenhua.github.io/2024/10/19/defer-render/\">延迟渲染</a>的能力，在G-Buffer中我们已经存储了屏幕空间的各种相关数据，包括深度值和法线向量。有了这些数据，按照第2点计算反射向量也是很顺理成章的事情。</p>\n<p>对应的代码如下：</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs c++\"><span class=\"hljs-comment\">// 将延迟渲染保存的数据传给SSR shader</span><br><span class=\"hljs-function\"><span class=\"hljs-type\">void</span> <span class=\"hljs-title\">CRender::SSReflectionRender</span><span class=\"hljs-params\">()</span> <span class=\"hljs-type\">const</span></span><br><span class=\"hljs-function\"></span>&#123;\t<br>\t<span class=\"hljs-comment\">// scene normal：defer_buffer_.g_normal_</span><br>\t<span class=\"hljs-comment\">// scene reflection mask: defer_buffer_.g_orm_</span><br>\t<span class=\"hljs-comment\">// scene position: defer_buffer_.g_position_</span><br>\t<span class=\"hljs-comment\">// scene depth存在于normal贴图的w分量上</span><br>\tssreflection_shader-&gt;<span class=\"hljs-built_in\">Use</span>();<br>\t<span class=\"hljs-built_in\">glActiveTexture</span>(GL_TEXTURE0);<br>\t<span class=\"hljs-built_in\">glBindTexture</span>(GL_TEXTURE_2D, defer_buffer_.g_position_);<br>\t<span class=\"hljs-built_in\">glActiveTexture</span>(GL_TEXTURE0 + <span class=\"hljs-number\">1</span>);<br>\t<span class=\"hljs-built_in\">glBindTexture</span>(GL_TEXTURE_2D, defer_buffer_.g_normal_);<br>\t<span class=\"hljs-built_in\">glActiveTexture</span>(GL_TEXTURE0 + <span class=\"hljs-number\">2</span>);<br>\t<span class=\"hljs-comment\">// 用给后处理的texture作为scene color</span><br>\t<span class=\"hljs-built_in\">glBindTexture</span>(GL_TEXTURE_2D, post_process.screen_quad_texture[<span class=\"hljs-number\">0</span>]);<br>\t<span class=\"hljs-built_in\">glActiveTexture</span>(GL_TEXTURE0 + <span class=\"hljs-number\">3</span>);<br>\t<span class=\"hljs-built_in\">glBindTexture</span>(GL_TEXTURE_2D, defer_buffer_.g_orm_);<br>\t<br>\tquad_shape-&gt;<span class=\"hljs-built_in\">Draw</span>();<br>&#125;<br></code></pre></td></tr></table></figure>\n\n\n<p>那么SSR的关键步骤，其实就在第三步，也是需要理解的重点部分。</p>\n<h2 id=\"获得反射的颜色\"><a href=\"#获得反射的颜色\" class=\"headerlink\" title=\"获得反射的颜色\"></a>获得反射的颜色</h2><p><img src=\"/2024/12/10/screen-space-reflection/ssr-step3.gif\" alt=\"SSR计算反射向量\"></p>\n<p>上面这张图大致描述了第3步的状态。图片中<strong>蓝色</strong>的向量代表了从相机向场景中的每个像素发射的观察向量，<strong>绿色</strong>的向量代表了场景中的法线向量，根据观察向量和法线向量，我们能够计算出反射向量，也就是图片中的<strong>红色</strong>向量。</p>\n<p>我们需要得到的反射结果的颜色，基于反射向量和渲染场景中的其他物体的相交结果，这个是通过在<em>屏幕空间进行步近，判断步近后的坐标深度和深度缓存中存储的物体深度是否相交</em>来得到的。如果有相交结果，则该像素的反射颜色就是相交处的场景颜色，若超出步近范围（会预先设置一个步近长度或者步数的范围），则改点没有反射需要处理。</p>\n<p><img src=\"/2024/12/10/screen-space-reflection/ssr-step4.gif\" alt=\"反射向量步近\"></p>\n<p>这个原理是非常简单易懂的，下面是这段逻辑的大致代码：</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\"><span class=\"hljs-type\">void</span> main()<br>&#123;<br>    <span class=\"hljs-type\">vec2</span> tex_size = <span class=\"hljs-built_in\">textureSize</span>(scene_position, <span class=\"hljs-number\">0</span>).xy;<br>    <span class=\"hljs-type\">vec2</span> tex_uv = <span class=\"hljs-built_in\">gl_FragCoord</span>.xy / tex_size;<br><br>    <span class=\"hljs-comment\">// 材质相关的参数</span><br>    <span class=\"hljs-type\">vec4</span> orm = <span class=\"hljs-built_in\">texture</span>(orm_texture, TexCoords);<br>    <span class=\"hljs-type\">float</span> roughness = orm.y;<br>    <span class=\"hljs-type\">float</span> metallic = orm.z;<br>    <span class=\"hljs-comment\">// 颜色信息</span><br>    <span class=\"hljs-type\">vec4</span> s_color = <span class=\"hljs-built_in\">texture</span>(scene_color, TexCoords);<br>    FragColor = s_color;<br><br>    <span class=\"hljs-comment\">// 深度和法线</span><br>    <span class=\"hljs-type\">vec4</span> normal_depth = <span class=\"hljs-built_in\">texture</span>(scene_normal, TexCoords);<br>    <span class=\"hljs-type\">vec3</span> world_normal = <span class=\"hljs-built_in\">normalize</span>(normal_depth.xyz + randVec3(<span class=\"hljs-built_in\">fract</span>(TexCoords.x*<span class=\"hljs-number\">12.345</span>)*<span class=\"hljs-built_in\">sin</span>(TexCoords.y)*<span class=\"hljs-number\">9876.31</span>)*<span class=\"hljs-number\">0.2</span>*roughness);<br><br>    ...<br><br>&#125;<br></code></pre></td></tr></table></figure>\n\n<p>上面这段代码是将gbuffer中的信息读出来，包括前面讲到的几个部分。其中法线信息world_normal和材质的粗糙度做了一个随机方向的叠加，可以稍微增加反射效果的粗糙感。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\">&#123;<br>    ...<br><br>    <span class=\"hljs-comment\">// 远近平面</span><br>    <span class=\"hljs-type\">vec2</span> near_far = matrix_ubo.near_far.xy;<br>    <span class=\"hljs-type\">vec3</span> world_pos = <span class=\"hljs-built_in\">texture</span>(scene_position, TexCoords).xyz;<br>    <span class=\"hljs-type\">vec3</span> cam_pos = matrix_ubo.cam_pos.xyz;<br><br>    <span class=\"hljs-type\">mat4</span> projection = matrix_ubo.projection;<br>    <span class=\"hljs-type\">mat4</span> view = matrix_ubo.view;<br>    <span class=\"hljs-type\">mat4</span> vp = projection * view;    <span class=\"hljs-comment\">// 世界坐标到裁切坐标的转换矩阵</span><br><br>    <span class=\"hljs-type\">vec3</span> view_dir = <span class=\"hljs-built_in\">normalize</span>(world_pos-cam_pos);<br>    <span class=\"hljs-type\">vec3</span> rd = <span class=\"hljs-built_in\">normalize</span>(<span class=\"hljs-built_in\">reflect</span>(view_dir, world_normal));<br>    <br>    <span class=\"hljs-type\">float</span> resolution = <span class=\"hljs-number\">0.5</span>;<br><br>    <span class=\"hljs-type\">float</span> max_step_dist = <span class=\"hljs-number\">5.0</span>;        <br>    <span class=\"hljs-type\">vec3</span> start_pos_world = world_pos  + rd*<span class=\"hljs-number\">0.1</span>;<br>    <span class=\"hljs-type\">vec3</span> end_pos_world = world_pos + max_step_dist*rd;<br><br>    <span class=\"hljs-comment\">// 在屏幕空间上的从起始点到结束点的坐标</span><br>    <span class=\"hljs-type\">vec4</span> start_clip = vp * <span class=\"hljs-type\">vec4</span>(start_pos_world, <span class=\"hljs-number\">1.0</span>);<br>    <span class=\"hljs-type\">vec4</span> end_clip   = vp * <span class=\"hljs-type\">vec4</span>(end_pos_world, <span class=\"hljs-number\">1.0</span>);<br><br>    ...<br>&#125;<br><br></code></pre></td></tr></table></figure>\n\n<p>在上面的代码，我们计算出了反射向量rd，同时也为步进设定了一个范围max_step_dist，得到了反射的步进区间，接下来就是进行步进的操作了。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\">&#123;<br>    ...<br>    <br>    <span class=\"hljs-comment\">// 步进的步数</span><br>    <span class=\"hljs-type\">int</span> step_count = <span class=\"hljs-number\">32</span>;<br>    <span class=\"hljs-type\">int</span> sample_count = step_count;<br>    <span class=\"hljs-type\">float</span> delta = <span class=\"hljs-number\">1.0</span> / sample_count;   <span class=\"hljs-comment\">// 如果sample count为10，则delta采样为总共的1/10</span><br><br>    <span class=\"hljs-type\">vec4</span> reflect_color = <span class=\"hljs-type\">vec4</span>(<span class=\"hljs-number\">0.0</span>);<br>    <span class=\"hljs-type\">vec3</span> cam_pos = matrix_ubo.cam_pos.xyz;<br>    <span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">int</span> i = <span class=\"hljs-number\">0</span>; i &lt; sample_count; i++)<br>    &#123;<br>        <span class=\"hljs-type\">float</span> sample_t = i*delta;<br>        <br>        <span class=\"hljs-comment\">// 步进到达处的屏幕空间uv</span><br>        <span class=\"hljs-type\">vec2</span> uv = <span class=\"hljs-type\">vec2</span>(<span class=\"hljs-number\">0</span>);<br><br>        <span class=\"hljs-keyword\">if</span>(campareDepth(start_clip, end_clip, start_pos_world, end_pos_world, sample_t, uv))<br>        &#123;<br>            reflect_color = <span class=\"hljs-built_in\">texture</span>(scene_color, uv);        <br>            <span class=\"hljs-keyword\">break</span>;<br>        &#125;<br>    &#125;<br><br>    FragColor = reflect_color*metallic;<br>&#125;<br><br></code></pre></td></tr></table></figure>\n\n<p>上面的步进代码中，根据设定好的步进步数迭代，campareDepth函数中将当前位置的深度和深度缓存中的数据作对比，若当前深度大于缓存中的值，则代表击中并返回对应的屏幕空间贴图对应的uv值。</p>\n<p>最后反射的颜色和金属度相乘，金属度越高的材质反射也是越高的。在场景渲染的最后，将反射颜色和场景实际的颜色结合，就得到了基本的反射效果了。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\">FragColor = <span class=\"hljs-built_in\">texture</span>(scene_texture, TexCoords);<br><span class=\"hljs-type\">vec4</span> reflection_color = <span class=\"hljs-built_in\">texture</span>(reflection_texture, TexCoords);<br><br>FragColor.rgb += reflection_color.rgb * reflection_color.a;<br></code></pre></td></tr></table></figure>\n\n<p><img src=\"/2024/12/10/screen-space-reflection/ssr_normal_s32.png\" alt=\"SSR效果:sample数32\"><br>上面是采样步数为32步时，得到的反射效果。可以看到反射效果确实出来了，但是条纹效果太过于明显。我们可以提高采样的精度，将sample的数量改为128后可以得到明显改善的结果，如下图。</p>\n<p><img src=\"/2024/12/10/screen-space-reflection/ssr_normal_s128.png\" alt=\"SSR效果：sample数128\"></p>\n<h1 id=\"屏幕空间反射的优化\"><a href=\"#屏幕空间反射的优化\" class=\"headerlink\" title=\"屏幕空间反射的优化\"></a>屏幕空间反射的优化</h1><p>现在我们已经有了基础的反射效果了，但是我们还是不满足不是吗。单纯提升采样精度确实能得到不错的效果，但是始终还是要考虑实际的性能的。那么有什么方法可以优化SSR的表现呢，下面会做一部分简单的介绍。</p>\n<h2 id=\"粗晒和精筛\"><a href=\"#粗晒和精筛\" class=\"headerlink\" title=\"粗晒和精筛\"></a>粗晒和精筛</h2><p>在上面的采样处理中，我们通过步进迭代获取到了深度超过gbuffer中的深度的位置。为了弥补采样步数不足，我们可以将采样过程分为两部分：首先是粗筛，用较低的采样精度获取到大致的区间；然后再利用二分法或者其他方法在大致区间内进行二次筛选。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\">&#123;<br>    ...<br><br>    <span class=\"hljs-type\">float</span> sample_t = i*delta;<br>    <span class=\"hljs-comment\">// 线性插值找到当前采样的屏幕空间的点</span><br>    <span class=\"hljs-type\">vec2</span> uv = <span class=\"hljs-type\">vec2</span>(<span class=\"hljs-number\">0</span>);<br><br>    <span class=\"hljs-keyword\">if</span>(campareDepth(start_clip, end_clip, start_pos_world, end_pos_world, sample_t, uv))<br>    &#123;<br>        reflect_color = <span class=\"hljs-built_in\">texture</span>(scene_color, uv);<br>        <span class=\"hljs-type\">int</span> split_count = <span class=\"hljs-number\">10</span>;<br>        <span class=\"hljs-type\">float</span> i_divide_pos = <span class=\"hljs-number\">0.5</span>;<br>        <span class=\"hljs-keyword\">while</span>(split_count &gt; <span class=\"hljs-number\">0</span>)<br>        &#123;<br>            <span class=\"hljs-keyword\">if</span>(campareDepth(start_clip, end_clip, start_pos_world, end_pos_world, (<span class=\"hljs-type\">float</span>(i)-i_divide_pos)*delta, uv))<br>            &#123;<br>                i_divide_pos += i_divide_pos*<span class=\"hljs-number\">0.5</span>;<br>            &#125;<br>            <span class=\"hljs-keyword\">else</span><br>            &#123;<br>                i_divide_pos -= i_divide_pos*<span class=\"hljs-number\">0.5</span>;<br>            &#125;<br>            split_count--;<br>        &#125;<br><br>        reflect_color = <span class=\"hljs-built_in\">texture</span>(scene_color, uv);<br>        <span class=\"hljs-keyword\">break</span>;<br>    &#125;<br><br>    ...<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<p>下面是这种方法的结果，可以看到效果是稍微好了些，不过如果需要再进一步的话，还是避免不了要提升采样精度。<br><img src=\"/2024/12/10/screen-space-reflection/ssr_sample_twice.png\" alt=\"SSR二次采样\"></p>\n<h2 id=\"屏幕空间步进\"><a href=\"#屏幕空间步进\" class=\"headerlink\" title=\"屏幕空间步进\"></a>屏幕空间步进</h2><p>目前比较常用的优化方法，是把三维空间做光线步近替换为在屏幕空间做光线步近。<br>传统的在三维空间做光线步近，很难避免采样不均的问题，如果我们是以三维空间的的步近长度作为采样依据的话，会出现下面的问题。其中蓝色小格子代表的是像素，红色的点对应的是每个采样点对应的像素位置。</p>\n<p><img src=\"/2024/12/10/screen-space-reflection/ssr_over_sample.png\" alt=\"SSR过采样\"><br>当反射角度相对来说比较大，很容易出现非常多采样点对应同一个像素，进行了大量的重复运算。</p>\n<p><img src=\"/2024/12/10/screen-space-reflection/ssr_under_sample.png\" alt=\"SSR欠采样\"></p>\n<p>反射角度过小的时候，有很容易出现跳过中间某些像素的情况，出现了欠采样的情况。这也是我们上面的反射效果出现了带状的原因。</p>\n<p>在<a href=\"https://jcgt.org/published/0003/04/04/\">Efficient GPU Screen-Space Ray Tracing</a>这篇文章提出了在屏幕空间采样的观点。通过将采样点的选择放在屏幕空间，实现采样点连续且分布均匀的效果。每个采样点不会进行重复计算，也保证了性能的最优。<br><img src=\"/2024/12/10/screen-space-reflection/ssr_ss_sample.png\" alt=\"SSR屏幕空间采样方法\"></p>\n<p>为了实现屏幕看见步近，代码需要做一些修改：</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\">&#123;<br>    ...<br><br>    <span class=\"hljs-type\">vec3</span> start_pos_world = world_pos  + rd*<span class=\"hljs-number\">0.1</span>;<br>    <span class=\"hljs-type\">vec3</span> end_pos_world = world_pos + max_step_dist*rd;<br>    <span class=\"hljs-comment\">// 在屏幕空间上的从起始点到结束点的坐标[0, resolution]</span><br>    <span class=\"hljs-type\">vec4</span> start_clip = vp * <span class=\"hljs-type\">vec4</span>(start_pos_world, <span class=\"hljs-number\">1.0</span>);<br>    <span class=\"hljs-type\">vec4</span> end_clip   = vp * <span class=\"hljs-type\">vec4</span>(end_pos_world, <span class=\"hljs-number\">1.0</span>);<br>    <span class=\"hljs-comment\">// 在屏幕空间进行光线步进</span><br>    <span class=\"hljs-comment\">// 起始点和结束点</span><br>    <span class=\"hljs-type\">vec3</span> start_ndc  = start_clip.xyz / start_clip.w;<br>    <span class=\"hljs-type\">vec3</span> end_ndc    = end_clip.xyz / end_clip.w;<br>    <span class=\"hljs-type\">vec3</span> ndc_diff = end_ndc - start_ndc;<br><br>    <span class=\"hljs-comment\">// ndc-&gt;屏幕坐标 [0, resolution.xy]</span><br>    <span class=\"hljs-type\">vec3</span> start_screen  = <span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">0</span>);<br>    start_screen.xy = (start_ndc.xy + <span class=\"hljs-number\">1</span>) / <span class=\"hljs-number\">2</span> * tex_size;<br>    start_screen.z = (near_far.y - near_far.x) * <span class=\"hljs-number\">0.5</span> * start_ndc.z + (near_far.x + near_far.y) * <span class=\"hljs-number\">0.5</span>;<br><br>    <span class=\"hljs-type\">vec3</span> end_screen    = <span class=\"hljs-type\">vec3</span>(<span class=\"hljs-number\">0</span>);    <br>    end_screen.xy = (end_ndc.xy + <span class=\"hljs-number\">1</span>) / <span class=\"hljs-number\">2</span> * tex_size;<br>    end_screen.z = (near_far.y - near_far.x) * <span class=\"hljs-number\">0.5</span> * end_ndc.z + (near_far.x + near_far.y) * <span class=\"hljs-number\">0.5</span>;<br><br>    <span class=\"hljs-type\">int</span> step_count = <span class=\"hljs-number\">32</span>;<br><br>    <span class=\"hljs-type\">vec3</span> screen_diff = end_screen - start_screen;<br>    <span class=\"hljs-type\">int</span> sample_count = <span class=\"hljs-type\">int</span>(<span class=\"hljs-built_in\">max</span>(<span class=\"hljs-built_in\">abs</span>(screen_diff.x), <span class=\"hljs-built_in\">abs</span>(screen_diff.y)) * resolution) ; <span class=\"hljs-comment\">// 大于1</span><br><br>    sample_count = <span class=\"hljs-built_in\">min</span>(sample_count, <span class=\"hljs-number\">64</span>);<br>    <span class=\"hljs-type\">vec3</span> delta_screen = screen_diff / <span class=\"hljs-type\">float</span>(sample_count);<br><br>    <span class=\"hljs-comment\">// 如果sample count为10，则每次采样的前进的长度为总长度的1/10</span><br>    <span class=\"hljs-type\">float</span> percentage_delta = <span class=\"hljs-number\">1.0</span> / <span class=\"hljs-type\">float</span>(sample_count);<br>    <span class=\"hljs-type\">vec3</span> current_screen = start_screen;<br>    <span class=\"hljs-type\">vec3</span> last_screen = current_screen;<br>    <span class=\"hljs-type\">float</span> current_percentage = <span class=\"hljs-number\">0.0</span>;<br>    <span class=\"hljs-type\">float</span> last_percentage = <span class=\"hljs-number\">0.0</span>;<br><br>    ...<br><br></code></pre></td></tr></table></figure>\n<p>使用屏幕空间步近，前面和原来的差不多，在获取步近的起始点和结束点的时候，需要将坐标转换为屏幕空间的坐标，也就是其中的current_screen和last_screen。</p>\n<p>屏幕空间采样点数和<strong>采样的起始和结束位置的像素差值</strong>有关，所以和渲染输出的分辨率也是相关的。如果渲染分辨率越高，其对应所需要的采样点数可能也会增加，这里我们控制在64以内。当然如果起始点和结束点的像素差值较小，对应的采样点数也会变小，也就是对于距离相机很远的位置的采样会减少，在怎么不影响效果的情况下提升性能表现。</p>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs glsl\">    ...<br><br>    <span class=\"hljs-type\">vec4</span> reflect_color = <span class=\"hljs-type\">vec4</span>(<span class=\"hljs-number\">0.0</span>);<br>    <span class=\"hljs-type\">vec3</span> cam_pos = matrix_ubo.cam_pos.xyz;<br>    <span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">int</span> i = <span class=\"hljs-number\">0</span>; i &lt; sample_count; i++)<br>    &#123;<br>        <span class=\"hljs-comment\">// 采样当前屏幕上的点对应场景世界空间坐标的位置</span><br>        <span class=\"hljs-type\">vec2</span> uv = current_screen.xy / tex_size;<br><br>        <span class=\"hljs-comment\">// 转换为贴图坐标，检查越界</span><br>        <span class=\"hljs-keyword\">if</span>(uv.x &lt; <span class=\"hljs-number\">0.0</span> || uv.y &lt; <span class=\"hljs-number\">0.0</span> || uv.x &gt; <span class=\"hljs-number\">1.0</span> || uv.y &gt; <span class=\"hljs-number\">1.0</span>)<br>        &#123;<br>            <span class=\"hljs-keyword\">continue</span>;<br>        &#125;<br>        <span class=\"hljs-keyword\">else</span><br>        &#123;<br>            <span class=\"hljs-comment\">// 延迟渲染存储的屏幕对应的世界位置</span><br>            <span class=\"hljs-type\">vec3</span> sample_world = <span class=\"hljs-built_in\">texture</span>(scene_position, uv).xyz;<br>            <br>            <span class=\"hljs-type\">vec4</span> sample_ndc = <span class=\"hljs-type\">vec4</span>(<span class=\"hljs-built_in\">mix</span>(start_ndc, end_ndc, current_percentage), <span class=\"hljs-number\">1.0</span>);<br>            <span class=\"hljs-keyword\">if</span>(compareDepth(sample_ndc, uv))<br>            &#123;<br>                <span class=\"hljs-comment\">// 初筛后再二分法检查</span><br>                <span class=\"hljs-type\">int</span> split_count = <span class=\"hljs-number\">5</span>;<br>                <span class=\"hljs-keyword\">while</span>(split_count &gt; <span class=\"hljs-number\">0</span>)<br>                &#123;<br>                    <span class=\"hljs-type\">vec3</span> mid_screen = (last_screen + current_screen) * <span class=\"hljs-number\">0.5</span>;<br>                    <span class=\"hljs-type\">float</span> mid_percentage = (last_percentage + current_percentage) * <span class=\"hljs-number\">0.5</span>;<br>                    <span class=\"hljs-type\">vec4</span> mid_ndc = <span class=\"hljs-type\">vec4</span>(<span class=\"hljs-built_in\">mix</span>(start_ndc, end_ndc, mid_percentage), <span class=\"hljs-number\">1.0</span>);<br>                    uv = mid_screen.xy / tex_size;<br>                    <span class=\"hljs-keyword\">if</span>(compareDepth(mid_ndc, uv))<br>                    &#123;<br>                        current_screen = mid_screen;<br>                    &#125;<br>                    <span class=\"hljs-keyword\">else</span><br>                    &#123;<br>                        last_screen = mid_screen;<br>                    &#125;<br>                    split_count--;<br>                &#125;<br><br>                reflect_color = <span class=\"hljs-built_in\">texture</span>(scene_color, uv);<br>                <span class=\"hljs-keyword\">break</span>;<br>            &#125;<br><br>            last_screen = current_screen;<br>            last_percentage = current_percentage;<br>            current_screen += delta_screen;<br>            current_percentage += percentage_delta;<br>        &#125;<br>    &#125;<br><br>    FragColor = reflect_color*metallic;<br><br>&#125;<br><br></code></pre></td></tr></table></figure>\n<p>这里在屏幕空间采样还配合了之前的粗筛和精筛的方法，下面是使用屏幕空间采样的表现。可以看到条纹的状况被极大的缓解了。</p>\n<p><img src=\"/2024/12/10/screen-space-reflection/ssr_result.png\" alt=\"SSR屏幕空间采样结果\"></p>\n<p>应用在实际场景中，SSR的效果能比较明显的提升渲染质感。<br><img src=\"/2024/12/10/screen-space-reflection/ssr_mugshot.png\" alt=\"SSR实际应用\"></p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>SSR是一种计算场景的反射效果的算法，它基于屏幕空间已有的深度图和法线图等信息，通过计算反射向量，在屏幕空间中进行光线追踪，查找反射光线与场景中其他物体的相交点，获取相交点的颜色作为反射颜色，并与原始颜色合成得到最终渲染结果。</p>\n<p>SSR的优点是计算效率相对较高，能实时反映场景中物体的变化，适用于复杂几何形状和不规则表面，适合大规模的动态场景，无需额外的镜头或几何体。</p>\n<p>当然，SSR也有局限性，它只能反射屏幕上可见的物体，超出屏幕边界的内容无法被反射；反射的物体可能存在失真或错误，尤其是边缘区域；依赖屏幕分辨率，高分辨率下可能对性能有较大影响</p>\n<h2 id=\"SSR的优化改进算法：\"><a href=\"#SSR的优化改进算法：\" class=\"headerlink\" title=\"SSR的优化改进算法：\"></a>SSR的优化改进算法：</h2><h3 id=\"SSSR（Spatially-Separated-Screen-Space-Reflection）\"><a href=\"#SSSR（Spatially-Separated-Screen-Space-Reflection）\" class=\"headerlink\" title=\"SSSR（Spatially Separated Screen Space Reflection）\"></a>SSSR（Spatially Separated Screen Space Reflection）</h3><p>原理：SSSR 是对传统 SSR 技术的一种改进。它主要是基于空间分离的思想来处理屏幕空间反射。传统 SSR 在处理反射时可能会受到屏幕空间限制和采样不足等问题的影响。SSSR 通过将屏幕空间划分为不同的区域，在这些区域内分别进行更精细的反射处理。</p>\n<p>例如，它可以根据场景中物体的距离、重要性或者反射特性等因素，对空间进行划分。对于反射效果比较复杂或者重要的区域，分配更多的资源进行反射计算，而对于相对简单或者不重要的区域，则采用较为简略的计算方式。</p>\n<p>优点：</p>\n<ul>\n<li>提高反射精度：通过对特定区域的精细处理，能够有效提高反射的精度。比如在处理具有高反射率的物体表面或者复杂的光照反射场景时，可以得到更真实、细腻的反射效果。</li>\n<li>优化性能：与传统 SSR 相比，SSSR 能够更合理地分配计算资源。它避免了在整个屏幕空间进行统一标准的反射计算，从而在一定程度上减轻了计算负担，特别是在大规模复杂场景中，可以更好地平衡反射效果和性能。</li>\n</ul>\n<p>局限性：</p>\n<ul>\n<li>空间划分的复杂性：如何合理地划分空间是一个具有挑战性的问题。如果空间划分不合理，可能会导致反射效果出现不自然的边界或者遗漏重要的反射区域。</li>\n<li>增加算法复杂度：空间划分和不同区域的分别处理增加了算法的复杂度。这可能会导致开发和调试的难度增加，并且在某些情况下，可能会引入新的错误或者视觉瑕疵。</li>\n</ul>\n<h3 id=\"Hi-z-SSR（Hierarchical-z-Screen-Space-Reflection）\"><a href=\"#Hi-z-SSR（Hierarchical-z-Screen-Space-Reflection）\" class=\"headerlink\" title=\"Hi-z SSR（Hierarchical - z Screen Space Reflection）\"></a>Hi-z SSR（Hierarchical - z Screen Space Reflection）</h3><p>原理：Hi - z SSR 是利用层次化的深度信息（Hierarchical-z）来改进 SSR。它构建了一个层次化的深度缓冲区，这个缓冲区可以更有效地存储和检索深度信息。在计算反射时，通过这个层次化的结构，可以快速地在不同层次的深度信息中进行搜索和采样。<br>例如，在较高层次的深度信息中，可以快速定位反射光线可能相交的大致区域，然后在较低层次的深度信息中进行更精细的搜索，就像在地图的不同比例尺中查找目标位置一样。这种层次化的搜索方式能够更高效地利用深度信息来计算反射。</p>\n<p>优点：</p>\n<ul>\n<li>高效的深度搜索：层次化的深度搜索大大提高了反射光线与场景相交点的查找效率。尤其是在处理具有深度层次丰富的复杂场景时，能够快速定位反射位置，减少计算时间。</li>\n<li>增强的反射范围：由于能够更好地利用深度信息，Hi-z SSR 可以在一定程度上缓解传统 SSR 中屏幕外反射难以处理的问题。它可以通过层次化的深度结构，对屏幕外部分场景的深度信息进行合理推测和利用，从而扩展反射的有效范围。</li>\n</ul>\n<p>局限性</p>\n<ul>\n<li><p>深度缓冲区的构建成本：构建层次化的深度缓冲区需要额外的存储空间和计算资源来生成和维护。这可能会在一些资源受限的场景或者硬件平台上带来一定的负担。</p>\n</li>\n<li><p>精度与性能的平衡：尽管 Hi-z SSR 提高了搜索效率，但在平衡反射精度和性能方面仍然是一个挑战。在某些情况下，过于追求效率可能会导致反射精度下降，而过度强调精度又可能会使性能开销过大。</p>\n</li>\n</ul>\n<h1 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h1><p><a href=\"https://lettier.github.io/3d-game-shaders-for-beginners/screen-space-reflection.html\">https://lettier.github.io/3d-game-shaders-for-beginners/screen-space-reflection.html</a></p>\n<p><a href=\"https://jcgt.org/published/0003/04/04/\">https://jcgt.org/published/0003/04/04/</a></p>\n<p><a href=\"https://blog.csdn.net/qjh5606/article/details/120102582?ops_request_misc=%257B%2522request%255Fid%2522%253A%25225a1434f7df5d388dc4166f4877eb172b%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=5a1434f7df5d388dc4166f4877eb172b&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-120102582-null-null.142%5Ev100%5Econtrol&utm_term=Efficient%20GPU%20Screen-Space%20Ray%20Tracing&spm=1018.2226.3001.4187\">https://blog.csdn.net/qjh5606/article/details/120102582?ops_request_misc=%257B%2522request%255Fid%2522%253A%25225a1434f7df5d388dc4166f4877eb172b%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=5a1434f7df5d388dc4166f4877eb172b&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-120102582-null-null.142^v100^control&amp;utm_term=Efficient%20GPU%20Screen-Space%20Ray%20Tracing&amp;spm=1018.2226.3001.4187</a></p>\n"}],"PostAsset":[{"_id":"source/_posts/ProceduralTerrainGeneration/calc_soft_shadow.png","slug":"calc_soft_shadow.png","post":"cm5ht44vd000334574tjcdf63","modified":0,"renderable":0},{"_id":"source/_posts/ProceduralTerrainGeneration/shadertoy_oc11_hardshadow.png","slug":"shadertoy_oc11_hardshadow.png","post":"cm5ht44vd000334574tjcdf63","modified":0,"renderable":0},{"_id":"source/_posts/ProceduralTerrainGeneration/shadertoy_oc11_noshadow.png","slug":"shadertoy_oc11_noshadow.png","post":"cm5ht44vd000334574tjcdf63","modified":0,"renderable":0},{"_id":"source/_posts/ProceduralTerrainGeneration/shadertoy_oc5_noshadow.png","slug":"shadertoy_oc5_noshadow.png","post":"cm5ht44vd000334574tjcdf63","modified":0,"renderable":0},{"_id":"source/_posts/ProceduralTerrainGeneration/shadertoy_terrain.png","slug":"shadertoy_terrain.png","post":"cm5ht44vd000334574tjcdf63","modified":0,"renderable":0},{"_id":"source/_posts/ProceduralTerrainGeneration/shadertoy_terrain_oct5.png","slug":"shadertoy_terrain_oct5.png","post":"cm5ht44vd000334574tjcdf63","modified":0,"renderable":0},{"_id":"source/_posts/ProceduralTerrainGeneration2/terrain_no_grass.png","slug":"terrain_no_grass.png","post":"cm5ht44vf000734572fxi9aa2","modified":0,"renderable":0},{"_id":"source/_posts/ProceduralTerrainGeneration2/terrain_with_all_cloud.png","slug":"terrain_with_all_cloud.png","post":"cm5ht44vf000734572fxi9aa2","modified":0,"renderable":0},{"_id":"source/_posts/ProceduralTerrainGeneration2/terrain_with_grass.png","slug":"terrain_with_grass.png","post":"cm5ht44vf000734572fxi9aa2","modified":0,"renderable":0},{"_id":"source/_posts/ProceduralTerrainGeneration2/terrain_with_high_cloud.png","slug":"terrain_with_high_cloud.png","post":"cm5ht44vf000734572fxi9aa2","modified":0,"renderable":0},{"_id":"source/_posts/ProceduralTerrainGeneration2/terrain_with_sky.png","slug":"terrain_with_sky.png","post":"cm5ht44vf000734572fxi9aa2","modified":0,"renderable":0},{"_id":"source/_posts/ProceduralTerrainGeneration2/terrain_with_sky_fog.png","slug":"terrain_with_sky_fog.png","post":"cm5ht44vf000734572fxi9aa2","modified":0,"renderable":0},{"_id":"source/_posts/ProceduralTerrainGeneration2/terrain_with_sky_fog_diffuse_from_mountain.png","slug":"terrain_with_sky_fog_diffuse_from_mountain.png","post":"cm5ht44vf000734572fxi9aa2","modified":0,"renderable":0},{"_id":"source/_posts/ProceduralTerrainGeneration2/terrain_with_sky_fog_diffuse_from_sky.png","slug":"terrain_with_sky_fog_diffuse_from_sky.png","post":"cm5ht44vf000734572fxi9aa2","modified":0,"renderable":0},{"_id":"source/_posts/ProceduralTerrainGeneration2/terrain_with_sky_fog_less_ambient.png","slug":"terrain_with_sky_fog_less_ambient.png","post":"cm5ht44vf000734572fxi9aa2","modified":0,"renderable":0},{"_id":"source/_posts/digital-human-render-1/metahuman_mugshot.jpg","slug":"metahuman_mugshot.jpg","post":"cm5ht44vg000834578p411g6s","modified":0,"renderable":0},{"_id":"source/_posts/digital-human-render-1/metahuman面部驱动.png","slug":"metahuman面部驱动.png","post":"cm5ht44vg000834578p411g6s","modified":0,"renderable":0},{"_id":"source/_posts/digital-human-render-1/siren.png","slug":"siren.png","post":"cm5ht44vg000834578p411g6s","modified":0,"renderable":0},{"_id":"source/_posts/digital-human-render-1/初音未来.png","slug":"初音未来.png","post":"cm5ht44vg000834578p411g6s","modified":0,"renderable":0},{"_id":"source/_posts/digital-human-render-1/林明美.png","slug":"林明美.png","post":"cm5ht44vg000834578p411g6s","modified":0,"renderable":0},{"_id":"source/_posts/digital-human-render-1/老黄建模.png","slug":"老黄建模.png","post":"cm5ht44vg000834578p411g6s","modified":0,"renderable":0},{"_id":"source/_posts/digital-human-render-1/老黄扫描.png","slug":"老黄扫描.png","post":"cm5ht44vg000834578p411g6s","modified":0,"renderable":0},{"_id":"source/_posts/digital-human-render-1/老黄模型渲染.png","slug":"老黄模型渲染.png","post":"cm5ht44vg000834578p411g6s","modified":0,"renderable":0},{"_id":"source/_posts/digital-human-render-1/老黄驱动.png","slug":"老黄驱动.png","post":"cm5ht44vg000834578p411g6s","modified":0,"renderable":0},{"_id":"source/_posts/digital-human-render-1/虚拟偶像市场规模.png","slug":"虚拟偶像市场规模.png","post":"cm5ht44vg000834578p411g6s","modified":0,"renderable":0},{"_id":"source/_posts/digital-human-render-2/blur-camparison.png","slug":"blur-camparison.png","post":"cm5ht44vi000b34574gl60tn2","modified":0,"renderable":0},{"_id":"source/_posts/digital-human-render-2/BSSRDF-1.png","slug":"BSSRDF-1.png","post":"cm5ht44vi000b34574gl60tn2","modified":0,"renderable":0},{"_id":"source/_posts/digital-human-render-2/BSSRDF.png","slug":"BSSRDF.png","post":"cm5ht44vi000b34574gl60tn2","modified":0,"renderable":0},{"_id":"source/_posts/digital-human-render-2/BTDF.png","slug":"BTDF.png","post":"cm5ht44vi000b34574gl60tn2","modified":0,"renderable":0},{"_id":"source/_posts/digital-human-render-2/Cook-Torrance-BRDF.png","slug":"Cook-Torrance-BRDF.png","post":"cm5ht44vi000b34574gl60tn2","modified":0,"renderable":0},{"_id":"source/_posts/digital-human-render-2/diffuse-comparison.png","slug":"diffuse-comparison.png","post":"cm5ht44vi000b34574gl60tn2","modified":0,"renderable":0},{"_id":"source/_posts/digital-human-render-2/layers_of_skin.png","slug":"layers_of_skin.png","post":"cm5ht44vi000b34574gl60tn2","modified":0,"renderable":0},{"_id":"source/_posts/digital-human-render-2/pbr_sample.png","slug":"pbr_sample.png","post":"cm5ht44vi000b34574gl60tn2","modified":0,"renderable":0},{"_id":"source/_posts/digital-human-render-2/reflectance-lookup-table.png","slug":"reflectance-lookup-table.png","post":"cm5ht44vi000b34574gl60tn2","modified":0,"renderable":0},{"_id":"source/_posts/digital-human-render-2/screen-space-blur.png","slug":"screen-space-blur.png","post":"cm5ht44vi000b34574gl60tn2","modified":0,"renderable":0},{"_id":"source/_posts/digital-human-render-2/skin-specular.png","slug":"skin-specular.png","post":"cm5ht44vi000b34574gl60tn2","modified":0,"renderable":0},{"_id":"source/_posts/digital-human-render-2/specular-lobe.png","slug":"specular-lobe.png","post":"cm5ht44vi000b34574gl60tn2","modified":0,"renderable":0},{"_id":"source/_posts/digital-human-render-2/texture-space-blur.png","slug":"texture-space-blur.png","post":"cm5ht44vi000b34574gl60tn2","modified":0,"renderable":0},{"_id":"source/_posts/digital-human-render-2/ue-skin.png","slug":"ue-skin.png","post":"cm5ht44vi000b34574gl60tn2","modified":0,"renderable":0},{"_id":"source/_posts/cascade-shadow-map/csm_far.png","slug":"csm_far.png","post":"cm5ht44vi000c3457fj093ouj","modified":0,"renderable":0},{"_id":"source/_posts/cascade-shadow-map/csm_mid.png","slug":"csm_mid.png","post":"cm5ht44vi000c3457fj093ouj","modified":0,"renderable":0},{"_id":"source/_posts/cascade-shadow-map/csm_near.png","slug":"csm_near.png","post":"cm5ht44vi000c3457fj093ouj","modified":0,"renderable":0},{"_id":"source/_posts/cascade-shadow-map/csm_result.png","slug":"csm_result.png","post":"cm5ht44vi000c3457fj093ouj","modified":0,"renderable":0},{"_id":"source/_posts/cascade-shadow-map/sm_far.png","slug":"sm_far.png","post":"cm5ht44vi000c3457fj093ouj","modified":0,"renderable":0},{"_id":"source/_posts/cascade-shadow-map/sm_near.png","slug":"sm_near.png","post":"cm5ht44vi000c3457fj093ouj","modified":0,"renderable":0},{"_id":"source/_posts/defer-render/defer_render.png","slug":"defer_render.png","post":"cm5ht44vj000g3457hmrd48k7","modified":0,"renderable":0},{"_id":"source/_posts/defer-render/defer_render_banner.png","slug":"defer_render_banner.png","post":"cm5ht44vj000g3457hmrd48k7","modified":0,"renderable":0},{"_id":"source/_posts/defer-render/no_defer_render.png","slug":"no_defer_render.png","post":"cm5ht44vj000g3457hmrd48k7","modified":0,"renderable":0},{"_id":"source/_posts/defer-render/ssao.png","slug":"ssao.png","post":"cm5ht44vj000g3457hmrd48k7","modified":0,"renderable":0},{"_id":"source/_posts/depth-of-field/dilate_after.png","slug":"dilate_after.png","post":"cm5ht44vk000i3457fr8k5jn8","modified":0,"renderable":0},{"_id":"source/_posts/depth-of-field/dilate_before.png","slug":"dilate_before.png","post":"cm5ht44vk000i3457fr8k5jn8","modified":0,"renderable":0},{"_id":"source/_posts/depth-of-field/dof_butterfly.JPG","slug":"dof_butterfly.JPG","post":"cm5ht44vk000i3457fr8k5jn8","modified":0,"renderable":0},{"_id":"source/_posts/depth-of-field/dof_far.png","slug":"dof_far.png","post":"cm5ht44vk000i3457fr8k5jn8","modified":0,"renderable":0},{"_id":"source/_posts/depth-of-field/dof_near.png","slug":"dof_near.png","post":"cm5ht44vk000i3457fr8k5jn8","modified":0,"renderable":0},{"_id":"source/_posts/depth-of-field/game1.jpg","slug":"game1.jpg","post":"cm5ht44vk000i3457fr8k5jn8","modified":0,"renderable":0},{"_id":"source/_posts/reflective-shadow-map/rsm_info.png","slug":"rsm_info.png","post":"cm5ht44vk000l3457fhgx5jjx","modified":0,"renderable":0},{"_id":"source/_posts/reflective-shadow-map/rsm_off_sphere.png","slug":"rsm_off_sphere.png","post":"cm5ht44vk000l3457fhgx5jjx","modified":0,"renderable":0},{"_id":"source/_posts/reflective-shadow-map/rsm_off_suit.png","slug":"rsm_off_suit.png","post":"cm5ht44vk000l3457fhgx5jjx","modified":0,"renderable":0},{"_id":"source/_posts/reflective-shadow-map/rsm_on_sphere.png","slug":"rsm_on_sphere.png","post":"cm5ht44vk000l3457fhgx5jjx","modified":0,"renderable":0},{"_id":"source/_posts/reflective-shadow-map/rsm_on_suit.png","slug":"rsm_on_suit.png","post":"cm5ht44vk000l3457fhgx5jjx","modified":0,"renderable":0},{"_id":"source/_posts/reflective-shadow-map/rsm_principle_1.png","slug":"rsm_principle_1.png","post":"cm5ht44vk000l3457fhgx5jjx","modified":0,"renderable":0},{"_id":"source/_posts/reflective-shadow-map/rsm_principle_2.png","slug":"rsm_principle_2.png","post":"cm5ht44vk000l3457fhgx5jjx","modified":0,"renderable":0},{"_id":"source/_posts/reflective-shadow-map/rsm_sample.png","slug":"rsm_sample.png","post":"cm5ht44vk000l3457fhgx5jjx","modified":0,"renderable":0},{"_id":"source/_posts/ue-ai-texture-generation/run_texgen.png","slug":"run_texgen.png","post":"cm5ht44vq002e345762oy6kcy","modified":0,"renderable":0},{"_id":"source/_posts/ue-ai-texture-generation/texgen_bp.png","slug":"texgen_bp.png","post":"cm5ht44vq002e345762oy6kcy","modified":0,"renderable":0},{"_id":"source/_posts/ue-ai-texture-generation/texgen_bp_macro.png","slug":"texgen_bp_macro.png","post":"cm5ht44vq002e345762oy6kcy","modified":0,"renderable":0},{"_id":"source/_posts/ue-ai-texture-generation/texgen_cmd.png","slug":"texgen_cmd.png","post":"cm5ht44vq002e345762oy6kcy","modified":0,"renderable":0},{"_id":"source/_posts/soft-shadow/get-d_blocker-1.png","slug":"get-d_blocker-1.png","post":"cm5ht44vq002f34579f1tfmkj","modified":0,"renderable":0},{"_id":"source/_posts/soft-shadow/get-d_blocker-2.png","slug":"get-d_blocker-2.png","post":"cm5ht44vq002f34579f1tfmkj","modified":0,"renderable":0},{"_id":"source/_posts/soft-shadow/normal-shadow.png","slug":"normal-shadow.png","post":"cm5ht44vq002f34579f1tfmkj","modified":0,"renderable":0},{"_id":"source/_posts/soft-shadow/pcf-shadow-1.png","slug":"pcf-shadow-1.png","post":"cm5ht44vq002f34579f1tfmkj","modified":0,"renderable":0},{"_id":"source/_posts/soft-shadow/pcf-shadow-3.png","slug":"pcf-shadow-3.png","post":"cm5ht44vq002f34579f1tfmkj","modified":0,"renderable":0},{"_id":"source/_posts/soft-shadow/PCSS_OFF.png","slug":"PCSS_OFF.png","post":"cm5ht44vq002f34579f1tfmkj","modified":0,"renderable":0},{"_id":"source/_posts/soft-shadow/PCSS_ON.png","slug":"PCSS_ON.png","post":"cm5ht44vq002f34579f1tfmkj","modified":0,"renderable":0},{"_id":"source/_posts/soft-shadow/penumbra.png","slug":"penumbra.png","post":"cm5ht44vq002f34579f1tfmkj","modified":0,"renderable":0},{"_id":"source/_posts/soft-shadow/real_shadow.png","slug":"real_shadow.png","post":"cm5ht44vq002f34579f1tfmkj","modified":0,"renderable":0},{"_id":"source/_posts/soft-shadow/soft-shadow-thumbnail.png","slug":"soft-shadow-thumbnail.png","post":"cm5ht44vq002f34579f1tfmkj","modified":0,"renderable":0},{"_id":"source/_posts/soft-shadow/umbra-contrast.png","slug":"umbra-contrast.png","post":"cm5ht44vq002f34579f1tfmkj","modified":0,"renderable":0},{"_id":"source/_posts/soft-shadow/umbra-principle.png","slug":"umbra-principle.png","post":"cm5ht44vq002f34579f1tfmkj","modified":0,"renderable":0},{"_id":"source/_posts/single-scatter-atmosphere/kong-screen-shot.png","slug":"kong-screen-shot.png","post":"cm5ht44vr002h34576ucy7xvt","modified":0,"renderable":0},{"_id":"source/_posts/single-scatter-atmosphere/single-scatter-atmosphere.mp4","slug":"single-scatter-atmosphere.mp4","post":"cm5ht44vr002h34576ucy7xvt","modified":0,"renderable":0},{"_id":"source/_posts/single-scatter-atmosphere/single-scatter-atmosphere.png","slug":"single-scatter-atmosphere.png","post":"cm5ht44vr002h34576ucy7xvt","modified":0,"renderable":0},{"_id":"source/_posts/screen-space-reflection/screen-space-reflection-in-kong.png","slug":"screen-space-reflection-in-kong.png","post":"cm5ht44vr002i3457f0ld1hf6","modified":0,"renderable":0},{"_id":"source/_posts/screen-space-reflection/ssr-step3.gif","slug":"ssr-step3.gif","post":"cm5ht44vr002i3457f0ld1hf6","modified":0,"renderable":0},{"_id":"source/_posts/screen-space-reflection/ssr-step4.gif","slug":"ssr-step4.gif","post":"cm5ht44vr002i3457f0ld1hf6","modified":0,"renderable":0},{"_id":"source/_posts/screen-space-reflection/ssr_mugshot.png","slug":"ssr_mugshot.png","post":"cm5ht44vr002i3457f0ld1hf6","modified":0,"renderable":0},{"_id":"source/_posts/screen-space-reflection/ssr_normal_s128.png","slug":"ssr_normal_s128.png","post":"cm5ht44vr002i3457f0ld1hf6","modified":0,"renderable":0},{"_id":"source/_posts/screen-space-reflection/ssr_normal_s32.png","slug":"ssr_normal_s32.png","post":"cm5ht44vr002i3457f0ld1hf6","modified":0,"renderable":0},{"_id":"source/_posts/screen-space-reflection/ssr_over_sample.png","slug":"ssr_over_sample.png","post":"cm5ht44vr002i3457f0ld1hf6","modified":0,"renderable":0},{"_id":"source/_posts/screen-space-reflection/ssr_result.png","slug":"ssr_result.png","post":"cm5ht44vr002i3457f0ld1hf6","modified":0,"renderable":0},{"_id":"source/_posts/screen-space-reflection/ssr_sample_twice.png","slug":"ssr_sample_twice.png","post":"cm5ht44vr002i3457f0ld1hf6","modified":0,"renderable":0},{"_id":"source/_posts/screen-space-reflection/ssr_ss_sample.png","slug":"ssr_ss_sample.png","post":"cm5ht44vr002i3457f0ld1hf6","modified":0,"renderable":0},{"_id":"source/_posts/screen-space-reflection/ssr_under_sample.png","slug":"ssr_under_sample.png","post":"cm5ht44vr002i3457f0ld1hf6","modified":0,"renderable":0}],"PostCategory":[{"post_id":"cm5ht44vf000734572fxi9aa2","category_id":"cm5ht44ve00043457e31s9pfj","_id":"cm5ht44vj000d34578zoy0651"},{"post_id":"cm5ht44va00013457cnap5ss6","category_id":"cm5ht44ve00043457e31s9pfj","_id":"cm5ht44vj000h3457f0jl8hxi"},{"post_id":"cm5ht44vg000834578p411g6s","category_id":"cm5ht44ve00043457e31s9pfj","_id":"cm5ht44vk000j3457ct4y68p7"},{"post_id":"cm5ht44vi000b34574gl60tn2","category_id":"cm5ht44ve00043457e31s9pfj","_id":"cm5ht44vl000m345738fdafo7"},{"post_id":"cm5ht44vd000334574tjcdf63","category_id":"cm5ht44ve00043457e31s9pfj","_id":"cm5ht44vl000n3457hyju0bcg"},{"post_id":"cm5ht44vi000c3457fj093ouj","category_id":"cm5ht44ve00043457e31s9pfj","_id":"cm5ht44vl000q34578bnhd8x8"},{"post_id":"cm5ht44vj000g3457hmrd48k7","category_id":"cm5ht44ve00043457e31s9pfj","_id":"cm5ht44vl000s345705moen8c"},{"post_id":"cm5ht44vf00063457063u804r","category_id":"cm5ht44vj000e3457gn1x7npn","_id":"cm5ht44vl000u3457c3u81m35"},{"post_id":"cm5ht44vk000i3457fr8k5jn8","category_id":"cm5ht44ve00043457e31s9pfj","_id":"cm5ht44vl000x3457evkv32o7"},{"post_id":"cm5ht44vk000l3457fhgx5jjx","category_id":"cm5ht44ve00043457e31s9pfj","_id":"cm5ht44vl000z3457b66rfr3g"},{"post_id":"cm5ht44vq002e345762oy6kcy","category_id":"cm5ht44ve00043457e31s9pfj","_id":"cm5ht44vr002j34578a4hftl0"},{"post_id":"cm5ht44vq002f34579f1tfmkj","category_id":"cm5ht44ve00043457e31s9pfj","_id":"cm5ht44vr002l3457di5bbln9"},{"post_id":"cm5ht44vr002h34576ucy7xvt","category_id":"cm5ht44ve00043457e31s9pfj","_id":"cm5ht44vs002o345703jk0fjq"},{"post_id":"cm5ht44vr002i3457f0ld1hf6","category_id":"cm5ht44ve00043457e31s9pfj","_id":"cm5ht44vs002q34576nybbd7y"}],"PostTag":[{"post_id":"cm5ht44vk000l3457fhgx5jjx","tag_id":"cm5ht44ve000534572nmb8eqy","_id":"cm5ht44vl000o3457blwu2for"},{"post_id":"cm5ht44vk000l3457fhgx5jjx","tag_id":"cm5ht44vg000a34577dr78n3x","_id":"cm5ht44vl000r34570ids9jyb"},{"post_id":"cm5ht44vk000l3457fhgx5jjx","tag_id":"cm5ht44vj000f3457a737gg6k","_id":"cm5ht44vl000t3457eggzd7ka"},{"post_id":"cm5ht44vk000l3457fhgx5jjx","tag_id":"cm5ht44vk000k34578vdnfhzo","_id":"cm5ht44vl000w34571mya962m"},{"post_id":"cm5ht44va00013457cnap5ss6","tag_id":"cm5ht44ve000534572nmb8eqy","_id":"cm5ht44vl000y34579j42fe45"},{"post_id":"cm5ht44va00013457cnap5ss6","tag_id":"cm5ht44vg000a34577dr78n3x","_id":"cm5ht44vm001134572w3sap4s"},{"post_id":"cm5ht44va00013457cnap5ss6","tag_id":"cm5ht44vj000f3457a737gg6k","_id":"cm5ht44vm001234579xxw6bqm"},{"post_id":"cm5ht44va00013457cnap5ss6","tag_id":"cm5ht44vk000k34578vdnfhzo","_id":"cm5ht44vm001434571emt0ibd"},{"post_id":"cm5ht44va00013457cnap5ss6","tag_id":"cm5ht44vl000p3457e08h4kob","_id":"cm5ht44vm001534574ppoc1fn"},{"post_id":"cm5ht44vd000334574tjcdf63","tag_id":"cm5ht44ve000534572nmb8eqy","_id":"cm5ht44vm00193457h3jb97ku"},{"post_id":"cm5ht44vd000334574tjcdf63","tag_id":"cm5ht44vg000a34577dr78n3x","_id":"cm5ht44vm001a34577k38h8u0"},{"post_id":"cm5ht44vd000334574tjcdf63","tag_id":"cm5ht44vj000f3457a737gg6k","_id":"cm5ht44vm001c3457bz0ed131"},{"post_id":"cm5ht44vd000334574tjcdf63","tag_id":"cm5ht44vk000k34578vdnfhzo","_id":"cm5ht44vm001d34570eqcfp07"},{"post_id":"cm5ht44vd000334574tjcdf63","tag_id":"cm5ht44vl000p3457e08h4kob","_id":"cm5ht44vn001f3457bsbzhigy"},{"post_id":"cm5ht44vf00063457063u804r","tag_id":"cm5ht44vm00183457cmmt2hmv","_id":"cm5ht44vn001g3457eo5bdfh0"},{"post_id":"cm5ht44vf000734572fxi9aa2","tag_id":"cm5ht44ve000534572nmb8eqy","_id":"cm5ht44vn001k34572mwnfut6"},{"post_id":"cm5ht44vf000734572fxi9aa2","tag_id":"cm5ht44vg000a34577dr78n3x","_id":"cm5ht44vn001l3457arp99fvu"},{"post_id":"cm5ht44vf000734572fxi9aa2","tag_id":"cm5ht44vj000f3457a737gg6k","_id":"cm5ht44vn001n34573c5b7n0s"},{"post_id":"cm5ht44vf000734572fxi9aa2","tag_id":"cm5ht44vk000k34578vdnfhzo","_id":"cm5ht44vn001o34577hv59ejx"},{"post_id":"cm5ht44vf000734572fxi9aa2","tag_id":"cm5ht44vl000p3457e08h4kob","_id":"cm5ht44vn001q34579kzqg3bx"},{"post_id":"cm5ht44vg000834578p411g6s","tag_id":"cm5ht44ve000534572nmb8eqy","_id":"cm5ht44vn001r34577mwe1szx"},{"post_id":"cm5ht44vg000834578p411g6s","tag_id":"cm5ht44vj000f3457a737gg6k","_id":"cm5ht44vn001t34572dtc891e"},{"post_id":"cm5ht44vg000834578p411g6s","tag_id":"cm5ht44vn001m345794w57bvk","_id":"cm5ht44vo001u34572p7vamc5"},{"post_id":"cm5ht44vi000b34574gl60tn2","tag_id":"cm5ht44ve000534572nmb8eqy","_id":"cm5ht44vo001w3457hiodd5nk"},{"post_id":"cm5ht44vi000b34574gl60tn2","tag_id":"cm5ht44vj000f3457a737gg6k","_id":"cm5ht44vo001x3457dbb26ejc"},{"post_id":"cm5ht44vi000b34574gl60tn2","tag_id":"cm5ht44vn001m345794w57bvk","_id":"cm5ht44vo001z34573v5mhy7z"},{"post_id":"cm5ht44vi000c3457fj093ouj","tag_id":"cm5ht44ve000534572nmb8eqy","_id":"cm5ht44vo002134571hxv1doj"},{"post_id":"cm5ht44vi000c3457fj093ouj","tag_id":"cm5ht44vg000a34577dr78n3x","_id":"cm5ht44vo0022345751o4f6tn"},{"post_id":"cm5ht44vi000c3457fj093ouj","tag_id":"cm5ht44vj000f3457a737gg6k","_id":"cm5ht44vo002434574lk1e3y9"},{"post_id":"cm5ht44vi000c3457fj093ouj","tag_id":"cm5ht44vk000k34578vdnfhzo","_id":"cm5ht44vo002534572tiu0esr"},{"post_id":"cm5ht44vj000g3457hmrd48k7","tag_id":"cm5ht44ve000534572nmb8eqy","_id":"cm5ht44vo0026345794b4hn4x"},{"post_id":"cm5ht44vj000g3457hmrd48k7","tag_id":"cm5ht44vg000a34577dr78n3x","_id":"cm5ht44vo002734576el65bhu"},{"post_id":"cm5ht44vj000g3457hmrd48k7","tag_id":"cm5ht44vj000f3457a737gg6k","_id":"cm5ht44vo002834574v3b1cox"},{"post_id":"cm5ht44vj000g3457hmrd48k7","tag_id":"cm5ht44vk000k34578vdnfhzo","_id":"cm5ht44vo00293457gkh29bk3"},{"post_id":"cm5ht44vk000i3457fr8k5jn8","tag_id":"cm5ht44ve000534572nmb8eqy","_id":"cm5ht44vo002a3457dvcq7o8s"},{"post_id":"cm5ht44vk000i3457fr8k5jn8","tag_id":"cm5ht44vg000a34577dr78n3x","_id":"cm5ht44vo002b34574nb03nav"},{"post_id":"cm5ht44vk000i3457fr8k5jn8","tag_id":"cm5ht44vj000f3457a737gg6k","_id":"cm5ht44vo002c34573erqf7s5"},{"post_id":"cm5ht44vk000i3457fr8k5jn8","tag_id":"cm5ht44vk000k34578vdnfhzo","_id":"cm5ht44vo002d3457axrmenn3"},{"post_id":"cm5ht44vr002h34576ucy7xvt","tag_id":"cm5ht44ve000534572nmb8eqy","_id":"cm5ht44vr002k3457cg15ej8f"},{"post_id":"cm5ht44vr002h34576ucy7xvt","tag_id":"cm5ht44vg000a34577dr78n3x","_id":"cm5ht44vs002n3457e6x8ht24"},{"post_id":"cm5ht44vr002h34576ucy7xvt","tag_id":"cm5ht44vj000f3457a737gg6k","_id":"cm5ht44vs002p3457gkic8dan"},{"post_id":"cm5ht44vr002h34576ucy7xvt","tag_id":"cm5ht44vk000k34578vdnfhzo","_id":"cm5ht44vs002s3457cfcfhtzh"},{"post_id":"cm5ht44vr002i3457f0ld1hf6","tag_id":"cm5ht44ve000534572nmb8eqy","_id":"cm5ht44vs002t34576sr810m9"},{"post_id":"cm5ht44vr002i3457f0ld1hf6","tag_id":"cm5ht44vj000f3457a737gg6k","_id":"cm5ht44vs002u345733aa804o"},{"post_id":"cm5ht44vr002i3457f0ld1hf6","tag_id":"cm5ht44vk000k34578vdnfhzo","_id":"cm5ht44vs002v345739qp42gz"},{"post_id":"cm5ht44vq002e345762oy6kcy","tag_id":"cm5ht44vq002g3457dfgqhqi3","_id":"cm5ht44vs002w3457dwen1or7"},{"post_id":"cm5ht44vq002e345762oy6kcy","tag_id":"cm5ht44vr002m3457ff87632p","_id":"cm5ht44vs002x34574tu74wva"},{"post_id":"cm5ht44vq002f34579f1tfmkj","tag_id":"cm5ht44ve000534572nmb8eqy","_id":"cm5ht44vs002y3457eeofe3zo"},{"post_id":"cm5ht44vq002f34579f1tfmkj","tag_id":"cm5ht44vj000f3457a737gg6k","_id":"cm5ht44vs002z3457flpzbxha"},{"post_id":"cm5ht44vq002f34579f1tfmkj","tag_id":"cm5ht44vk000k34578vdnfhzo","_id":"cm5ht44vs003034570eiv4jwu"},{"post_id":"cm5ht44vq002f34579f1tfmkj","tag_id":"cm5ht44vs002r3457g56j0n64","_id":"cm5ht44vs003134570zmz9tr6"}],"Tag":[{"name":"3D","_id":"cm5ht44ve000534572nmb8eqy"},{"name":"render","_id":"cm5ht44vg000a34577dr78n3x"},{"name":"渲染","_id":"cm5ht44vj000f3457a737gg6k"},{"name":"编程","_id":"cm5ht44vk000k34578vdnfhzo"},{"name":"程序化生成","_id":"cm5ht44vl000p3457e08h4kob"},{"name":"生活","_id":"cm5ht44vm00183457cmmt2hmv"},{"name":"数字孪生","_id":"cm5ht44vn001m345794w57bvk"},{"name":"虚幻引擎","_id":"cm5ht44vq002g3457dfgqhqi3"},{"name":"AIGC","_id":"cm5ht44vr002m3457ff87632p"},{"name":"阴影","_id":"cm5ht44vs002r3457g56j0n64"}]}}